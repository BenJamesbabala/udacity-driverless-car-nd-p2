--- [START 2017-02-22 00:06:46] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project2_01/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/05

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def DenseNet_2( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input     = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    #color preprocessing using conv net:
    #see "Systematic evaluation of CNN advances on the ImageNet"-Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas, ARXIV 2016
    # https://arxiv.org/abs/1606.02228

    with tf.variable_scope('preprocess') as scope:
        input = conv2d(input, num_kernels=8, kernel_size=(3, 3), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c1')
        input = vlrelu(input, alpha=0.25, name='r1')
        input = conv2d(input, num_kernels=8, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c2')
        input = vlrelu(input, alpha=0.25, name='r2')


    with tf.variable_scope('block1') as scope:
        block1 = bn_relu_conv2d(input, num_kernels=32, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME')
        block1 = maxpool(block1, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    # we use conv-bn-relu in DENSE block (different from paper)
    # dropout is taken out of the block
    with tf.variable_scope('block2') as scope:
        block2 = dense_block_cbr(block1, num=4, num_kernels=16, kernel_size=(3, 3), drop=None)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = dense_block_cbr(block2, num=4, num_kernels=24, kernel_size=(3, 3), drop=None)
        block3 = dropout(block3, keep=0.9)
        block3 = maxpool(block3,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = dense_block_cbr(block3, num=4, num_kernels=32, kernel_size=(3, 3), drop=None)
        block4 = conv2d_bn_relu(block4, num_kernels=num_class, kernel_size=(1,1), stride=[1, 1, 1, 1], padding='SAME')
        block4 = dropout(block4, keep=0.8)
        block4 = avgpool(block4, is_global_pool=True)


    logit = block4
    return logit
-------------------------------


net summary :
	num of conv     = 16
	all mac         = 27.0 (M)
	all param_size  = 0.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  0.548564    (0.851562)  |  0.566115    (0.821333)  |  1.4 min
 0.2    4.0   01132   0.100000  |  0.276899    (0.921875)  |  0.277198    (0.916000)  |  1.8 min
 0.3    6.0   01698   0.100000  |  0.160163    (0.937500)  |  0.046799    (0.985333)  |  2.2 min
 0.3    8.0   02264   0.100000  |  0.097810    (0.968750)  |  0.033130    (0.990000)  |  2.6 min
 0.4   10.0   02830   0.100000  |  0.028182    (0.992188)  |  0.028318    (0.992000)  |  3.0 min
 0.5   12.0   03396   0.100000  |  0.166102    (0.945312)  |  0.066386    (0.980667)  |  3.4 min
 0.6   14.0   03962   0.100000  |  0.028348    (0.992188)  |  0.023227    (0.994000)  |  3.8 min
 0.7   16.0   04528   0.100000  |  0.025965    (0.992188)  |  0.009104    (0.997333)  |  4.2 min
 0.8   18.0   05094   0.100000  |  0.090260    (0.976562)  |  0.016199    (0.995000)  |  4.6 min
 0.8   20.0   05660   0.100000  |  0.074664    (0.976562)  |  0.003963    (0.998667)  |  5.0 min
 0.9   22.0   06226   0.100000  |  0.019126    (0.992188)  |  0.012067    (0.996000)  |  5.4 min
 1.0   24.0   06792   0.100000  |  0.106132    (0.976562)  |  0.011501    (0.997000)  |  6.8 min
 1.1   26.0   07358   0.100000  |  0.053135    (0.968750)  |  0.013389    (0.995667)  |  7.2 min
 1.2   28.0   07924   0.100000  |  0.055232    (0.984375)  |  0.013974    (0.994667)  |  7.6 min
 1.3   30.0   08490   0.100000  |  0.008704    (1.000000)  |  0.004134    (0.998667)  |  8.0 min
 1.3   32.0   09056   0.100000  |  0.012799    (0.992188)  |  0.005394    (0.998333)  |  8.4 min
 1.4   34.0   09622   0.100000  |  0.008008    (1.000000)  |  0.002132    (0.999667)  |  8.8 min
 1.5   36.0   10188   0.100000  |  0.017708    (0.992188)  |  0.006164    (0.998333)  |  9.3 min
 1.6   38.0   10754   0.100000  |  0.022774    (0.992188)  |  0.005054    (0.998333)  |  9.7 min
 1.7   40.0   11320   0.100000  |  0.043702    (0.984375)  |  0.003193    (0.999333)  | 10.1 min
 1.8   42.0   11886   0.100000  |  0.010055    (0.992188)  |  0.004323    (0.999000)  | 10.5 min
 1.9   44.0   12452   0.100000  |  0.025431    (0.992188)  |  0.003647    (0.999333)  | 10.9 min
 1.9   46.0   13018   0.100000  |  0.004228    (1.000000)  |  0.004937    (0.999333)  | 11.2 min
 2.0   48.0   13584   0.100000  |  0.006582    (1.000000)  |  0.004894    (0.999333)  | 12.7 min
 2.1   50.0   14150   0.100000  |  0.003217    (1.000000)  |  0.001081    (1.000000)  | 13.1 min
 2.2   52.0   14716   0.100000  |  0.030057    (0.992188)  |  0.003401    (0.998667)  | 13.5 min
 2.3   54.0   15282   0.100000  |  0.010672    (1.000000)  |  0.001525    (0.999667)  | 13.9 min
 2.4   56.0   15848   0.100000  |  0.008439    (1.000000)  |  0.005266    (0.998667)  | 14.3 min
 2.4   58.0   16414   0.100000  |  0.014642    (0.992188)  |  0.002980    (0.999000)  | 14.7 min
 2.5   60.0   16980   0.100000  |  0.001059    (1.000000)  |  0.004096    (0.998333)  | 15.1 min
 2.6   62.0   17546   0.100000  |  0.067850    (0.984375)  |  0.001744    (0.999333)  | 15.5 min
 2.7   64.0   18112   0.100000  |  0.031989    (0.992188)  |  0.002824    (0.999000)  | 15.9 min
 2.8   66.0   18678   0.100000  |  0.005438    (1.000000)  |  0.002164    (0.999333)  | 16.3 min
 2.9   68.0   19244   0.100000  |  0.001260    (1.000000)  |  0.001331    (0.999667)  | 16.7 min
 2.9   70.0   19810   0.100000  |  0.015516    (0.992188)  |  0.002475    (0.998667)  | 17.1 min
 3.0   72.0   20376   0.010000  |  0.000858    (1.000000)  |  0.000080    (1.000000)  | 18.5 min
 3.1   74.0   20942   0.010000  |  0.010126    (0.992188)  |  0.000175    (1.000000)  | 18.9 min
 3.2   76.0   21508   0.010000  |  0.001518    (1.000000)  |  0.000089    (1.000000)  | 19.3 min
 3.3   78.0   22074   0.010000  |  0.001342    (1.000000)  |  0.000092    (1.000000)  | 19.7 min
 3.4   80.0   22640   0.010000  |  0.015032    (0.992188)  |  0.000124    (1.000000)  | 20.1 min
 3.5   82.0   23206   0.010000  |  0.001093    (1.000000)  |  0.000106    (1.000000)  | 20.5 min
 3.5   84.0   23772   0.010000  |  0.000082    (1.000000)  |  0.000063    (1.000000)  | 20.9 min
 3.6   86.0   24338   0.010000  |  0.000070    (1.000000)  |  0.000105    (1.000000)  | 21.3 min
 3.7   88.0   24904   0.010000  |  0.000339    (1.000000)  |  0.000139    (1.000000)  | 21.7 min
 3.8   90.0   25470   0.010000  |  0.006624    (0.992188)  |  0.000087    (1.000000)  | 22.1 min
 3.9   92.0   26036   0.010000  |  0.005223    (1.000000)  |  0.000121    (1.000000)  | 22.5 min
 4.0   94.0   26602   0.010000  |  0.000623    (1.000000)  |  0.000082    (1.000000)  | 22.9 min
 4.0   96.0   27168   0.010000  |  0.010683    (0.992188)  |  0.000149    (1.000000)  | 24.3 min
 4.1   98.0   27734   0.010000  |  0.002136    (1.000000)  |  0.000144    (1.000000)  | 24.7 min
 4.2  100.0   28300   0.010000  |  0.000300    (1.000000)  |  0.000111    (1.000000)  | 25.1 min
 4.3  102.0   28866   0.010000  |  0.000700    (1.000000)  |  0.000246    (1.000000)  | 25.5 min
 4.4  104.0   29432   0.010000  |  0.000927    (1.000000)  |  0.000107    (1.000000)  | 25.9 min
 4.5  106.0   29998   0.010000  |  0.001417    (1.000000)  |  0.000153    (1.000000)  | 26.3 min
 4.6  108.0   30564   0.010000  |  0.002501    (1.000000)  |  0.000144    (1.000000)  | 26.7 min
 4.6  110.0   31130   0.010000  |  0.000650    (1.000000)  |  0.000143    (1.000000)  | 27.1 min
 4.7  112.0   31696   0.010000  |  0.000311    (1.000000)  |  0.000111    (1.000000)  | 27.6 min
 4.8  114.0   32262   0.010000  |  0.001792    (1.000000)  |  0.000093    (1.000000)  | 27.9 min
 4.9  116.0   32828   0.010000  |  0.000758    (1.000000)  |  0.000086    (1.000000)  | 28.3 min
 5.0  118.0   33394   0.010000  |  0.000944    (1.000000)  |  0.000098    (1.000000)  | 28.7 min
 5.1  120.0   33960   0.010000  |  0.022731    (0.992188)  |  0.000124    (1.000000)  | 30.2 min
 5.1  122.1   34526   0.010000  |  0.003620    (1.000000)  |  0.000109    (1.000000)  | 30.6 min
 5.2  124.1   35092   0.010000  |  0.000587    (1.000000)  |  0.000075    (1.000000)  | 30.9 min
 5.3  126.1   35658   0.010000  |  0.000812    (1.000000)  |  0.000144    (1.000000)  | 31.3 min
 5.4  128.1   36224   0.010000  |  0.002253    (1.000000)  |  0.000151    (1.000000)  | 31.7 min
 5.5  130.1   36790   0.010000  |  0.012099    (0.992188)  |  0.000101    (1.000000)  | 32.1 min
 5.6  132.1   37356   0.010000  |  0.002835    (1.000000)  |  0.000190    (1.000000)  | 32.5 min
 5.6  134.1   37922   0.010000  |  0.003676    (1.000000)  |  0.000187    (1.000000)  | 32.9 min
 5.7  136.1   38488   0.010000  |  0.000685    (1.000000)  |  0.000175    (1.000000)  | 33.3 min
 5.8  138.1   39054   0.010000  |  0.000165    (1.000000)  |  0.000085    (1.000000)  | 33.7 min
 5.9  140.1   39620   0.010000  |  0.000452    (1.000000)  |  0.000103    (1.000000)  | 34.1 min
 6.0  142.1   40186   0.010000  |  0.000936    (1.000000)  |  0.000081    (1.000000)  | 34.5 min
 6.1  144.1   40752   0.001000  |  0.000503    (1.000000)  |  0.000096    (1.000000)  | 35.9 min
 6.2  146.1   41318   0.001000  |  0.000689    (1.000000)  |  0.000143    (1.000000)  | 36.3 min
 6.2  148.1   41884   0.001000  |  0.000527    (1.000000)  |  0.000086    (1.000000)  | 36.7 min
 6.3  150.1   42450   0.001000  |  0.001658    (1.000000)  |  0.000177    (1.000000)  | 37.1 min
 6.4  152.1   43016   0.001000  |  0.001026    (1.000000)  |  0.000126    (1.000000)  | 37.4 min
 6.5  154.1   43582   0.001000  |  0.004010    (1.000000)  |  0.000169    (1.000000)  | 37.8 min
 6.6  156.1   44148   0.001000  |  0.000259    (1.000000)  |  0.000223    (1.000000)  | 38.2 min
 6.7  158.1   44714   0.001000  |  0.002018    (1.000000)  |  0.000158    (1.000000)  | 38.6 min
 6.7  160.1   45280   0.001000  |  0.000459    (1.000000)  |  0.000137    (1.000000)  | 39.0 min
 6.8  162.1   45846   0.001000  |  0.000603    (1.000000)  |  0.000144    (1.000000)  | 39.4 min
 6.9  164.1   46412   0.001000  |  0.002223    (1.000000)  |  0.000114    (1.000000)  | 39.8 min
 7.0  166.1   46978   0.001000  |  0.023734    (0.992188)  |  0.000140    (1.000000)  | 40.2 min
 7.1  168.1   47544   0.001000  |  0.000578    (1.000000)  |  0.000144    (1.000000)  | 41.6 min
 7.2  170.1   48110   0.001000  |  0.001192    (1.000000)  |  0.000169    (1.000000)  | 42.0 min
 7.2  172.1   48676   0.001000  |  0.000292    (1.000000)  |  0.000157    (1.000000)  | 42.4 min
 7.3  174.1   49242   0.001000  |  0.004990    (1.000000)  |  0.000144    (1.000000)  | 42.8 min
 7.4  176.1   49808   0.001000  |  0.000174    (1.000000)  |  0.000137    (1.000000)  | 43.2 min
 7.5  178.1   50374   0.001000  |  0.000410    (1.000000)  |  0.000152    (1.000000)  | 43.6 min
 7.6  180.1   50940   0.001000  |  0.008614    (1.000000)  |  0.000109    (1.000000)  | 44.0 min
 7.7  182.1   51506   0.001000  |  0.000249    (1.000000)  |  0.000157    (1.000000)  | 44.4 min
 7.8  184.1   52072   0.001000  |  0.004524    (1.000000)  |  0.000148    (1.000000)  | 44.8 min
 7.8  186.1   52638   0.001000  |  0.001652    (1.000000)  |  0.000113    (1.000000)  | 45.2 min
 7.9  188.1   53204   0.001000  |  0.000542    (1.000000)  |  0.000162    (1.000000)  | 45.6 min
 8.0  190.1   53770   0.000100  |  0.000370    (1.000000)  |  0.000125    (1.000000)  | 47.0 min
 8.1  192.1   54336   0.000100  |  0.000628    (1.000000)  |  0.000122    (1.000000)  | 47.4 min
 8.2  194.1   54902   0.000100  |  0.000207    (1.000000)  |  0.000143    (1.000000)  | 47.8 min
 8.3  196.1   55468   0.000100  |  0.000634    (1.000000)  |  0.000126    (1.000000)  | 48.2 min
 8.3  198.1   56034   0.000100  |  0.000609    (1.000000)  |  0.000114    (1.000000)  | 48.6 min
 8.4  200.1   56600   0.000100  |  0.000098    (1.000000)  |  0.000116    (1.000000)  | 49.0 min
 8.5  202.1   57166   0.000100  |  0.001276    (1.000000)  |  0.000133    (1.000000)  | 49.4 min
 8.6  204.1   57732   0.000100  |  0.001103    (1.000000)  |  0.000128    (1.000000)  | 49.8 min
 8.7  206.1   58298   0.000100  |  0.001143    (1.000000)  |  0.000113    (1.000000)  | 50.1 min
 8.8  208.1   58864   0.000100  |  0.000436    (1.000000)  |  0.000126    (1.000000)  | 50.6 min
 8.8  210.1   59430   0.000100  |  0.000283    (1.000000)  |  0.000187    (1.000000)  | 51.0 min
 8.9  212.1   59996   0.000100  |  0.002723    (1.000000)  |  0.000136    (1.000000)  | 51.4 min

** evaluation on test set **

test_loss=0.023283    (test_acc=0.993983)

sucess
