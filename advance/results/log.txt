 
--- [START 2017-02-25 13:02:39] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project2_01/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/20

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def DenseNet_3( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input     = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    #color preprocessing using conv net:
    #see "Systematic evaluation of CNN advances on the ImageNet"-Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas, ARXIV 2016
    # https://arxiv.org/abs/1606.02228
    # we use learnable prelu (different from paper) and 3x3 onv
    with tf.variable_scope('preprocess') as scope:
        input = bn(input, name='b1')
        input = conv2d(input, num_kernels=8, kernel_size=(3, 3), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c1')
        input = prelu(input, name='r1')
        input = conv2d(input, num_kernels=8, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c2')
        input = prelu(input, name='r2')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=32, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME')
        block1 = maxpool(block1, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    # we use conv-bn-relu in DENSE block (different from paper).dropout is taken out of the block
    with tf.variable_scope('block2') as scope:
        block2 = dense_block_cbr(block1, num=4, num_kernels=16, kernel_size=(3, 3), drop=None)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = dense_block_cbr(block2, num=4, num_kernels=24, kernel_size=(3, 3), drop=None)
        block3 = dropout(block3, keep=0.9)
        block3 = maxpool(block3,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = dense_block_cbr(block3, num=4, num_kernels=32, kernel_size=(3, 3), drop=None)
        block4 = conv2d_bn_relu(block4, num_kernels=num_class, kernel_size=(1,1), stride=[1, 1, 1, 1], padding='SAME')
        block4 = dropout(block4, keep=0.8)
        block4 = avgpool(block4, is_global_pool=True)

    logit = block4
    return logit
-------------------------------


net summary : 
	num of conv     = 16
	all mac         = 27.0 (M)
	all param_size  = 0.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  0.672168    (0.765625)  |  0.348899    (0.887667)  |       2.8 min  
 0.2    4.0   01132   0.100000  |  0.321161    (0.921875)  |  0.162553    (0.949000)  |       3.3 min  
 0.3    6.0   01698   0.100000  |  0.211609    (0.929688)  |  0.024789    (0.993333)  |       3.7 min  
 0.3    8.0   02264   0.100000  |  0.143025    (0.976562)  |  0.037956    (0.987000)  |       4.2 min  
 0.4   10.0   02830   0.100000  |  0.113731    (0.968750)  |  0.056130    (0.981333)  |       4.6 min  
 0.5   12.0   03396   0.100000  |  0.211394    (0.937500)  |  0.061882    (0.980667)  |       5.1 min  
 0.6   14.0   03962   0.100000  |  0.114329    (0.968750)  |  0.038405    (0.988667)  |       5.5 min  
 0.7   16.0   04528   0.100000  |  0.091791    (0.960938)  |  0.021111    (0.994333)  |       6.0 min  
 0.8   18.0   05094   0.100000  |  0.113265    (0.960938)  |  0.079799    (0.977000)  |       6.4 min  
 0.8   20.0   05660   0.100000  |  0.087412    (0.984375)  |  0.100011    (0.971667)  |       6.9 min  
 0.9   22.0   06226   0.100000  |  0.103682    (0.984375)  |  0.049890    (0.984667)  |       7.3 min  
 1.0   24.0   06792   0.100000  |  0.094415    (0.976562)  |  0.022964    (0.993333)  |      10.2 min  
 1.1   26.0   07358   0.100000  |  0.139149    (0.968750)  |  0.062875    (0.980667)  |      10.7 min  
 1.2   28.0   07924   0.100000  |  0.058842    (0.992188)  |  0.019937    (0.992333)  |      11.1 min  
 1.3   30.0   08490   0.100000  |  0.147280    (0.953125)  |  0.029883    (0.991333)  |      11.6 min  
 1.3   32.0   09056   0.100000  |  0.062341    (0.984375)  |  0.014457    (0.997000)  |      12.0 min  
 1.4   34.0   09622   0.100000  |  0.246125    (0.953125)  |  0.015957    (0.994333)  |      12.5 min  
 1.5   36.0   10188   0.100000  |  0.077645    (0.976562)  |  0.037441    (0.988000)  |      13.0 min  
 1.6   38.0   10754   0.100000  |  0.070650    (0.976562)  |  0.071707    (0.975000)  |      13.4 min  
 1.7   40.0   11320   0.100000  |  0.109818    (0.968750)  |  0.052181    (0.985000)  |      13.9 min  
 1.8   42.0   11886   0.100000  |  0.061149    (0.984375)  |  0.147608    (0.959000)  |      14.3 min  
 1.9   44.0   12452   0.100000  |  0.053912    (0.992188)  |  0.025094    (0.992667)  |      14.8 min  
 1.9   46.0   13018   0.100000  |  0.064889    (0.976562)  |  0.036145    (0.989667)  |      15.2 min  
 2.0   48.0   13584   0.100000  |  0.126535    (0.960938)  |  0.046641    (0.987667)  |      18.1 min  
 2.1   50.0   14150   0.100000  |  0.046558    (0.984375)  |  0.051575    (0.985333)  |      18.6 min  
 2.2   52.0   14716   0.100000  |  0.047519    (0.992188)  |  0.021801    (0.992667)  |      19.0 min  
 2.3   54.0   15282   0.100000  |  0.051000    (0.992188)  |  0.024364    (0.992667)  |      19.5 min  
 2.4   56.0   15848   0.100000  |  0.243994    (0.929688)  |  0.026485    (0.992667)  |      19.9 min  
 2.4   58.0   16414   0.100000  |  0.160916    (0.953125)  |  0.033919    (0.991667)  |      20.4 min  
 2.5   60.0   16980   0.100000  |  0.075618    (0.953125)  |  0.013175    (0.996333)  |      20.8 min  
 2.6   62.0   17546   0.100000  |  0.043174    (0.992188)  |  0.043561    (0.989000)  |      21.3 min  
 2.7   64.0   18112   0.100000  |  0.157271    (0.953125)  |  0.020146    (0.994000)  |      21.8 min  
 2.8   66.0   18678   0.100000  |  0.062659    (0.976562)  |  0.046360    (0.985667)  |      22.2 min  
 2.9   68.0   19244   0.100000  |  0.129657    (0.976562)  |  0.025819    (0.990667)  |      22.7 min  
 2.9   70.0   19810   0.100000  |  0.057979    (0.976562)  |  0.044132    (0.991000)  |      23.1 min  
 3.0   72.0   20376   0.010000  |  0.016463    (1.000000)  |  0.003127    (0.999333)  |      26.0 min  
 3.1   74.0   20942   0.010000  |  0.009697    (1.000000)  |  0.000522    (1.000000)  |      26.5 min  
 3.2   76.0   21508   0.010000  |  0.043817    (0.984375)  |  0.000571    (1.000000)  |      26.9 min  
 3.3   78.0   22074   0.010000  |  0.001787    (1.000000)  |  0.000251    (1.000000)  |      27.4 min  
 3.4   80.0   22640   0.010000  |  0.028415    (0.992188)  |  0.000326    (1.000000)  |      27.9 min  
 3.5   82.0   23206   0.010000  |  0.004421    (1.000000)  |  0.000249    (1.000000)  |      28.3 min  
 3.5   84.0   23772   0.010000  |  0.009255    (1.000000)  |  0.000401    (1.000000)  |      28.8 min  
 3.6   86.0   24338   0.010000  |  0.002182    (1.000000)  |  0.000167    (1.000000)  |      29.2 min  
 3.7   88.0   24904   0.010000  |  0.014643    (1.000000)  |  0.000167    (1.000000)  |      29.7 min  
 3.8   90.0   25470   0.010000  |  0.004432    (1.000000)  |  0.000112    (1.000000)  |      30.1 min  
 3.9   92.0   26036   0.010000  |  0.093616    (0.976562)  |  0.000491    (0.999667)  |      30.6 min  
 4.0   94.0   26602   0.010000  |  0.008848    (1.000000)  |  0.000816    (0.999667)  |      31.0 min  
 4.0   96.0   27168   0.010000  |  0.016370    (0.992188)  |  0.001084    (0.999667)  |      34.0 min  
 4.1   98.0   27734   0.010000  |  0.012019    (1.000000)  |  0.001921    (0.999667)  |      34.4 min  
 4.2  100.0   28300   0.010000  |  0.025516    (0.992188)  |  0.000757    (0.999667)  |      34.9 min  
 4.3  102.0   28866   0.010000  |  0.002248    (1.000000)  |  0.002361    (0.999000)  |      35.3 min  
 4.4  104.0   29432   0.010000  |  0.000646    (1.000000)  |  0.000376    (0.999667)  |      35.8 min  
 4.5  106.0   29998   0.010000  |  0.031731    (0.992188)  |  0.003940    (0.999667)  |      36.2 min  
 4.6  108.0   30564   0.010000  |  0.042537    (0.984375)  |  0.001887    (0.999667)  |      36.7 min  
 4.6  110.0   31130   0.010000  |  0.005920    (1.000000)  |  0.003212    (0.999667)  |      37.1 min  
 4.7  112.0   31696   0.010000  |  0.042331    (0.992188)  |  0.005366    (0.999333)  |      37.6 min  
 4.8  114.0   32262   0.010000  |  0.027858    (0.992188)  |  0.003624    (0.999667)  |      38.0 min  
 4.9  116.0   32828   0.010000  |  0.035059    (0.984375)  |  0.002815    (0.999667)  |      38.5 min  
 5.0  118.0   33394   0.010000  |  0.005123    (1.000000)  |  0.000340    (1.000000)  |      38.9 min  
 5.1  120.0   33960   0.010000  |  0.077603    (0.976562)  |  0.000324    (1.000000)  |      41.8 min  
 5.1  122.1   34526   0.010000  |  0.043101    (0.992188)  |  0.002702    (0.999667)  |      42.2 min  
 5.2  124.1   35092   0.010000  |  0.026240    (0.992188)  |  0.004038    (0.999667)  |      42.7 min  
 5.3  126.1   35658   0.010000  |  0.006599    (0.992188)  |  0.001022    (0.999667)  |      43.1 min  
 5.4  128.1   36224   0.010000  |  0.007749    (1.000000)  |  0.001742    (0.999667)  |      43.6 min  
 5.5  130.1   36790   0.010000  |  0.066976    (0.984375)  |  0.002878    (0.999667)  |      44.0 min  
 5.6  132.1   37356   0.010000  |  0.034159    (0.976562)  |  0.000748    (0.999667)  |      44.5 min  
 5.6  134.1   37922   0.010000  |  0.009110    (1.000000)  |  0.001543    (0.999667)  |      44.9 min  
 5.7  136.1   38488   0.010000  |  0.013002    (0.992188)  |  0.000348    (1.000000)  |      45.4 min  
 5.8  138.1   39054   0.010000  |  0.008757    (1.000000)  |  0.002376    (0.999667)  |      45.8 min  
 5.9  140.1   39620   0.010000  |  0.026648    (0.992188)  |  0.001885    (0.999667)  |      46.3 min  
 6.0  142.1   40186   0.010000  |  0.057222    (0.984375)  |  0.000734    (0.999667)  |      46.7 min  
 6.1  144.1   40752   0.001000  |  0.006379    (1.000000)  |  0.000225    (1.000000)  |      49.6 min  
 6.2  146.1   41318   0.001000  |  0.017887    (0.992188)  |  0.000106    (1.000000)  |      50.1 min  
 6.2  148.1   41884   0.001000  |  0.000641    (1.000000)  |  0.000058    (1.000000)  |      50.5 min  
 6.3  150.1   42450   0.001000  |  0.001592    (1.000000)  |  0.000054    (1.000000)  |      51.0 min  
 6.4  152.1   43016   0.001000  |  0.002915    (1.000000)  |  0.000046    (1.000000)  |      51.4 min  
 6.5  154.1   43582   0.001000  |  0.000627    (1.000000)  |  0.000050    (1.000000)  |      51.9 min  
 6.6  156.1   44148   0.001000  |  0.009965    (1.000000)  |  0.000043    (1.000000)  |      52.3 min  
 6.7  158.1   44714   0.001000  |  0.056123    (0.992188)  |  0.000048    (1.000000)  |      52.8 min  
 6.7  160.1   45280   0.001000  |  0.011056    (1.000000)  |  0.000085    (1.000000)  |      53.2 min  
 6.8  162.1   45846   0.001000  |  0.022304    (0.992188)  |  0.000361    (0.999667)  |      53.7 min  
 6.9  164.1   46412   0.001000  |  0.041306    (0.984375)  |  0.000517    (0.999667)  |      54.2 min  
 7.0  166.1   46978   0.001000  |  0.000480    (1.000000)  |  0.000100    (1.000000)  |      54.6 min  
 7.1  168.1   47544   0.001000  |  0.000563    (1.000000)  |  0.000555    (0.999667)  |      57.5 min  
 7.2  170.1   48110   0.001000  |  0.041711    (0.992188)  |  0.000186    (1.000000)  |      58.0 min  
 7.2  172.1   48676   0.001000  |  0.000498    (1.000000)  |  0.000043    (1.000000)  |      58.4 min  
 7.3  174.1   49242   0.001000  |  0.000355    (1.000000)  |  0.000033    (1.000000)  |      58.9 min  
 7.4  176.1   49808   0.001000  |  0.002724    (1.000000)  |  0.000608    (0.999667)  |      59.3 min  
 7.5  178.1   50374   0.001000  |  0.000509    (1.000000)  |  0.000405    (0.999667)  |      59.8 min  
 7.6  180.1   50940   0.001000  |  0.002025    (1.000000)  |  0.000091    (1.000000)  |      60.2 min  
 7.7  182.1   51506   0.001000  |  0.000267    (1.000000)  |  0.001259    (0.999667)  |      60.7 min  
 7.8  184.1   52072   0.001000  |  0.019827    (0.992188)  |  0.000235    (1.000000)  |      61.1 min  
 7.8  186.1   52638   0.001000  |  0.000421    (1.000000)  |  0.001064    (0.999667)  |      61.5 min  
 7.9  188.1   53204   0.001000  |  0.009865    (0.992188)  |  0.000263    (1.000000)  |      62.0 min  
 8.0  190.1   53770   0.000100  |  0.002082    (1.000000)  |  0.000119    (1.000000)  |      64.9 min  
 8.1  192.1   54336   0.000100  |  0.000189    (1.000000)  |  0.000328    (0.999667)  |      65.3 min  
 8.2  194.1   54902   0.000100  |  0.000342    (1.000000)  |  0.000130    (1.000000)  |      65.8 min  
 8.3  196.1   55468   0.000100  |  0.000069    (1.000000)  |  0.000170    (1.000000)  |      66.2 min  
 8.3  198.1   56034   0.000100  |  0.010340    (0.992188)  |  0.000102    (1.000000)  |      66.7 min  
 8.4  200.1   56600   0.000100  |  0.009607    (0.992188)  |  0.000320    (0.999667)  |      67.1 min  
 8.5  202.1   57166   0.000100  |  0.023920    (0.992188)  |  0.000314    (0.999667)  |      67.6 min  
 8.6  204.1   57732   0.000100  |  0.000260    (1.000000)  |  0.000225    (1.000000)  |      68.0 min  
 8.7  206.1   58298   0.000100  |  0.003953    (1.000000)  |  0.000128    (1.000000)  |      68.5 min  
 8.8  208.1   58864   0.000100  |  0.000973    (1.000000)  |  0.000282    (0.999667)  |      68.9 min  
 8.8  210.1   59430   0.000100  |  0.000315    (1.000000)  |  0.000227    (1.000000)  |      69.4 min  
 8.9  212.1   59996   0.000100  |  0.002169    (1.000000)  |  0.000199    (1.000000)  |      69.9 min  

** evaluation on test set **

test_loss=0.014213    (test_acc=0.996358)

sucess
