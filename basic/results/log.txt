--- [START 2017-02-23 23:29:51] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  0.873258    (0.726562)  |  0.555464    (0.812333)  |   1.4 min  
 0.2    4.0   01132   0.100000  |  0.394482    (0.898438)  |  0.117926    (0.962000)  |   1.7 min  
 0.3    6.0   01698   0.100000  |  0.143616    (0.953125)  |  0.073254    (0.976000)  |   2.1 min  
 0.3    8.0   02264   0.100000  |  0.141050    (0.968750)  |  0.076793    (0.973333)  |   2.4 min  
 0.4   10.0   02830   0.100000  |  0.057308    (0.984375)  |  0.047959    (0.984333)  |   2.8 min  
 0.5   12.0   03396   0.100000  |  0.122119    (0.953125)  |  0.022039    (0.993667)  |   3.1 min  
 0.6   14.0   03962   0.100000  |  0.038802    (0.984375)  |  0.024685    (0.993000)  |   3.4 min  
 0.7   16.0   04528   0.100000  |  0.031758    (0.992188)  |  0.018491    (0.995000)  |   3.8 min  
 0.8   18.0   05094   0.100000  |  0.050367    (0.992188)  |  0.012617    (0.996333)  |   4.1 min  
 0.8   20.0   05660   0.100000  |  0.029836    (0.992188)  |  0.026321    (0.991667)  |   4.5 min  
 0.9   22.0   06226   0.100000  |  0.029904    (0.992188)  |  0.010794    (0.997333)  |   4.8 min  
 1.0   24.0   06792   0.100000  |  0.057817    (0.968750)  |  0.012134    (0.996667)  |   6.2 min  
 1.1   26.0   07358   0.100000  |  0.076495    (0.953125)  |  0.015384    (0.995333)  |   6.6 min  
 1.2   28.0   07924   0.100000  |  0.056770    (0.976562)  |  0.009704    (0.996333)  |   6.9 min  
 1.3   30.0   08490   0.100000  |  0.041107    (0.984375)  |  0.011802    (0.996000)  |   7.3 min  
 1.3   32.0   09056   0.100000  |  0.019519    (1.000000)  |  0.014911    (0.994667)  |   7.6 min  
 1.4   34.0   09622   0.100000  |  0.016870    (1.000000)  |  0.007276    (0.997333)  |   7.9 min  
 1.5   36.0   10188   0.100000  |  0.035650    (0.984375)  |  0.006684    (0.997667)  |   8.3 min  
 1.6   38.0   10754   0.100000  |  0.015882    (1.000000)  |  0.027530    (0.989667)  |   8.6 min  
 1.7   40.0   11320   0.100000  |  0.073723    (0.992188)  |  0.008893    (0.997000)  |   8.9 min  
 1.8   42.0   11886   0.100000  |  0.035261    (0.984375)  |  0.004342    (0.999000)  |   9.3 min  
 1.9   44.0   12452   0.100000  |  0.084741    (0.984375)  |  0.007501    (0.998000)  |   9.6 min  
 1.9   46.0   13018   0.100000  |  0.006672    (1.000000)  |  0.009885    (0.996333)  |  10.0 min  
 2.0   48.0   13584   0.100000  |  0.035162    (0.992188)  |  0.010141    (0.997667)  |  11.4 min  
 2.1   50.0   14150   0.100000  |  0.011693    (1.000000)  |  0.006410    (0.998000)  |  11.7 min  
 2.2   52.0   14716   0.100000  |  0.046713    (0.992188)  |  0.002539    (0.999333)  |  12.1 min  
 2.3   54.0   15282   0.100000  |  0.025815    (0.984375)  |  0.005361    (0.998667)  |  12.4 min  
 2.4   56.0   15848   0.100000  |  0.002715    (1.000000)  |  0.002105    (0.999333)  |  12.8 min  
 2.4   58.0   16414   0.100000  |  0.012076    (1.000000)  |  0.003962    (0.998667)  |  13.1 min  
 2.5   60.0   16980   0.100000  |  0.006007    (1.000000)  |  0.003933    (0.998667)  |  13.5 min  
 2.6   62.0   17546   0.100000  |  0.030686    (0.992188)  |  0.003668    (0.999333)  |  13.8 min  
 2.7   64.0   18112   0.100000  |  0.006881    (1.000000)  |  0.004726    (0.998667)  |  14.2 min  
 2.8   66.0   18678   0.100000  |  0.017891    (0.992188)  |  0.004230    (0.999333)  |  14.5 min  
 2.9   68.0   19244   0.100000  |  0.001176    (1.000000)  |  0.002055    (0.999667)  |  14.8 min  
 2.9   70.0   19810   0.100000  |  0.015810    (0.992188)  |  0.004681    (0.998667)  |  15.2 min  
 3.0   72.0   20376   0.010000  |  0.002051    (1.000000)  |  0.002028    (0.999333)  |  16.6 min  
 3.1   74.0   20942   0.010000  |  0.024516    (0.984375)  |  0.002678    (0.999333)  |  17.0 min  
 3.2   76.0   21508   0.010000  |  0.012830    (0.992188)  |  0.002004    (0.999333)  |  17.3 min  
 3.3   78.0   22074   0.010000  |  0.001292    (1.000000)  |  0.002735    (0.998667)  |  17.6 min  
 3.4   80.0   22640   0.010000  |  0.011111    (0.992188)  |  0.002146    (0.999333)  |  18.0 min  
 3.5   82.0   23206   0.010000  |  0.003545    (1.000000)  |  0.002140    (0.999333)  |  18.3 min  
 3.5   84.0   23772   0.010000  |  0.000319    (1.000000)  |  0.001908    (0.999333)  |  18.7 min  
 3.6   86.0   24338   0.010000  |  0.005304    (1.000000)  |  0.002189    (0.999333)  |  19.0 min  
 3.7   88.0   24904   0.010000  |  0.002108    (1.000000)  |  0.002512    (0.999333)  |  19.3 min  
 3.8   90.0   25470   0.010000  |  0.001081    (1.000000)  |  0.002225    (0.999333)  |  19.7 min  
 3.9   92.0   26036   0.010000  |  0.009188    (1.000000)  |  0.002309    (0.999333)  |  20.0 min  
 4.0   94.0   26602   0.010000  |  0.023652    (0.992188)  |  0.002602    (0.999333)  |  20.4 min  
 4.0   96.0   27168   0.010000  |  0.000785    (1.000000)  |  0.002148    (0.999333)  |  21.8 min  
 4.1   98.0   27734   0.010000  |  0.000515    (1.000000)  |  0.001898    (0.999333)  |  22.2 min  
 4.2  100.0   28300   0.010000  |  0.000742    (1.000000)  |  0.002010    (0.999667)  |  22.5 min  
 4.3  102.0   28866   0.010000  |  0.002114    (1.000000)  |  0.002124    (0.999333)  |  22.9 min  
 4.4  104.0   29432   0.010000  |  0.005325    (1.000000)  |  0.002379    (0.999333)  |  23.2 min  
 4.5  106.0   29998   0.010000  |  0.004225    (1.000000)  |  0.002107    (0.999333)  |  23.6 min  
 4.6  108.0   30564   0.010000  |  0.002149    (1.000000)  |  0.001847    (0.999333)  |  23.9 min  
 4.6  110.0   31130   0.010000  |  0.002408    (1.000000)  |  0.002274    (0.999000)  |  24.3 min  
 4.7  112.0   31696   0.010000  |  0.001024    (1.000000)  |  0.002108    (0.999333)  |  24.6 min  
 4.8  114.0   32262   0.010000  |  0.002589    (1.000000)  |  0.002177    (0.999000)  |  25.0 min  
 4.9  116.0   32828   0.010000  |  0.000686    (1.000000)  |  0.002083    (0.999333)  |  25.3 min  
 5.0  118.0   33394   0.010000  |  0.006362    (1.000000)  |  0.002020    (0.999333)  |  25.7 min  
 5.1  120.0   33960   0.010000  |  0.014770    (0.992188)  |  0.001927    (0.999333)  |  27.1 min  
 5.1  122.1   34526   0.010000  |  0.003927    (1.000000)  |  0.002216    (0.999333)  |  27.4 min  
 5.2  124.1   35092   0.010000  |  0.005027    (1.000000)  |  0.002118    (0.999333)  |  27.8 min  
 5.3  126.1   35658   0.010000  |  0.001220    (1.000000)  |  0.002339    (0.999000)  |  28.1 min  
 5.4  128.1   36224   0.010000  |  0.001213    (1.000000)  |  0.002095    (0.999333)  |  28.5 min  
 5.5  130.1   36790   0.010000  |  0.065407    (0.984375)  |  0.002125    (0.999333)  |  28.8 min  
 5.6  132.1   37356   0.010000  |  0.001064    (1.000000)  |  0.002124    (0.999333)  |  29.2 min  
 5.6  134.1   37922   0.010000  |  0.033781    (0.992188)  |  0.001827    (0.999333)  |  29.5 min  
 5.7  136.1   38488   0.010000  |  0.001244    (1.000000)  |  0.001867    (0.999333)  |  29.8 min  
 5.8  138.1   39054   0.010000  |  0.002984    (1.000000)  |  0.001985    (0.999333)  |  30.2 min  
 5.9  140.1   39620   0.010000  |  0.024435    (0.992188)  |  0.001935    (0.999333)  |  30.5 min  
 6.0  142.1   40186   0.010000  |  0.004747    (1.000000)  |  0.002070    (0.999333)  |  30.9 min  
 6.1  144.1   40752   0.001000  |  0.000923    (1.000000)  |  0.002103    (0.999333)  |  32.3 min  
 6.2  146.1   41318   0.001000  |  0.001629    (1.000000)  |  0.001881    (0.999333)  |  32.6 min  
 6.2  148.1   41884   0.001000  |  0.005482    (1.000000)  |  0.001817    (0.999333)  |  33.0 min  
 6.3  150.1   42450   0.001000  |  0.001350    (1.000000)  |  0.002102    (0.999333)  |  33.3 min  
 6.4  152.1   43016   0.001000  |  0.000196    (1.000000)  |  0.001606    (0.999333)  |  33.7 min  
 6.5  154.1   43582   0.001000  |  0.003011    (1.000000)  |  0.001948    (0.999333)  |  34.0 min  
 6.6  156.1   44148   0.001000  |  0.010207    (0.992188)  |  0.002086    (0.999333)  |  34.3 min  
 6.7  158.1   44714   0.001000  |  0.032403    (0.992188)  |  0.001982    (0.999333)  |  34.7 min  
 6.7  160.1   45280   0.001000  |  0.000472    (1.000000)  |  0.001805    (0.999333)  |  35.0 min  
 6.8  162.1   45846   0.001000  |  0.000258    (1.000000)  |  0.001945    (0.999333)  |  35.4 min  
 6.9  164.1   46412   0.001000  |  0.010424    (0.992188)  |  0.001723    (0.999333)  |  35.7 min  
 7.0  166.1   46978   0.001000  |  0.009479    (1.000000)  |  0.001800    (0.999333)  |  36.0 min  
 7.1  168.1   47544   0.001000  |  0.000481    (1.000000)  |  0.001912    (0.999333)  |  37.5 min  
 7.2  170.1   48110   0.001000  |  0.000501    (1.000000)  |  0.002033    (0.999333)  |  37.8 min  
 7.2  172.1   48676   0.001000  |  0.000364    (1.000000)  |  0.001923    (0.999333)  |  38.2 min  
 7.3  174.1   49242   0.001000  |  0.006090    (1.000000)  |  0.001971    (0.999333)  |  38.5 min  
 7.4  176.1   49808   0.001000  |  0.000693    (1.000000)  |  0.002118    (0.999333)  |  38.8 min  
 7.5  178.1   50374   0.001000  |  0.000619    (1.000000)  |  0.001897    (0.999333)  |  39.2 min  
 7.6  180.1   50940   0.001000  |  0.018032    (0.992188)  |  0.001979    (0.999333)  |  39.5 min  
 7.7  182.1   51506   0.001000  |  0.000981    (1.000000)  |  0.002162    (0.999333)  |  39.9 min  
 7.8  184.1   52072   0.001000  |  0.001599    (1.000000)  |  0.002095    (0.999333)  |  40.2 min  
 7.8  186.1   52638   0.001000  |  0.005576    (1.000000)  |  0.002057    (0.999333)  |  40.5 min  
 7.9  188.1   53204   0.001000  |  0.000484    (1.000000)  |  0.001951    (0.999333)  |  40.9 min  
 8.0  190.1   53770   0.000100  |  0.008199    (1.000000)  |  0.002070    (0.999333)  |  42.3 min  
 8.1  192.1   54336   0.000100  |  0.005599    (1.000000)  |  0.001973    (0.999333)  |  42.7 min  
 8.2  194.1   54902   0.000100  |  0.005845    (1.000000)  |  0.002193    (0.999333)  |  43.0 min  
 8.3  196.1   55468   0.000100  |  0.006455    (1.000000)  |  0.001679    (0.999333)  |  43.4 min  
 8.3  198.1   56034   0.000100  |  0.000620    (1.000000)  |  0.001964    (0.999333)  |  43.7 min  
 8.4  200.1   56600   0.000100  |  0.000128    (1.000000)  |  0.002091    (0.999333)  |  44.1 min  
 8.5  202.1   57166   0.000100  |  0.002214    (1.000000)  |  0.002272    (0.999333)  |  44.4 min  
 8.6  204.1   57732   0.000100  |  0.003795    (1.000000)  |  0.001991    (0.999333)  |  44.8 min  
 8.7  206.1   58298   0.000100  |  0.005929    (1.000000)  |  0.002002    (0.999333)  |  45.1 min  
 8.8  208.1   58864   0.000100  |  0.008156    (1.000000)  |  0.001950    (0.999333)  |  45.4 min  
 8.8  210.1   59430   0.000100  |  0.000878    (1.000000)  |  0.002085    (0.999333)  |  45.8 min  
 8.9  212.1   59996   0.000100  |  0.000225    (1.000000)  |  0.002208    (0.999333)  |  46.1 min  

** evaluation on test set **
test_loss=0.114286    (test_acc=0.972051)

sucess
--- [START 2017-02-24 10:56:55] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_2(input_shape=(1, 1, 1), output_shape=(1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block1 = bn(block1)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block2 = bn(block2)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=False)
        block3 = bn(block3)
        block3 = relu(block3)
        block3 = dropout(block3, keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=False)
        block4 = bn(block4)
        block4 = relu(block4)
        block4 = dropout(block4, keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  2.120995    (0.343750)  |  1.365244    (0.554667)  |   1.3 min  
 0.2    4.0   01132   0.100000  |  1.680425    (0.492188)  |  0.868487    (0.705667)  |   1.6 min  
 0.3    6.0   01698   0.100000  |  1.347128    (0.585938)  |  0.546386    (0.821667)  |   1.8 min  
 0.3    8.0   02264   0.100000  |  0.862501    (0.718750)  |  0.363800    (0.869000)  |   2.1 min  
 0.4   10.0   02830   0.100000  |  0.841599    (0.710938)  |  0.295676    (0.901333)  |   2.4 min  
 0.5   12.0   03396   0.100000  |  0.549909    (0.828125)  |  0.204471    (0.932333)  |   2.6 min  
 0.6   14.0   03962   0.100000  |  0.468211    (0.875000)  |  0.161668    (0.946000)  |   2.9 min  
 0.7   16.0   04528   0.100000  |  0.458205    (0.851562)  |  0.169325    (0.941333)  |   3.2 min  
 0.8   18.0   05094   0.100000  |  0.420570    (0.867188)  |  0.108348    (0.970333)  |   3.4 min  
 0.8   20.0   05660   0.100000  |  0.378703    (0.867188)  |  0.130017    (0.953333)  |   3.7 min  
 0.9   22.0   06226   0.100000  |  0.440144    (0.867188)  |  0.078833    (0.978667)  |   4.0 min  
 1.0   24.0   06792   0.100000  |  0.448602    (0.867188)  |  0.055725    (0.988000)  |   5.3 min  
 1.1   26.0   07358   0.100000  |  0.383091    (0.882812)  |  0.044738    (0.987667)  |   5.6 min  
 1.2   28.0   07924   0.100000  |  0.350142    (0.882812)  |  0.072395    (0.975000)  |   5.8 min  
 1.3   30.0   08490   0.100000  |  0.355426    (0.882812)  |  0.048564    (0.988667)  |   6.1 min  
 1.3   32.0   09056   0.100000  |  0.336644    (0.914062)  |  0.039499    (0.989000)  |   6.3 min  
 1.4   34.0   09622   0.100000  |  0.311128    (0.914062)  |  0.048379    (0.985667)  |   6.6 min  
 1.5   36.0   10188   0.100000  |  0.246918    (0.914062)  |  0.048050    (0.985000)  |   6.9 min  
 1.6   38.0   10754   0.100000  |  0.285615    (0.945312)  |  0.020264    (0.995333)  |   7.1 min  
 1.7   40.0   11320   0.100000  |  0.333736    (0.898438)  |  0.042542    (0.989333)  |   7.4 min  
 1.8   42.0   11886   0.100000  |  0.234199    (0.914062)  |  0.022172    (0.995000)  |   7.7 min  
 1.9   44.0   12452   0.100000  |  0.282444    (0.929688)  |  0.028265    (0.992000)  |   7.9 min  
 1.9   46.0   13018   0.100000  |  0.194804    (0.921875)  |  0.016943    (0.996667)  |   8.2 min  
 2.0   48.0   13584   0.100000  |  0.181306    (0.937500)  |  0.028853    (0.990667)  |   9.5 min  
 2.1   50.0   14150   0.100000  |  0.215641    (0.968750)  |  0.009626    (0.998333)  |   9.8 min  
 2.2   52.0   14716   0.100000  |  0.233235    (0.937500)  |  0.030839    (0.989667)  |  10.1 min  
 2.3   54.0   15282   0.100000  |  0.178690    (0.937500)  |  0.015452    (0.996333)  |  10.3 min  
 2.4   56.0   15848   0.100000  |  0.107638    (0.960938)  |  0.021741    (0.995333)  |  10.6 min  
 2.4   58.0   16414   0.100000  |  0.350262    (0.914062)  |  0.009070    (0.997333)  |  10.8 min  
 2.5   60.0   16980   0.100000  |  0.249297    (0.945312)  |  0.036069    (0.988667)  |  11.1 min  
 2.6   62.0   17546   0.100000  |  0.178970    (0.968750)  |  0.029864    (0.991333)  |  11.4 min  
 2.7   64.0   18112   0.100000  |  0.117920    (0.953125)  |  0.018617    (0.994667)  |  11.6 min  
 2.8   66.0   18678   0.100000  |  0.156406    (0.968750)  |  0.011576    (0.997667)  |  11.9 min  
 2.9   68.0   19244   0.100000  |  0.278698    (0.929688)  |  0.029828    (0.993000)  |  12.2 min  
 2.9   70.0   19810   0.100000  |  0.184152    (0.945312)  |  0.011833    (0.995000)  |  12.5 min  
 3.0   72.0   20376   0.010000  |  0.070646    (0.984375)  |  0.004218    (0.999333)  |  13.8 min  
 3.1   74.0   20942   0.010000  |  0.179514    (0.960938)  |  0.003120    (0.999667)  |  14.1 min  
 3.2   76.0   21508   0.010000  |  0.056732    (0.976562)  |  0.002596    (0.999667)  |  14.3 min  
 3.3   78.0   22074   0.010000  |  0.116493    (0.968750)  |  0.003407    (0.999333)  |  14.6 min  
 3.4   80.0   22640   0.010000  |  0.080649    (0.976562)  |  0.002462    (0.999333)  |  14.9 min  
 3.5   82.0   23206   0.010000  |  0.121606    (0.968750)  |  0.002438    (0.999333)  |  15.1 min  
 3.5   84.0   23772   0.010000  |  0.090021    (0.960938)  |  0.002152    (0.999333)  |  15.4 min  
 3.6   86.0   24338   0.010000  |  0.171893    (0.960938)  |  0.001527    (1.000000)  |  15.7 min  
 3.7   88.0   24904   0.010000  |  0.113417    (0.953125)  |  0.002606    (0.999667)  |  15.9 min  
 3.8   90.0   25470   0.010000  |  0.039546    (0.992188)  |  0.003068    (0.999000)  |  16.2 min  
 3.9   92.0   26036   0.010000  |  0.046320    (0.976562)  |  0.002873    (0.999333)  |  16.5 min  
 4.0   94.0   26602   0.010000  |  0.111058    (0.968750)  |  0.002128    (0.999333)  |  16.8 min  
 4.0   96.0   27168   0.010000  |  0.034913    (0.992188)  |  0.002031    (0.999667)  |  18.1 min  
 4.1   98.0   27734   0.010000  |  0.149604    (0.960938)  |  0.001871    (0.999667)  |  18.4 min  
 4.2  100.0   28300   0.010000  |  0.052278    (0.984375)  |  0.002262    (0.999667)  |  18.6 min  
 4.3  102.0   28866   0.010000  |  0.090331    (0.976562)  |  0.002411    (0.999333)  |  18.9 min  
 4.4  104.0   29432   0.010000  |  0.150157    (0.976562)  |  0.002210    (0.999667)  |  19.2 min  
 4.5  106.0   29998   0.010000  |  0.177227    (0.945312)  |  0.001770    (0.999667)  |  19.5 min  
 4.6  108.0   30564   0.010000  |  0.085769    (0.976562)  |  0.001896    (0.999667)  |  19.7 min  
 4.6  110.0   31130   0.010000  |  0.060943    (0.992188)  |  0.001077    (1.000000)  |  20.0 min  
 4.7  112.0   31696   0.010000  |  0.042190    (0.992188)  |  0.001592    (0.999667)  |  20.3 min  
 4.8  114.0   32262   0.010000  |  0.156366    (0.945312)  |  0.002209    (0.999667)  |  20.5 min  
 4.9  116.0   32828   0.010000  |  0.036902    (0.992188)  |  0.002716    (0.999333)  |  20.8 min  
 5.0  118.0   33394   0.010000  |  0.052136    (0.984375)  |  0.002380    (0.999333)  |  21.1 min  
 5.1  120.0   33960   0.010000  |  0.061263    (0.984375)  |  0.001583    (0.999667)  |  22.5 min  
 5.1  122.1   34526   0.010000  |  0.037916    (0.984375)  |  0.001819    (0.999667)  |  22.7 min  
 5.2  124.1   35092   0.010000  |  0.270336    (0.953125)  |  0.002232    (0.999333)  |  23.0 min  
 5.3  126.1   35658   0.010000  |  0.074333    (0.984375)  |  0.002483    (0.999333)  |  23.3 min  
 5.4  128.1   36224   0.010000  |  0.044281    (0.992188)  |  0.002382    (0.999667)  |  23.5 min  
 5.5  130.1   36790   0.010000  |  0.182181    (0.945312)  |  0.002132    (0.999667)  |  23.8 min  
 5.6  132.1   37356   0.010000  |  0.081239    (0.960938)  |  0.002296    (0.999333)  |  24.1 min  
 5.6  134.1   37922   0.010000  |  0.162987    (0.960938)  |  0.002175    (0.999333)  |  24.4 min  
 5.7  136.1   38488   0.010000  |  0.132408    (0.945312)  |  0.001759    (0.999333)  |  24.7 min  
 5.8  138.1   39054   0.010000  |  0.060973    (0.976562)  |  0.001331    (0.999667)  |  24.9 min  
 5.9  140.1   39620   0.010000  |  0.090958    (0.968750)  |  0.002352    (0.999333)  |  25.2 min  
 6.0  142.1   40186   0.010000  |  0.179474    (0.929688)  |  0.002082    (0.999333)  |  25.5 min  
 6.1  144.1   40752   0.001000  |  0.112248    (0.968750)  |  0.002738    (0.999333)  |  26.8 min  
 6.2  146.1   41318   0.001000  |  0.030390    (0.992188)  |  0.002400    (0.999333)  |  27.1 min  
 6.2  148.1   41884   0.001000  |  0.022595    (1.000000)  |  0.002381    (0.999333)  |  27.3 min  
 6.3  150.1   42450   0.001000  |  0.084158    (0.976562)  |  0.002191    (0.999333)  |  27.6 min  
 6.4  152.1   43016   0.001000  |  0.135731    (0.937500)  |  0.002278    (0.999667)  |  27.9 min  
 6.5  154.1   43582   0.001000  |  0.039138    (0.992188)  |  0.002027    (0.999667)  |  28.2 min  
 6.6  156.1   44148   0.001000  |  0.045157    (0.984375)  |  0.001998    (0.999667)  |  28.4 min  
 6.7  158.1   44714   0.001000  |  0.154000    (0.968750)  |  0.001849    (0.999667)  |  28.7 min  
 6.7  160.1   45280   0.001000  |  0.014401    (1.000000)  |  0.002014    (0.999667)  |  29.0 min  
 6.8  162.1   45846   0.001000  |  0.015404    (1.000000)  |  0.002196    (0.999667)  |  29.3 min  
 6.9  164.1   46412   0.001000  |  0.060484    (0.976562)  |  0.002255    (0.999333)  |  29.5 min  
 7.0  166.1   46978   0.001000  |  0.108857    (0.968750)  |  0.001953    (0.999667)  |  29.8 min  
 7.1  168.1   47544   0.001000  |  0.056530    (0.976562)  |  0.001938    (0.999667)  |  31.2 min  
 7.2  170.1   48110   0.001000  |  0.022028    (1.000000)  |  0.001848    (0.999667)  |  31.5 min  
 7.2  172.1   48676   0.001000  |  0.145550    (0.976562)  |  0.002006    (0.999667)  |  31.7 min  
 7.3  174.1   49242   0.001000  |  0.065075    (0.984375)  |  0.001940    (0.999333)  |  32.0 min  
 7.4  176.1   49808   0.001000  |  0.020949    (1.000000)  |  0.002204    (0.999667)  |  32.3 min  
 7.5  178.1   50374   0.001000  |  0.048489    (0.992188)  |  0.002109    (0.999667)  |  32.5 min  
 7.6  180.1   50940   0.001000  |  0.119608    (0.976562)  |  0.001823    (0.999667)  |  32.8 min  
 7.7  182.1   51506   0.001000  |  0.122724    (0.976562)  |  0.002195    (0.999667)  |  33.1 min  
 7.8  184.1   52072   0.001000  |  0.024450    (1.000000)  |  0.002216    (0.999667)  |  33.4 min  
 7.8  186.1   52638   0.001000  |  0.096470    (0.984375)  |  0.002529    (0.999333)  |  33.6 min  
 7.9  188.1   53204   0.001000  |  0.065361    (0.992188)  |  0.002283    (0.999333)  |  33.9 min  
 8.0  190.1   53770   0.000100  |  0.083831    (0.976562)  |  0.002354    (0.999667)  |  35.3 min  
 8.1  192.1   54336   0.000100  |  0.064167    (0.992188)  |  0.002236    (0.999667)  |  35.5 min  
 8.2  194.1   54902   0.000100  |  0.117403    (0.953125)  |  0.002286    (0.999667)  |  35.8 min  
 8.3  196.1   55468   0.000100  |  0.062923    (0.984375)  |  0.002069    (0.999667)  |  36.1 min  
 8.3  198.1   56034   0.000100  |  0.118459    (0.968750)  |  0.002362    (0.999667)  |  36.4 min  
 8.4  200.1   56600   0.000100  |  0.043081    (0.984375)  |  0.002301    (0.999667)  |  36.6 min  
 8.5  202.1   57166   0.000100  |  0.063465    (0.976562)  |  0.002478    (0.999333)  |  36.9 min  
 8.6  204.1   57732   0.000100  |  0.041933    (1.000000)  |  0.002259    (0.999667)  |  37.2 min  
 8.7  206.1   58298   0.000100  |  0.112500    (0.960938)  |  0.002200    (0.999667)  |  37.5 min  
 8.8  208.1   58864   0.000100  |  0.066481    (0.976562)  |  0.002352    (0.999667)  |  37.7 min  
 8.8  210.1   59430   0.000100  |  0.055528    (0.992188)  |  0.002365    (0.999667)  |  38.0 min  
 8.9  212.1   59996   0.000100  |  0.085730    (0.976562)  |  0.002551    (0.999667)  |  38.3 min  
 9.0  213.7   60453   0.000100  |  0.127443    (0.953125)  |  0.002384    (0.999667)  |  38.5 min  

** evaluation on test set **
test_loss=0.085179    (test_acc=0.977910)

sucess
--- [START 2017-02-24 15:19:30] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

--- [START 2017-02-24 16:14:44] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

--- [START 2017-02-24 16:34:28] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

--- [START 2017-02-24 16:35:31] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

--- [START 2017-02-24 16:40:45] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

--- [START 2017-02-24 16:57:42] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_1( input_shape=(1,1,1), output_shape = (1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=True)
        block3 = relu(block3)
        block3 = dropout(block3,keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)
        block4 = dropout(block4,keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  nan    (0.039062)  |  nan    (0.003000)  |   1.2 min  
 0.2    4.0   01132   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   1.4 min  
 0.3    6.0   01698   0.100000  |  nan    (0.015625)  |  nan    (0.003000)  |   1.6 min  
 0.3    8.0   02264   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   1.8 min  
 0.4   10.0   02830   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   1.9 min  
 0.5   12.0   03396   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   2.1 min  
 0.6   14.0   03962   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   2.3 min  
 0.7   16.0   04528   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   2.4 min  
 0.8   18.0   05094   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   2.6 min  
 0.8   20.0   05660   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   2.8 min  
 0.9   22.0   06226   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   2.9 min  
 1.0   24.0   06792   0.100000  |  nan    (0.046875)  |  nan    (0.003000)  |   4.1 min  
 1.1   26.0   07358   0.100000  |  nan    (0.046875)  |  nan    (0.003000)  |   4.3 min  
 1.2   28.0   07924   0.100000  |  nan    (0.015625)  |  nan    (0.003000)  |   4.5 min  
 1.3   30.0   08490   0.100000  |  nan    (0.015625)  |  nan    (0.003000)  |   4.6 min  
 1.3   32.0   09056   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   4.8 min  
 1.4   34.0   09622   0.100000  |  nan    (0.000000)  |  nan    (0.003000)  |   5.0 min  
 1.5   36.0   10188   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   5.1 min  
 1.6   38.0   10754   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   5.3 min  
 1.7   40.0   11320   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   5.5 min  
 1.8   42.0   11886   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   5.7 min  
 1.9   44.0   12452   0.100000  |  nan    (0.015625)  |  nan    (0.003000)  |   5.8 min  
 1.9   46.0   13018   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   6.0 min  
 2.0   48.0   13584   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   7.2 min  
 2.1   50.0   14150   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   7.4 min  
 2.2   52.0   14716   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   7.5 min  
 2.3   54.0   15282   0.100000  |  nan    (0.007812)  |  nan    (0.003000)  |   7.7 min  
 2.4   56.0   15848   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   7.9 min  
 2.4   58.0   16414   0.100000  |  nan    (0.039062)  |  nan    (0.003000)  |   8.0 min  
 2.5   60.0   16980   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   8.2 min  
 2.6   62.0   17546   0.100000  |  nan    (0.023438)  |  nan    (0.003000)  |   8.4 min  
 2.7   64.0   18112   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   8.5 min  
 2.8   66.0   18678   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   8.7 min  
 2.9   68.0   19244   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   8.9 min  
 2.9   70.0   19810   0.100000  |  nan    (0.031250)  |  nan    (0.003000)  |   9.0 min  
 3.0   72.0   20376   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  10.2 min  
 3.1   74.0   20942   0.010000  |  nan    (0.031250)  |  nan    (0.003000)  |  10.4 min  
 3.2   76.0   21508   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  10.6 min  
 3.3   78.0   22074   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  10.7 min  
 3.4   80.0   22640   0.010000  |  nan    (0.015625)  |  nan    (0.003000)  |  10.9 min  
 3.5   82.0   23206   0.010000  |  nan    (0.046875)  |  nan    (0.003000)  |  11.0 min  
 3.5   84.0   23772   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  11.2 min  
 3.6   86.0   24338   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  11.4 min  
 3.7   88.0   24904   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  11.5 min  
 3.8   90.0   25470   0.010000  |  nan    (0.015625)  |  nan    (0.003000)  |  11.7 min  
 3.9   92.0   26036   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  11.9 min  
 4.0   94.0   26602   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  12.1 min  
 4.0   96.0   27168   0.010000  |  nan    (0.031250)  |  nan    (0.003000)  |  13.3 min  
 4.1   98.0   27734   0.010000  |  nan    (0.031250)  |  nan    (0.003000)  |  13.4 min  
 4.2  100.0   28300   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  13.6 min  
 4.3  102.0   28866   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  13.8 min  
 4.4  104.0   29432   0.010000  |  nan    (0.015625)  |  nan    (0.003000)  |  14.0 min  
 4.5  106.0   29998   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  14.1 min  
 4.6  108.0   30564   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  14.3 min  
 4.6  110.0   31130   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  14.5 min  
 4.7  112.0   31696   0.010000  |  nan    (0.015625)  |  nan    (0.003000)  |  14.6 min  
 4.8  114.0   32262   0.010000  |  nan    (0.015625)  |  nan    (0.003000)  |  14.8 min  
 4.9  116.0   32828   0.010000  |  nan    (0.000000)  |  nan    (0.003000)  |  15.0 min  
 5.0  118.0   33394   0.010000  |  nan    (0.039062)  |  nan    (0.003000)  |  15.1 min  
 5.1  120.0   33960   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  16.3 min  
 5.1  122.1   34526   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  16.5 min  
 5.2  124.1   35092   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  16.7 min  
 5.3  126.1   35658   0.010000  |  nan    (0.031250)  |  nan    (0.003000)  |  16.8 min  
 5.4  128.1   36224   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  17.0 min  
 5.5  130.1   36790   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  17.2 min  
 5.6  132.1   37356   0.010000  |  nan    (0.007812)  |  nan    (0.003000)  |  17.3 min  
 5.6  134.1   37922   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  17.5 min  
 5.7  136.1   38488   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  17.7 min  
 5.8  138.1   39054   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  17.8 min  
 5.9  140.1   39620   0.010000  |  nan    (0.031250)  |  nan    (0.003000)  |  18.0 min  
 6.0  142.1   40186   0.010000  |  nan    (0.023438)  |  nan    (0.003000)  |  18.2 min  
 6.1  144.1   40752   0.001000  |  nan    (0.000000)  |  nan    (0.003000)  |  19.4 min  
 6.2  146.1   41318   0.001000  |  nan    (0.031250)  |  nan    (0.003000)  |  19.5 min  
 6.2  148.1   41884   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  19.7 min  
 6.3  150.1   42450   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  19.9 min  
 6.4  152.1   43016   0.001000  |  nan    (0.007812)  |  nan    (0.003000)  |  20.1 min  
 6.5  154.1   43582   0.001000  |  nan    (0.007812)  |  nan    (0.003000)  |  20.2 min  
 6.6  156.1   44148   0.001000  |  nan    (0.031250)  |  nan    (0.003000)  |  20.4 min  
 6.7  158.1   44714   0.001000  |  nan    (0.031250)  |  nan    (0.003000)  |  20.6 min  
 6.7  160.1   45280   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  20.7 min  
 6.8  162.1   45846   0.001000  |  nan    (0.023438)  |  nan    (0.003000)  |  20.9 min  
 6.9  164.1   46412   0.001000  |  nan    (0.023438)  |  nan    (0.003000)  |  21.1 min  
 7.0  166.1   46978   0.001000  |  nan    (0.039062)  |  nan    (0.003000)  |  21.2 min  
 7.1  168.1   47544   0.001000  |  nan    (0.023438)  |  nan    (0.003000)  |  22.4 min  
 7.2  170.1   48110   0.001000  |  nan    (0.007812)  |  nan    (0.003000)  |  22.6 min  
 7.2  172.1   48676   0.001000  |  nan    (0.023438)  |  nan    (0.003000)  |  22.8 min  
 7.3  174.1   49242   0.001000  |  nan    (0.023438)  |  nan    (0.003000)  |  22.9 min  
 7.4  176.1   49808   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  23.1 min  
 7.5  178.1   50374   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  23.3 min  
 7.6  180.1   50940   0.001000  |  nan    (0.007812)  |  nan    (0.003000)  |  23.4 min  
 7.7  182.1   51506   0.001000  |  nan    (0.046875)  |  nan    (0.003000)  |  23.6 min  
 7.8  184.1   52072   0.001000  |  nan    (0.031250)  |  nan    (0.003000)  |  23.8 min  
 7.8  186.1   52638   0.001000  |  nan    (0.031250)  |  nan    (0.003000)  |  24.0 min  
 7.9  188.1   53204   0.001000  |  nan    (0.015625)  |  nan    (0.003000)  |  24.1 min  
 8.0  190.1   53770   0.000100  |  nan    (0.015625)  |  nan    (0.003000)  |  25.3 min  
 8.1  192.1   54336   0.000100  |  nan    (0.007812)  |  nan    (0.003000)  |  25.5 min  
 8.2  194.1   54902   0.000100  |  nan    (0.039062)  |  nan    (0.003000)  |  25.7 min  
 8.3  196.1   55468   0.000100  |  nan    (0.031250)  |  nan    (0.003000)  |  25.8 min  
 8.3  198.1   56034   0.000100  |  nan    (0.015625)  |  nan    (0.003000)  |  26.0 min  
 8.4  200.1   56600   0.000100  |  nan    (0.015625)  |  nan    (0.003000)  |  26.2 min  
 8.5  202.1   57166   0.000100  |  nan    (0.007812)  |  nan    (0.003000)  |  26.3 min  
 8.6  204.1   57732   0.000100  |  nan    (0.039062)  |  nan    (0.003000)  |  26.5 min  
 8.7  206.1   58298   0.000100  |  nan    (0.054688)  |  nan    (0.003000)  |  26.7 min  
 8.8  208.1   58864   0.000100  |  nan    (0.054688)  |  nan    (0.003000)  |  26.8 min  
 8.8  210.1   59430   0.000100  |  nan    (0.000000)  |  nan    (0.003000)  |  27.0 min  
 8.9  212.1   59996   0.000100  |  nan    (0.007812)  |  nan    (0.003000)  |  27.2 min  
 9.0  213.7   60453   0.000100  |  nan    (0.015625)  |  nan    (0.003000)  |  27.3 min  

** evaluation on test set **
test_loss=nan    (test_acc=0.004751)

sucess
--- [START 2017-02-24 23:27:04] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_1( input_shape=(1,1,1), output_shape = (1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=True)
        block3 = relu(block3)
        block3 = dropout(block3,keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)
        block4 = dropout(block4,keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  3.560298    (0.039062)  |  3.527604    (0.072667)  |   1.2 min  
 0.2    4.0   01132   0.010000  |  3.317884    (0.054688)  |  3.197207    (0.103000)  |   1.3 min  
 0.3    6.0   01698   0.010000  |  3.005768    (0.085938)  |  2.695368    (0.225000)  |   1.5 min  
 0.3    8.0   02264   0.010000  |  2.741473    (0.171875)  |  2.202375    (0.347667)  |   1.7 min  
 0.4   10.0   02830   0.010000  |  2.376663    (0.304688)  |  1.939158    (0.395667)  |   1.8 min  
 0.5   12.0   03396   0.010000  |  2.284436    (0.250000)  |  1.670757    (0.456000)  |   2.0 min  
 0.6   14.0   03962   0.010000  |  2.249002    (0.289062)  |  1.555849    (0.467667)  |   2.2 min  
 0.7   16.0   04528   0.010000  |  1.970675    (0.328125)  |  1.422157    (0.505000)  |   2.3 min  
 0.8   18.0   05094   0.010000  |  1.843835    (0.421875)  |  1.349148    (0.544667)  |   2.5 min  
 0.8   20.0   05660   0.010000  |  2.120304    (0.351562)  |  1.255794    (0.544333)  |   2.7 min  
 0.9   22.0   06226   0.010000  |  1.705116    (0.398438)  |  1.111946    (0.615333)  |   2.8 min  
 1.0   24.0   06792   0.010000  |  1.619609    (0.406250)  |  1.055019    (0.607667)  |   4.0 min  
 1.1   26.0   07358   0.010000  |  1.575036    (0.468750)  |  0.982980    (0.669000)  |   4.2 min  
 1.2   28.0   07924   0.010000  |  1.527467    (0.476562)  |  0.933908    (0.653000)  |   4.3 min  
 1.3   30.0   08490   0.010000  |  1.166650    (0.562500)  |  0.799068    (0.722000)  |   4.5 min  
 1.3   32.0   09056   0.010000  |  1.243427    (0.492188)  |  0.697333    (0.752333)  |   4.7 min  
 1.4   34.0   09622   0.010000  |  1.146806    (0.609375)  |  0.646547    (0.767667)  |   4.8 min  
 1.5   36.0   10188   0.010000  |  1.182441    (0.601562)  |  0.598934    (0.790667)  |   5.0 min  
 1.6   38.0   10754   0.010000  |  1.035945    (0.648438)  |  0.478043    (0.839667)  |   5.2 min  
 1.7   40.0   11320   0.010000  |  1.174863    (0.671875)  |  0.418646    (0.872667)  |   5.4 min  
 1.8   42.0   11886   0.010000  |  0.934377    (0.679688)  |  0.399664    (0.883000)  |   5.5 min  
 1.9   44.0   12452   0.010000  |  0.852244    (0.710938)  |  0.299644    (0.912667)  |   5.7 min  
 1.9   46.0   13018   0.010000  |  0.734182    (0.773438)  |  0.259571    (0.929000)  |   5.9 min  
 2.0   48.0   13584   0.010000  |  0.616072    (0.796875)  |  0.251448    (0.932667)  |   7.0 min  
 2.1   50.0   14150   0.010000  |  0.631721    (0.789062)  |  0.235253    (0.933000)  |   7.2 min  
 2.2   52.0   14716   0.010000  |  0.599756    (0.812500)  |  0.216663    (0.929000)  |   7.4 min  
 2.3   54.0   15282   0.010000  |  0.823308    (0.750000)  |  0.198555    (0.932333)  |   7.5 min  
 2.4   56.0   15848   0.010000  |  0.529280    (0.757812)  |  0.175112    (0.956000)  |   7.7 min  
 2.4   58.0   16414   0.010000  |  0.703655    (0.781250)  |  0.139483    (0.956333)  |   7.9 min  
 2.5   60.0   16980   0.010000  |  0.300843    (0.875000)  |  0.140762    (0.960667)  |   8.0 min  
 2.6   62.0   17546   0.010000  |  0.360076    (0.875000)  |  0.136295    (0.955667)  |   8.2 min  
 2.7   64.0   18112   0.010000  |  0.477909    (0.820312)  |  0.126298    (0.966333)  |   8.4 min  
 2.8   66.0   18678   0.010000  |  0.402300    (0.882812)  |  0.115059    (0.971667)  |   8.6 min  
 2.9   68.0   19244   0.010000  |  0.295937    (0.914062)  |  0.104875    (0.970667)  |   8.7 min  
 2.9   70.0   19810   0.010000  |  0.401270    (0.898438)  |  0.087168    (0.983000)  |   8.9 min  
 3.0   72.0   20376   0.001000  |  0.220246    (0.929688)  |  0.061542    (0.988333)  |  10.1 min  
 3.1   74.0   20942   0.001000  |  0.357358    (0.875000)  |  0.055829    (0.986667)  |  10.2 min  
 3.2   76.0   21508   0.001000  |  0.286711    (0.914062)  |  0.052032    (0.989333)  |  10.4 min  
 3.3   78.0   22074   0.001000  |  0.353174    (0.914062)  |  0.049436    (0.991333)  |  10.6 min  
 3.4   80.0   22640   0.001000  |  0.220999    (0.914062)  |  0.044769    (0.991333)  |  10.7 min  
 3.5   82.0   23206   0.001000  |  0.339878    (0.914062)  |  0.050979    (0.989000)  |  10.9 min  
 3.5   84.0   23772   0.001000  |  0.185658    (0.937500)  |  0.044675    (0.989667)  |  11.1 min  
 3.6   86.0   24338   0.001000  |  0.207958    (0.914062)  |  0.041709    (0.991000)  |  11.3 min  
 3.7   88.0   24904   0.001000  |  0.253053    (0.914062)  |  0.042554    (0.990667)  |  11.4 min  
 3.8   90.0   25470   0.001000  |  0.246511    (0.906250)  |  0.041586    (0.990667)  |  11.6 min  
 3.9   92.0   26036   0.001000  |  0.259906    (0.914062)  |  0.042738    (0.991000)  |  11.8 min  
 4.0   94.0   26602   0.001000  |  0.248945    (0.898438)  |  0.041897    (0.991000)  |  12.0 min  
 4.0   96.0   27168   0.001000  |  0.206090    (0.890625)  |  0.039310    (0.991333)  |  13.1 min  
 4.1   98.0   27734   0.001000  |  0.367654    (0.898438)  |  0.042417    (0.991000)  |  13.3 min  
 4.2  100.0   28300   0.001000  |  0.293120    (0.937500)  |  0.041218    (0.992000)  |  13.5 min  
 4.3  102.0   28866   0.001000  |  0.284265    (0.882812)  |  0.041292    (0.991000)  |  13.6 min  
 4.4  104.0   29432   0.001000  |  0.250770    (0.906250)  |  0.037639    (0.991333)  |  13.8 min  
 4.5  106.0   29998   0.001000  |  0.452398    (0.882812)  |  0.037460    (0.989667)  |  14.0 min  
 4.6  108.0   30564   0.001000  |  0.167850    (0.945312)  |  0.039819    (0.992333)  |  14.1 min  
 4.6  110.0   31130   0.001000  |  0.223808    (0.945312)  |  0.035865    (0.992000)  |  14.3 min  
 4.7  112.0   31696   0.001000  |  0.252299    (0.898438)  |  0.036704    (0.991333)  |  14.5 min  
 4.8  114.0   32262   0.001000  |  0.334087    (0.890625)  |  0.040412    (0.990000)  |  14.7 min  
 4.9  116.0   32828   0.001000  |  0.246639    (0.937500)  |  0.036658    (0.992667)  |  14.8 min  
 5.0  118.0   33394   0.001000  |  0.159922    (0.953125)  |  0.036130    (0.991667)  |  15.0 min  
 5.1  120.0   33960   0.001000  |  0.348407    (0.890625)  |  0.035076    (0.992000)  |  16.2 min  
 5.1  122.1   34526   0.001000  |  0.190685    (0.921875)  |  0.032420    (0.992333)  |  16.3 min  
 5.2  124.1   35092   0.001000  |  0.173588    (0.953125)  |  0.031499    (0.992667)  |  16.5 min  
 5.3  126.1   35658   0.001000  |  0.199904    (0.921875)  |  0.032729    (0.992000)  |  16.7 min  
 5.4  128.1   36224   0.001000  |  0.193251    (0.953125)  |  0.034263    (0.993000)  |  16.8 min  
 5.5  130.1   36790   0.001000  |  0.244459    (0.937500)  |  0.035301    (0.992333)  |  17.0 min  
 5.6  132.1   37356   0.001000  |  0.198834    (0.937500)  |  0.029694    (0.992667)  |  17.2 min  
 5.6  134.1   37922   0.001000  |  0.313846    (0.921875)  |  0.033779    (0.993000)  |  17.4 min  
 5.7  136.1   38488   0.001000  |  0.348594    (0.898438)  |  0.031701    (0.992667)  |  17.5 min  
 5.8  138.1   39054   0.001000  |  0.173491    (0.945312)  |  0.031181    (0.992333)  |  17.7 min  
 5.9  140.1   39620   0.001000  |  0.152225    (0.945312)  |  0.032473    (0.991667)  |  17.9 min  
 6.0  142.1   40186   0.001000  |  0.201223    (0.929688)  |  0.028866    (0.992333)  |  18.1 min  
 6.1  144.1   40752   0.000100  |  0.094756    (0.976562)  |  0.029518    (0.992667)  |  19.2 min  
 6.2  146.1   41318   0.000100  |  0.248650    (0.945312)  |  0.028399    (0.992667)  |  19.4 min  
 6.2  148.1   41884   0.000100  |  0.220898    (0.929688)  |  0.028189    (0.992667)  |  19.6 min  
 6.3  150.1   42450   0.000100  |  0.204000    (0.945312)  |  0.028363    (0.993000)  |  19.7 min  
 6.4  152.1   43016   0.000100  |  0.252958    (0.937500)  |  0.028014    (0.992667)  |  19.9 min  
 6.5  154.1   43582   0.000100  |  0.207579    (0.914062)  |  0.028129    (0.993000)  |  20.1 min  
 6.6  156.1   44148   0.000100  |  0.114865    (0.968750)  |  0.028139    (0.993667)  |  20.3 min  
 6.7  158.1   44714   0.000100  |  0.141907    (0.937500)  |  0.027556    (0.992667)  |  20.4 min  
 6.7  160.1   45280   0.000100  |  0.180240    (0.960938)  |  0.027537    (0.993000)  |  20.6 min  
 6.8  162.1   45846   0.000100  |  0.132495    (0.953125)  |  0.028032    (0.992667)  |  20.8 min  
 6.9  164.1   46412   0.000100  |  0.162526    (0.937500)  |  0.027749    (0.993000)  |  21.0 min  
 7.0  166.1   46978   0.000100  |  0.235391    (0.929688)  |  0.027428    (0.993333)  |  21.1 min  
 7.1  168.1   47544   0.000100  |  0.192319    (0.953125)  |  0.027468    (0.993000)  |  22.3 min  
 7.2  170.1   48110   0.000100  |  0.113973    (0.960938)  |  0.027221    (0.993333)  |  22.5 min  
 7.2  172.1   48676   0.000100  |  0.161808    (0.984375)  |  0.027848    (0.993667)  |  22.6 min  
 7.3  174.1   49242   0.000100  |  0.217070    (0.960938)  |  0.027372    (0.993333)  |  22.8 min  
 7.4  176.1   49808   0.000100  |  0.164931    (0.953125)  |  0.027741    (0.993333)  |  23.0 min  
 7.5  178.1   50374   0.000100  |  0.108043    (0.945312)  |  0.027221    (0.993000)  |  23.1 min  
 7.6  180.1   50940   0.000100  |  0.214302    (0.937500)  |  0.027242    (0.993333)  |  23.3 min  
 7.7  182.1   51506   0.000100  |  0.155704    (0.953125)  |  0.027312    (0.993333)  |  23.5 min  
 7.8  184.1   52072   0.000100  |  0.199890    (0.929688)  |  0.027177    (0.993333)  |  23.7 min  
 7.8  186.1   52638   0.000100  |  0.225656    (0.898438)  |  0.027341    (0.993000)  |  23.8 min  
 7.9  188.1   53204   0.000100  |  0.169374    (0.968750)  |  0.027351    (0.993000)  |  24.0 min  
 8.0  190.1   53770   0.000100  |  0.263781    (0.914062)  |  0.027268    (0.993667)  |  25.2 min  
 8.1  192.1   54336   0.000100  |  0.266918    (0.914062)  |  0.027331    (0.993333)  |  25.4 min  
 8.2  194.1   54902   0.000100  |  0.177735    (0.945312)  |  0.026935    (0.993333)  |  25.5 min  
 8.3  196.1   55468   0.000100  |  0.226472    (0.953125)  |  0.026628    (0.993333)  |  25.7 min  
 8.3  198.1   56034   0.000100  |  0.244110    (0.953125)  |  0.026715    (0.993667)  |  25.9 min  
 8.4  200.1   56600   0.000100  |  0.140268    (0.953125)  |  0.027045    (0.993333)  |  26.0 min  
 8.5  202.1   57166   0.000100  |  0.169385    (0.921875)  |  0.027495    (0.993667)  |  26.2 min  
 8.6  204.1   57732   0.000100  |  0.202783    (0.945312)  |  0.026872    (0.993000)  |  26.4 min  
 8.7  206.1   58298   0.000100  |  0.252076    (0.906250)  |  0.026257    (0.993667)  |  26.6 min  
 8.8  208.1   58864   0.000100  |  0.192309    (0.945312)  |  0.026977    (0.992667)  |  26.7 min  
 8.8  210.1   59430   0.000100  |  0.176830    (0.937500)  |  0.026728    (0.993667)  |  26.9 min  
 8.9  212.1   59996   0.000100  |  0.224405    (0.937500)  |  0.026253    (0.993333)  |  27.1 min  
 9.0  213.7   60453   0.000100  |  0.238007    (0.937500)  |  0.027308    (0.993333)  |  27.2 min  

** evaluation on test set **
test_loss=0.172335    (test_acc=0.968171)

sucess
--- [START 2017-02-24 23:56:29] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  1.841308    (0.429688)  |  1.291803    (0.568667)  |   1.1 min  
 0.2    4.0   01132   0.010000  |  1.226745    (0.656250)  |  0.756713    (0.734000)  |   1.3 min  
 0.3    6.0   01698   0.010000  |  0.822463    (0.757812)  |  0.634261    (0.779000)  |   1.5 min  
 0.3    8.0   02264   0.010000  |  0.698155    (0.757812)  |  0.454969    (0.844000)  |   1.6 min  
 0.4   10.0   02830   0.010000  |  0.463036    (0.843750)  |  0.280161    (0.900667)  |   1.8 min  
 0.5   12.0   03396   0.010000  |  0.454927    (0.859375)  |  0.225665    (0.916667)  |   2.0 min  
 0.6   14.0   03962   0.010000  |  0.387574    (0.898438)  |  0.260410    (0.906667)  |   2.1 min  
 0.7   16.0   04528   0.010000  |  0.336579    (0.882812)  |  0.155264    (0.948000)  |   2.3 min  
 0.8   18.0   05094   0.010000  |  0.218000    (0.914062)  |  0.097515    (0.965000)  |   2.5 min  
 0.8   20.0   05660   0.010000  |  0.327985    (0.898438)  |  0.103453    (0.966667)  |   2.6 min  
 0.9   22.0   06226   0.010000  |  0.201253    (0.945312)  |  0.076322    (0.974333)  |   2.8 min  
 1.0   24.0   06792   0.010000  |  0.177907    (0.945312)  |  0.058912    (0.978667)  |   4.0 min  
 1.1   26.0   07358   0.010000  |  0.256770    (0.914062)  |  0.084501    (0.974000)  |   4.1 min  
 1.2   28.0   07924   0.010000  |  0.286723    (0.921875)  |  0.070922    (0.977000)  |   4.3 min  
 1.3   30.0   08490   0.010000  |  0.185787    (0.945312)  |  0.045696    (0.985000)  |   4.4 min  
 1.3   32.0   09056   0.010000  |  0.180894    (0.937500)  |  0.062670    (0.981667)  |   4.6 min  
 1.4   34.0   09622   0.010000  |  0.090209    (0.968750)  |  0.045322    (0.983667)  |   4.8 min  
 1.5   36.0   10188   0.010000  |  0.184576    (0.953125)  |  0.041958    (0.988667)  |   5.0 min  
 1.6   38.0   10754   0.010000  |  0.086636    (0.976562)  |  0.022518    (0.993000)  |   5.1 min  
 1.7   40.0   11320   0.010000  |  0.129882    (0.960938)  |  0.031039    (0.992000)  |   5.3 min  
 1.8   42.0   11886   0.010000  |  0.181816    (0.945312)  |  0.038079    (0.989000)  |   5.5 min  
 1.9   44.0   12452   0.010000  |  0.096962    (0.976562)  |  0.026757    (0.991000)  |   5.6 min  
 1.9   46.0   13018   0.010000  |  0.086113    (0.960938)  |  0.025505    (0.992000)  |   5.8 min  
 2.0   48.0   13584   0.010000  |  0.064359    (0.984375)  |  0.025577    (0.991333)  |   6.9 min  
 2.1   50.0   14150   0.010000  |  0.112900    (0.953125)  |  0.031735    (0.991667)  |   7.1 min  
 2.2   52.0   14716   0.010000  |  0.067873    (0.976562)  |  0.018665    (0.995333)  |   7.3 min  
 2.3   54.0   15282   0.010000  |  0.070912    (0.976562)  |  0.014787    (0.996000)  |   7.4 min  
 2.4   56.0   15848   0.010000  |  0.033351    (0.992188)  |  0.014602    (0.995333)  |   7.6 min  
 2.4   58.0   16414   0.010000  |  0.099084    (0.968750)  |  0.013752    (0.995667)  |   7.8 min  
 2.5   60.0   16980   0.010000  |  0.003354    (1.000000)  |  0.021020    (0.993000)  |   7.9 min  
 2.6   62.0   17546   0.010000  |  0.085782    (0.960938)  |  0.022121    (0.993667)  |   8.1 min  
 2.7   64.0   18112   0.010000  |  0.073414    (0.976562)  |  0.013446    (0.993667)  |   8.3 min  
 2.8   66.0   18678   0.010000  |  0.085884    (0.976562)  |  0.009573    (0.997333)  |   8.4 min  
 2.9   68.0   19244   0.010000  |  0.101261    (0.968750)  |  0.015317    (0.996000)  |   8.6 min  
 2.9   70.0   19810   0.010000  |  0.099012    (0.968750)  |  0.017076    (0.995000)  |   8.8 min  
 3.0   72.0   20376   0.001000  |  0.084898    (0.976562)  |  0.005734    (0.999000)  |   9.9 min  
 3.1   74.0   20942   0.001000  |  0.043146    (0.984375)  |  0.006022    (0.998333)  |  10.1 min  
 3.2   76.0   21508   0.001000  |  0.016345    (0.992188)  |  0.007549    (0.997667)  |  10.2 min  
 3.3   78.0   22074   0.001000  |  0.024905    (0.992188)  |  0.006596    (0.998333)  |  10.4 min  
 3.4   80.0   22640   0.001000  |  0.044474    (0.976562)  |  0.007140    (0.997333)  |  10.6 min  
 3.5   82.0   23206   0.001000  |  0.065535    (0.984375)  |  0.006899    (0.997667)  |  10.8 min  
 3.5   84.0   23772   0.001000  |  0.007898    (1.000000)  |  0.005974    (0.998000)  |  10.9 min  
 3.6   86.0   24338   0.001000  |  0.018363    (0.992188)  |  0.006151    (0.998333)  |  11.1 min  
 3.7   88.0   24904   0.001000  |  0.012089    (1.000000)  |  0.006447    (0.997667)  |  11.3 min  
 3.8   90.0   25470   0.001000  |  0.001613    (1.000000)  |  0.005656    (0.998333)  |  11.4 min  
 3.9   92.0   26036   0.001000  |  0.024231    (0.992188)  |  0.005672    (0.998000)  |  11.6 min  
 4.0   94.0   26602   0.001000  |  0.004299    (1.000000)  |  0.006193    (0.997333)  |  11.8 min  
 4.0   96.0   27168   0.001000  |  0.006040    (1.000000)  |  0.005310    (0.998667)  |  13.0 min  
 4.1   98.0   27734   0.001000  |  0.061621    (0.984375)  |  0.004685    (0.999000)  |  13.1 min  
 4.2  100.0   28300   0.001000  |  0.047100    (0.984375)  |  0.005519    (0.998667)  |  13.3 min  
 4.3  102.0   28866   0.001000  |  0.031818    (0.984375)  |  0.006269    (0.998667)  |  13.4 min  
 4.4  104.0   29432   0.001000  |  0.038201    (0.984375)  |  0.006081    (0.998333)  |  13.6 min  
 4.5  106.0   29998   0.001000  |  0.103485    (0.984375)  |  0.006438    (0.998333)  |  13.8 min  
 4.6  108.0   30564   0.001000  |  0.003392    (1.000000)  |  0.006135    (0.998333)  |  14.0 min  
 4.6  110.0   31130   0.001000  |  0.046439    (0.984375)  |  0.006222    (0.998667)  |  14.1 min  
 4.7  112.0   31696   0.001000  |  0.002517    (1.000000)  |  0.005243    (0.998667)  |  14.3 min  
 4.8  114.0   32262   0.001000  |  0.053696    (0.976562)  |  0.005877    (0.998000)  |  14.5 min  
 4.9  116.0   32828   0.001000  |  0.007678    (1.000000)  |  0.006004    (0.998667)  |  14.6 min  
 5.0  118.0   33394   0.001000  |  0.041613    (0.976562)  |  0.005223    (0.998667)  |  14.8 min  
 5.1  120.0   33960   0.001000  |  0.113189    (0.976562)  |  0.005310    (0.998000)  |  16.0 min  
 5.1  122.1   34526   0.001000  |  0.015748    (1.000000)  |  0.005613    (0.999000)  |  16.1 min  
 5.2  124.1   35092   0.001000  |  0.040555    (0.984375)  |  0.005212    (0.999000)  |  16.3 min  
 5.3  126.1   35658   0.001000  |  0.004211    (1.000000)  |  0.005760    (0.998667)  |  16.5 min  
 5.4  128.1   36224   0.001000  |  0.010617    (1.000000)  |  0.005685    (0.998333)  |  16.6 min  
 5.5  130.1   36790   0.001000  |  0.117977    (0.968750)  |  0.004624    (0.999000)  |  16.8 min  
 5.6  132.1   37356   0.001000  |  0.020735    (0.992188)  |  0.005364    (0.998667)  |  17.0 min  
 5.6  134.1   37922   0.001000  |  0.110804    (0.960938)  |  0.004321    (0.999000)  |  17.1 min  
 5.7  136.1   38488   0.001000  |  0.018180    (0.992188)  |  0.005411    (0.999000)  |  17.3 min  
 5.8  138.1   39054   0.001000  |  0.004450    (1.000000)  |  0.005363    (0.999000)  |  17.5 min  
 5.9  140.1   39620   0.001000  |  0.014527    (0.992188)  |  0.005465    (0.998667)  |  17.7 min  
 6.0  142.1   40186   0.001000  |  0.041196    (0.992188)  |  0.006151    (0.998333)  |  17.8 min  
 6.1  144.1   40752   0.000100  |  0.019385    (0.992188)  |  0.005542    (0.999000)  |  19.0 min  
 6.2  146.1   41318   0.000100  |  0.002752    (1.000000)  |  0.005635    (0.999000)  |  19.1 min  
 6.2  148.1   41884   0.000100  |  0.052755    (0.976562)  |  0.005521    (0.999000)  |  19.3 min  
 6.3  150.1   42450   0.000100  |  0.047574    (0.984375)  |  0.005499    (0.999000)  |  19.5 min  
 6.4  152.1   43016   0.000100  |  0.052337    (0.984375)  |  0.005326    (0.999000)  |  19.6 min  
 6.5  154.1   43582   0.000100  |  0.024592    (0.984375)  |  0.005252    (0.999000)  |  19.8 min  
 6.6  156.1   44148   0.000100  |  0.024067    (0.984375)  |  0.005290    (0.999000)  |  20.0 min  
 6.7  158.1   44714   0.000100  |  0.037117    (0.984375)  |  0.005428    (0.999000)  |  20.2 min  
 6.7  160.1   45280   0.000100  |  0.010341    (0.992188)  |  0.005472    (0.999000)  |  20.3 min  
 6.8  162.1   45846   0.000100  |  0.007795    (1.000000)  |  0.005290    (0.999000)  |  20.5 min  
 6.9  164.1   46412   0.000100  |  0.037826    (0.992188)  |  0.005329    (0.999000)  |  20.7 min  
 7.0  166.1   46978   0.000100  |  0.071861    (0.984375)  |  0.005329    (0.999000)  |  20.9 min  
 7.1  168.1   47544   0.000100  |  0.002628    (1.000000)  |  0.005392    (0.999000)  |  22.0 min  
 7.2  170.1   48110   0.000100  |  0.011460    (0.992188)  |  0.005352    (0.999000)  |  22.2 min  
 7.2  172.1   48676   0.000100  |  0.022118    (0.992188)  |  0.005226    (0.999000)  |  22.3 min  
 7.3  174.1   49242   0.000100  |  0.011200    (1.000000)  |  0.005320    (0.999000)  |  22.5 min  
 7.4  176.1   49808   0.000100  |  0.024486    (0.984375)  |  0.005428    (0.999000)  |  22.7 min  
 7.5  178.1   50374   0.000100  |  0.012153    (0.992188)  |  0.005346    (0.999000)  |  22.8 min  
 7.6  180.1   50940   0.000100  |  0.082436    (0.984375)  |  0.005354    (0.999000)  |  23.0 min  
 7.7  182.1   51506   0.000100  |  0.011455    (1.000000)  |  0.005275    (0.999000)  |  23.2 min  
 7.8  184.1   52072   0.000100  |  0.005425    (1.000000)  |  0.005381    (0.999000)  |  23.3 min  
 7.8  186.1   52638   0.000100  |  0.020215    (0.992188)  |  0.005200    (0.999000)  |  23.5 min  
 7.9  188.1   53204   0.000100  |  0.020246    (0.984375)  |  0.005299    (0.999000)  |  23.7 min  
 8.0  190.1   53770   0.000100  |  0.019568    (1.000000)  |  0.005396    (0.999000)  |  24.8 min  
 8.1  192.1   54336   0.000100  |  0.037228    (0.992188)  |  0.005390    (0.999000)  |  25.0 min  
 8.2  194.1   54902   0.000100  |  0.023460    (0.992188)  |  0.005237    (0.999000)  |  25.2 min  
 8.3  196.1   55468   0.000100  |  0.003802    (1.000000)  |  0.005215    (0.999000)  |  25.3 min  
 8.3  198.1   56034   0.000100  |  0.051164    (0.984375)  |  0.005189    (0.999000)  |  25.5 min  
 8.4  200.1   56600   0.000100  |  0.019197    (0.992188)  |  0.005225    (0.999000)  |  25.7 min  
 8.5  202.1   57166   0.000100  |  0.005850    (1.000000)  |  0.005297    (0.999000)  |  25.9 min  
 8.6  204.1   57732   0.000100  |  0.045000    (0.984375)  |  0.005234    (0.999000)  |  26.0 min  
 8.7  206.1   58298   0.000100  |  0.039906    (0.984375)  |  0.005059    (0.999000)  |  26.2 min  
 8.8  208.1   58864   0.000100  |  0.058393    (0.984375)  |  0.005121    (0.999000)  |  26.4 min  
 8.8  210.1   59430   0.000100  |  0.008789    (1.000000)  |  0.005264    (0.999000)  |  26.6 min  
 8.9  212.1   59996   0.000100  |  0.009734    (1.000000)  |  0.005269    (0.999000)  |  26.7 min  
 9.0  213.7   60453   0.000100  |  0.021753    (0.992188)  |  0.005211    (0.999000)  |  26.9 min  

** evaluation on test set **
test_loss=0.271286    (test_acc=0.956057)

sucess
--- [START 2017-02-25 00:35:10] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_3( input_shape=(1,1,1), output_shape = (1)):

    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    input = tf.map_fn(lambda img: tf.image.per_image_standardization(img), input)
    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=False)
        block1 = bn(block1)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=False)
        block2 = bn(block2)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=False)
        block3 = bn(block3)
        block3 = relu(block3)
        block3 = dropout(block3,keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=False)
        block4 = bn(block4)
        block4 = relu(block4)
        block4 = dropout(block4,keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  2.542369    (0.195312)  |  1.875752    (0.421667)  |   1.5 min  
 0.2    4.0   01132   0.010000  |  2.174343    (0.273438)  |  1.363986    (0.558000)  |   2.0 min  
 0.3    6.0   01698   0.010000  |  1.847490    (0.343750)  |  1.005684    (0.718667)  |   2.5 min  
 0.3    8.0   02264   0.010000  |  1.473885    (0.570312)  |  0.786180    (0.776667)  |   3.0 min  
 0.4   10.0   02830   0.010000  |  1.366808    (0.523438)  |  0.693716    (0.778667)  |   3.5 min  
 0.5   12.0   03396   0.010000  |  1.419814    (0.546875)  |  0.548232    (0.834333)  |   4.1 min  
 0.6   14.0   03962   0.010000  |  0.976949    (0.703125)  |  0.466074    (0.866333)  |   4.6 min  
 0.7   16.0   04528   0.010000  |  0.953854    (0.726562)  |  0.421929    (0.864667)  |   5.1 min  
 0.8   18.0   05094   0.010000  |  1.004841    (0.664062)  |  0.337675    (0.898000)  |   5.6 min  
 0.8   20.0   05660   0.010000  |  0.980551    (0.640625)  |  0.310710    (0.899333)  |   6.2 min  
 0.9   22.0   06226   0.010000  |  0.813567    (0.734375)  |  0.270963    (0.917333)  |   6.7 min  
 1.0   24.0   06792   0.010000  |  0.677684    (0.765625)  |  0.247128    (0.927000)  |   8.2 min  
 1.1   26.0   07358   0.010000  |  0.808942    (0.742188)  |  0.221423    (0.931000)  |   8.8 min  
 1.2   28.0   07924   0.010000  |  0.752685    (0.726562)  |  0.212120    (0.922000)  |   9.3 min  
 1.3   30.0   08490   0.010000  |  0.847861    (0.742188)  |  0.189712    (0.937667)  |   9.8 min  
 1.3   32.0   09056   0.010000  |  0.605078    (0.796875)  |  0.157453    (0.948667)  |  10.4 min  
 1.4   34.0   09622   0.010000  |  0.504792    (0.843750)  |  0.129674    (0.966000)  |  10.9 min  
 1.5   36.0   10188   0.010000  |  0.505578    (0.835938)  |  0.126490    (0.959000)  |  11.5 min  
 1.6   38.0   10754   0.010000  |  0.376302    (0.890625)  |  0.106089    (0.968000)  |  12.0 min  
 1.7   40.0   11320   0.010000  |  0.459798    (0.882812)  |  0.088077    (0.976000)  |  12.5 min  
 1.8   42.0   11886   0.010000  |  0.474086    (0.867188)  |  0.087268    (0.974333)  |  13.1 min  
 1.9   44.0   12452   0.010000  |  0.522667    (0.835938)  |  0.094862    (0.971667)  |  13.6 min  
 1.9   46.0   13018   0.010000  |  0.355028    (0.890625)  |  0.082459    (0.974667)  |  14.2 min  
 2.0   48.0   13584   0.010000  |  0.510612    (0.835938)  |  0.078530    (0.974333)  |  15.7 min  
 2.1   50.0   14150   0.010000  |  0.287614    (0.914062)  |  0.061539    (0.982333)  |  16.2 min  
 2.2   52.0   14716   0.010000  |  0.584591    (0.820312)  |  0.060791    (0.982000)  |  16.8 min  
 2.3   54.0   15282   0.010000  |  0.426106    (0.875000)  |  0.046832    (0.988333)  |  17.3 min  
 2.4   56.0   15848   0.010000  |  0.265826    (0.914062)  |  0.056194    (0.984000)  |  17.8 min  
 2.4   58.0   16414   0.010000  |  0.498359    (0.843750)  |  0.053021    (0.984667)  |  18.4 min  
 2.5   60.0   16980   0.010000  |  0.235280    (0.921875)  |  0.051631    (0.988333)  |  18.9 min  
 2.6   62.0   17546   0.010000  |  0.309780    (0.929688)  |  0.044787    (0.987667)  |  19.4 min  
 2.7   64.0   18112   0.010000  |  0.278020    (0.921875)  |  0.044063    (0.987667)  |  20.0 min  
 2.8   66.0   18678   0.010000  |  0.468529    (0.851562)  |  0.043513    (0.989000)  |  20.5 min  
 2.9   68.0   19244   0.010000  |  0.234996    (0.890625)  |  0.030663    (0.991667)  |  21.0 min  
 2.9   70.0   19810   0.010000  |  0.238577    (0.929688)  |  0.033654    (0.991000)  |  21.6 min  
 3.0   72.0   20376   0.001000  |  0.277094    (0.914062)  |  0.025229    (0.993333)  |  23.1 min  
 3.1   74.0   20942   0.001000  |  0.235462    (0.929688)  |  0.021480    (0.994333)  |  23.6 min  
 3.2   76.0   21508   0.001000  |  0.185783    (0.929688)  |  0.020339    (0.995667)  |  24.1 min  
 3.3   78.0   22074   0.001000  |  0.169163    (0.968750)  |  0.019952    (0.995333)  |  24.7 min  
 3.4   80.0   22640   0.001000  |  0.241199    (0.945312)  |  0.019585    (0.996333)  |  25.2 min  
 3.5   82.0   23206   0.001000  |  0.247284    (0.914062)  |  0.018983    (0.995000)  |  25.8 min  
 3.5   84.0   23772   0.001000  |  0.232558    (0.945312)  |  0.018990    (0.995667)  |  26.3 min  
 3.6   86.0   24338   0.001000  |  0.353193    (0.906250)  |  0.017961    (0.996667)  |  26.8 min  
 3.7   88.0   24904   0.001000  |  0.104425    (0.976562)  |  0.019013    (0.994333)  |  27.4 min  
 3.8   90.0   25470   0.001000  |  0.214549    (0.921875)  |  0.017499    (0.995667)  |  27.9 min  
 3.9   92.0   26036   0.001000  |  0.159025    (0.945312)  |  0.017197    (0.996000)  |  28.4 min  
 4.0   94.0   26602   0.001000  |  0.202070    (0.945312)  |  0.017984    (0.995333)  |  29.0 min  
 4.0   96.0   27168   0.001000  |  0.116219    (0.968750)  |  0.017687    (0.995333)  |  30.5 min  
 4.1   98.0   27734   0.001000  |  0.288155    (0.929688)  |  0.017691    (0.996000)  |  31.0 min  
 4.2  100.0   28300   0.001000  |  0.301766    (0.882812)  |  0.016523    (0.996333)  |  31.6 min  
 4.3  102.0   28866   0.001000  |  0.252883    (0.906250)  |  0.016435    (0.997333)  |  32.1 min  
 4.4  104.0   29432   0.001000  |  0.298002    (0.937500)  |  0.018114    (0.996333)  |  32.6 min  
 4.5  106.0   29998   0.001000  |  0.292814    (0.914062)  |  0.016599    (0.996000)  |  33.2 min  
 4.6  108.0   30564   0.001000  |  0.230120    (0.945312)  |  0.016681    (0.995333)  |  33.7 min  
 4.6  110.0   31130   0.001000  |  0.395830    (0.898438)  |  0.015973    (0.996333)  |  34.2 min  
 4.7  112.0   31696   0.001000  |  0.116593    (0.968750)  |  0.015620    (0.996667)  |  34.8 min  
 4.8  114.0   32262   0.001000  |  0.167998    (0.960938)  |  0.015272    (0.997333)  |  35.3 min  
 4.9  116.0   32828   0.001000  |  0.199748    (0.953125)  |  0.015980    (0.996667)  |  35.8 min  
 5.0  118.0   33394   0.001000  |  0.234581    (0.937500)  |  0.014800    (0.997000)  |  36.4 min  
 5.1  120.0   33960   0.001000  |  0.200282    (0.937500)  |  0.014403    (0.996667)  |  37.9 min  
 5.1  122.1   34526   0.001000  |  0.210054    (0.937500)  |  0.014978    (0.996333)  |  38.4 min  
 5.2  124.1   35092   0.001000  |  0.248635    (0.929688)  |  0.015049    (0.996667)  |  38.9 min  
 5.3  126.1   35658   0.001000  |  0.153609    (0.945312)  |  0.014979    (0.996000)  |  39.5 min  
 5.4  128.1   36224   0.001000  |  0.161978    (0.953125)  |  0.013786    (0.997000)  |  40.0 min  
 5.5  130.1   36790   0.001000  |  0.219608    (0.945312)  |  0.014445    (0.996667)  |  40.5 min  
 5.6  132.1   37356   0.001000  |  0.209581    (0.937500)  |  0.013763    (0.997000)  |  41.1 min  
 5.6  134.1   37922   0.001000  |  0.212712    (0.937500)  |  0.013681    (0.997000)  |  41.6 min  
 5.7  136.1   38488   0.001000  |  0.322187    (0.875000)  |  0.014089    (0.997000)  |  42.1 min  
 5.8  138.1   39054   0.001000  |  0.233981    (0.929688)  |  0.015281    (0.996000)  |  42.7 min  
 5.9  140.1   39620   0.001000  |  0.108874    (0.960938)  |  0.014955    (0.996333)  |  43.2 min  
 6.0  142.1   40186   0.001000  |  0.274687    (0.921875)  |  0.013527    (0.997000)  |  43.8 min  
 6.1  144.1   40752   0.000100  |  0.144791    (0.960938)  |  0.013340    (0.996667)  |  45.3 min  
 6.2  146.1   41318   0.000100  |  0.207863    (0.914062)  |  0.012923    (0.997000)  |  45.8 min  
 6.2  148.1   41884   0.000100  |  0.253926    (0.929688)  |  0.013050    (0.997667)  |  46.3 min  
 6.3  150.1   42450   0.000100  |  0.190217    (0.937500)  |  0.013811    (0.996667)  |  46.9 min  
 6.4  152.1   43016   0.000100  |  0.123760    (0.968750)  |  0.013597    (0.996000)  |  47.4 min  
 6.5  154.1   43582   0.000100  |  0.254496    (0.937500)  |  0.012787    (0.996667)  |  48.0 min  
 6.6  156.1   44148   0.000100  |  0.101029    (0.976562)  |  0.013803    (0.997000)  |  48.5 min  
 6.7  158.1   44714   0.000100  |  0.261782    (0.929688)  |  0.013999    (0.996667)  |  49.0 min  
 6.7  160.1   45280   0.000100  |  0.177625    (0.945312)  |  0.013648    (0.996333)  |  49.6 min  
 6.8  162.1   45846   0.000100  |  0.161957    (0.945312)  |  0.013413    (0.996000)  |  50.1 min  
 6.9  164.1   46412   0.000100  |  0.216504    (0.953125)  |  0.013223    (0.997000)  |  50.7 min  
 7.0  166.1   46978   0.000100  |  0.314413    (0.937500)  |  0.013227    (0.997000)  |  51.2 min  
 7.1  168.1   47544   0.000100  |  0.111311    (0.968750)  |  0.013277    (0.997333)  |  52.7 min  
 7.2  170.1   48110   0.000100  |  0.173932    (0.937500)  |  0.013107    (0.997333)  |  53.2 min  
 7.2  172.1   48676   0.000100  |  0.146847    (0.945312)  |  0.013400    (0.997667)  |  53.8 min  
 7.3  174.1   49242   0.000100  |  0.202035    (0.937500)  |  0.013102    (0.997333)  |  54.3 min  
 7.4  176.1   49808   0.000100  |  0.261945    (0.929688)  |  0.013173    (0.996667)  |  54.8 min  
 7.5  178.1   50374   0.000100  |  0.172659    (0.945312)  |  0.012583    (0.997333)  |  55.4 min  
 7.6  180.1   50940   0.000100  |  0.129374    (0.953125)  |  0.012913    (0.997000)  |  55.9 min  
 7.7  182.1   51506   0.000100  |  0.244629    (0.929688)  |  0.013074    (0.997000)  |  56.5 min  
 7.8  184.1   52072   0.000100  |  0.105190    (0.960938)  |  0.012936    (0.997333)  |  57.0 min  
 7.8  186.1   52638   0.000100  |  0.301815    (0.906250)  |  0.012755    (0.996667)  |  57.5 min  
 7.9  188.1   53204   0.000100  |  0.104299    (0.960938)  |  0.013206    (0.996667)  |  58.1 min  
 8.0  190.1   53770   0.000100  |  0.193564    (0.953125)  |  0.012612    (0.996667)  |  59.6 min  
 8.1  192.1   54336   0.000100  |  0.262731    (0.929688)  |  0.012903    (0.997000)  |  60.1 min  
 8.2  194.1   54902   0.000100  |  0.173284    (0.921875)  |  0.013077    (0.997000)  |  60.7 min  
 8.3  196.1   55468   0.000100  |  0.175230    (0.953125)  |  0.013078    (0.997000)  |  61.2 min  
 8.3  198.1   56034   0.000100  |  0.189163    (0.929688)  |  0.013477    (0.996667)  |  61.8 min  
 8.4  200.1   56600   0.000100  |  0.140221    (0.953125)  |  0.012598    (0.997000)  |  62.3 min  
 8.5  202.1   57166   0.000100  |  0.243727    (0.921875)  |  0.013087    (0.997000)  |  62.8 min  
 8.6  204.1   57732   0.000100  |  0.265745    (0.890625)  |  0.012746    (0.997333)  |  63.4 min  
 8.7  206.1   58298   0.000100  |  0.250397    (0.929688)  |  0.013055    (0.997000)  |  63.9 min  
 8.8  208.1   58864   0.000100  |  0.176446    (0.953125)  |  0.012530    (0.997667)  |  64.4 min  
 8.8  210.1   59430   0.000100  |  0.219397    (0.937500)  |  0.013011    (0.996667)  |  65.0 min  
 8.9  212.1   59996   0.000100  |  0.149199    (0.945312)  |  0.013200    (0.997000)  |  65.5 min  
 9.0  213.7   60453   0.000100  |  0.144802    (0.960938)  |  0.012866    (0.997000)  |  65.9 min  

** evaluation on test set **
test_loss=0.095924    (test_acc=0.969834)

sucess
--- [START 2017-02-25 02:46:17] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
--- [START 2017-02-25 02:50:18] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  1.793192    (0.492188)  |  1.276305    (0.654000)  |   2.8 min  
 0.2    4.0   01132   0.010000  |  1.131970    (0.656250)  |  0.447721    (0.867667)  |   3.1 min  
 0.3    6.0   01698   0.010000  |  0.600179    (0.804688)  |  0.194524    (0.942667)  |   3.5 min  
 0.3    8.0   02264   0.010000  |  0.429025    (0.890625)  |  0.111025    (0.969667)  |   3.8 min  
 0.4   10.0   02830   0.010000  |  0.376955    (0.898438)  |  0.083072    (0.977333)  |   4.1 min  
 0.5   12.0   03396   0.010000  |  0.419083    (0.898438)  |  0.071399    (0.977000)  |   4.4 min  
 0.6   14.0   03962   0.010000  |  0.279108    (0.890625)  |  0.049073    (0.987333)  |   4.7 min  
 0.7   16.0   04528   0.010000  |  0.187318    (0.945312)  |  0.035637    (0.988333)  |   5.1 min  
 0.8   18.0   05094   0.010000  |  0.295770    (0.906250)  |  0.023972    (0.994000)  |   5.4 min  
 0.8   20.0   05660   0.010000  |  0.237415    (0.937500)  |  0.025586    (0.992667)  |   5.7 min  
 0.9   22.0   06226   0.010000  |  0.193641    (0.953125)  |  0.021254    (0.994333)  |   6.0 min  
 1.0   24.0   06792   0.010000  |  0.273850    (0.937500)  |  0.015284    (0.996333)  |   8.9 min  
 1.1   26.0   07358   0.010000  |  0.244191    (0.937500)  |  0.025800    (0.993333)  |   9.2 min  
 1.2   28.0   07924   0.010000  |  0.082133    (0.984375)  |  0.016567    (0.995667)  |   9.5 min  
 1.3   30.0   08490   0.010000  |  0.255881    (0.929688)  |  0.013049    (0.995667)  |   9.9 min  
 1.3   32.0   09056   0.010000  |  0.135100    (0.953125)  |  0.010562    (0.997667)  |  10.2 min  
 1.4   34.0   09622   0.010000  |  0.112881    (0.968750)  |  0.010420    (0.996667)  |  10.5 min  
 1.5   36.0   10188   0.010000  |  0.177324    (0.945312)  |  0.010972    (0.997000)  |  10.8 min  
 1.6   38.0   10754   0.010000  |  0.128872    (0.968750)  |  0.012532    (0.996667)  |  11.2 min  
 1.7   40.0   11320   0.010000  |  0.128214    (0.976562)  |  0.010027    (0.996667)  |  11.5 min  
 1.8   42.0   11886   0.010000  |  0.112278    (0.968750)  |  0.007519    (0.998333)  |  11.8 min  
 1.9   44.0   12452   0.010000  |  0.127362    (0.968750)  |  0.009835    (0.996333)  |  12.1 min  
 1.9   46.0   13018   0.010000  |  0.186387    (0.953125)  |  0.005713    (0.999333)  |  12.4 min  
 2.0   48.0   13584   0.010000  |  0.184520    (0.953125)  |  0.008239    (0.998000)  |  15.3 min  
 2.1   50.0   14150   0.010000  |  0.126564    (0.960938)  |  0.006876    (0.998000)  |  15.6 min  
 2.2   52.0   14716   0.010000  |  0.118692    (0.968750)  |  0.009162    (0.997000)  |  15.9 min  
 2.3   54.0   15282   0.010000  |  0.110358    (0.976562)  |  0.009369    (0.997667)  |  16.2 min  
 2.4   56.0   15848   0.010000  |  0.139019    (0.945312)  |  0.006626    (0.998333)  |  16.6 min  
 2.4   58.0   16414   0.010000  |  0.133089    (0.968750)  |  0.007063    (0.997333)  |  16.9 min  
 2.5   60.0   16980   0.010000  |  0.083131    (0.976562)  |  0.006384    (0.998333)  |  17.2 min  
 2.6   62.0   17546   0.010000  |  0.011280    (1.000000)  |  0.004157    (0.999667)  |  17.5 min  
 2.7   64.0   18112   0.010000  |  0.117344    (0.968750)  |  0.008417    (0.998000)  |  17.8 min  
 2.8   66.0   18678   0.010000  |  0.155344    (0.968750)  |  0.005045    (0.999333)  |  18.2 min  
 2.9   68.0   19244   0.010000  |  0.085912    (0.968750)  |  0.008911    (0.996333)  |  18.5 min  
 2.9   70.0   19810   0.010000  |  0.157533    (0.968750)  |  0.007074    (0.998333)  |  18.8 min  
 3.0   72.0   20376   0.001000  |  0.118854    (0.960938)  |  0.003183    (0.999667)  |  21.6 min  
 3.1   74.0   20942   0.001000  |  0.065067    (0.984375)  |  0.003460    (0.999333)  |  22.0 min  
 3.2   76.0   21508   0.001000  |  0.130806    (0.960938)  |  0.002758    (1.000000)  |  22.3 min  
 3.3   78.0   22074   0.001000  |  0.235068    (0.937500)  |  0.003574    (0.999000)  |  22.6 min  
 3.4   80.0   22640   0.001000  |  0.069137    (0.984375)  |  0.002832    (1.000000)  |  22.9 min  
 3.5   82.0   23206   0.001000  |  0.106429    (0.968750)  |  0.002841    (0.999667)  |  23.2 min  
 3.5   84.0   23772   0.001000  |  0.051569    (0.984375)  |  0.003303    (0.999000)  |  23.6 min  
 3.6   86.0   24338   0.001000  |  0.151414    (0.960938)  |  0.002796    (0.999667)  |  23.9 min  
 3.7   88.0   24904   0.001000  |  0.046675    (0.992188)  |  0.002868    (0.999667)  |  24.2 min  
 3.8   90.0   25470   0.001000  |  0.171426    (0.960938)  |  0.002761    (0.999333)  |  24.5 min  
 3.9   92.0   26036   0.001000  |  0.075905    (0.976562)  |  0.002951    (0.999000)  |  24.8 min  
 4.0   94.0   26602   0.001000  |  0.019944    (0.992188)  |  0.003163    (0.999333)  |  25.2 min  
 4.0   96.0   27168   0.001000  |  0.072463    (0.976562)  |  0.002472    (1.000000)  |  28.0 min  
 4.1   98.0   27734   0.001000  |  0.097967    (0.976562)  |  0.002209    (1.000000)  |  28.3 min  
 4.2  100.0   28300   0.001000  |  0.023627    (0.992188)  |  0.002690    (0.999667)  |  28.7 min  
 4.3  102.0   28866   0.001000  |  0.027475    (1.000000)  |  0.002871    (0.999667)  |  29.0 min  
 4.4  104.0   29432   0.001000  |  0.062356    (0.984375)  |  0.002506    (0.999667)  |  29.3 min  
 4.5  106.0   29998   0.001000  |  0.104482    (0.960938)  |  0.002983    (0.999333)  |  29.6 min  
 4.6  108.0   30564   0.001000  |  0.189696    (0.960938)  |  0.002492    (0.999667)  |  29.9 min  
 4.6  110.0   31130   0.001000  |  0.059567    (0.984375)  |  0.002797    (0.999667)  |  30.3 min  
 4.7  112.0   31696   0.001000  |  0.139550    (0.960938)  |  0.002869    (0.999333)  |  30.6 min  
 4.8  114.0   32262   0.001000  |  0.067761    (0.984375)  |  0.002892    (0.999333)  |  30.9 min  
 4.9  116.0   32828   0.001000  |  0.176402    (0.945312)  |  0.002782    (0.999667)  |  31.2 min  
 5.0  118.0   33394   0.001000  |  0.049254    (0.984375)  |  0.002903    (0.999333)  |  31.5 min  
 5.1  120.0   33960   0.001000  |  0.098337    (0.976562)  |  0.002704    (0.999667)  |  34.4 min  
 5.1  122.1   34526   0.001000  |  0.088001    (0.976562)  |  0.002458    (0.999667)  |  34.7 min  
 5.2  124.1   35092   0.001000  |  0.098045    (0.968750)  |  0.002627    (0.999333)  |  35.0 min  
 5.3  126.1   35658   0.001000  |  0.009609    (1.000000)  |  0.003089    (0.999333)  |  35.3 min  
 5.4  128.1   36224   0.001000  |  0.054387    (0.992188)  |  0.002463    (0.999333)  |  35.6 min  
 5.5  130.1   36790   0.001000  |  0.199223    (0.960938)  |  0.002163    (0.999667)  |  36.0 min  
 5.6  132.1   37356   0.001000  |  0.098194    (0.960938)  |  0.002466    (0.999333)  |  36.3 min  
 5.6  134.1   37922   0.001000  |  0.041252    (0.992188)  |  0.002248    (0.999667)  |  36.6 min  
 5.7  136.1   38488   0.001000  |  0.119331    (0.976562)  |  0.002629    (0.999333)  |  36.9 min  
 5.8  138.1   39054   0.001000  |  0.028909    (0.992188)  |  0.002992    (0.999333)  |  37.2 min  
 5.9  140.1   39620   0.001000  |  0.065080    (0.976562)  |  0.003100    (0.999333)  |  37.6 min  
 6.0  142.1   40186   0.001000  |  0.051180    (0.984375)  |  0.002369    (0.999667)  |  37.9 min  
 6.1  144.1   40752   0.000100  |  0.083366    (0.976562)  |  0.002551    (0.999667)  |  40.7 min  
 6.2  146.1   41318   0.000100  |  0.102934    (0.968750)  |  0.002530    (0.999667)  |  41.0 min  
 6.2  148.1   41884   0.000100  |  0.081452    (0.976562)  |  0.002690    (0.999333)  |  41.4 min  
 6.3  150.1   42450   0.000100  |  0.037910    (0.992188)  |  0.002436    (0.999667)  |  41.7 min  
 6.4  152.1   43016   0.000100  |  0.102210    (0.968750)  |  0.002684    (0.999667)  |  42.0 min  
 6.5  154.1   43582   0.000100  |  0.136669    (0.968750)  |  0.002419    (0.999667)  |  42.3 min  
 6.6  156.1   44148   0.000100  |  0.009994    (1.000000)  |  0.002699    (0.999667)  |  42.6 min  
 6.7  158.1   44714   0.000100  |  0.136886    (0.968750)  |  0.002225    (0.999667)  |  42.9 min  
 6.7  160.1   45280   0.000100  |  0.070215    (0.976562)  |  0.002355    (0.999333)  |  43.3 min  
 6.8  162.1   45846   0.000100  |  0.069912    (0.984375)  |  0.002390    (0.999667)  |  43.6 min  
 6.9  164.1   46412   0.000100  |  0.057105    (0.976562)  |  0.002540    (0.999667)  |  43.9 min  
 7.0  166.1   46978   0.000100  |  0.068210    (0.984375)  |  0.002351    (0.999667)  |  44.2 min  
 7.1  168.1   47544   0.000100  |  0.024317    (1.000000)  |  0.002485    (0.999667)  |  47.1 min  
 7.2  170.1   48110   0.000100  |  0.031009    (0.992188)  |  0.002465    (0.999667)  |  47.4 min  
 7.2  172.1   48676   0.000100  |  0.117256    (0.976562)  |  0.002210    (0.999667)  |  47.7 min  
 7.3  174.1   49242   0.000100  |  0.025948    (0.984375)  |  0.002489    (0.999667)  |  48.0 min  
 7.4  176.1   49808   0.000100  |  0.062646    (0.984375)  |  0.002567    (0.999667)  |  48.3 min  
 7.5  178.1   50374   0.000100  |  0.062831    (0.976562)  |  0.002357    (0.999667)  |  48.7 min  
 7.6  180.1   50940   0.000100  |  0.064617    (0.976562)  |  0.002408    (0.999667)  |  49.0 min  
 7.7  182.1   51506   0.000100  |  0.014405    (1.000000)  |  0.002628    (0.999667)  |  49.3 min  
 7.8  184.1   52072   0.000100  |  0.076010    (0.984375)  |  0.002653    (0.999333)  |  49.6 min  
 7.8  186.1   52638   0.000100  |  0.049766    (0.992188)  |  0.002471    (0.999667)  |  49.9 min  
 7.9  188.1   53204   0.000100  |  0.186455    (0.953125)  |  0.002331    (0.999667)  |  50.3 min  
 8.0  190.1   53770   0.000100  |  0.097239    (0.968750)  |  0.002599    (0.999667)  |  53.1 min  
 8.1  192.1   54336   0.000100  |  0.048766    (0.984375)  |  0.002252    (0.999667)  |  53.4 min  
 8.2  194.1   54902   0.000100  |  0.080862    (0.976562)  |  0.002413    (0.999667)  |  53.7 min  
 8.3  196.1   55468   0.000100  |  0.039876    (0.992188)  |  0.002587    (0.999667)  |  54.0 min  
 8.3  198.1   56034   0.000100  |  0.061776    (0.976562)  |  0.002422    (0.999667)  |  54.4 min  
 8.4  200.1   56600   0.000100  |  0.056652    (0.984375)  |  0.002291    (0.999667)  |  54.7 min  
 8.5  202.1   57166   0.000100  |  0.116130    (0.960938)  |  0.002458    (0.999667)  |  55.0 min  
 8.6  204.1   57732   0.000100  |  0.039534    (0.992188)  |  0.002503    (0.999667)  |  55.3 min  
 8.7  206.1   58298   0.000100  |  0.127180    (0.968750)  |  0.002560    (0.999667)  |  55.6 min  
 8.8  208.1   58864   0.000100  |  0.073395    (0.976562)  |  0.002648    (0.999667)  |  56.0 min  
 8.8  210.1   59430   0.000100  |  0.076433    (0.953125)  |  0.002565    (0.999667)  |  56.3 min  
 8.9  212.1   59996   0.000100  |  0.044010    (0.976562)  |  0.002519    (0.999667)  |  56.6 min  
 9.0  213.7   60453   0.000100  |  0.110889    (0.968750)  |  0.002674    (0.999667)  |  56.9 min  

** evaluation on test set **
test_loss=0.062425    (test_acc=0.985194)

sucess
--- [START 2017-02-25 03:49:22] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
--- [START 2017-02-25 03:50:31] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
--- [START 2017-02-25 03:51:58] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_4( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')


    with tf.variable_scope('block1') as scope:
        block1 = conv2d_bn_relu(input, num_kernels=64, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME')
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d_bn_relu(block1, name='1', num_kernels=32,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='2', num_kernels=32,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block2 = conv2d_bn_relu(block2, name='3', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = conv2d_bn_relu(block2, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block3 = conv2d_bn_relu(block3, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')

    with tf.variable_scope('block4') as scope:
        block4 = conv2d_bn_relu(block3, name='1', num_kernels=64,  kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='2', num_kernels=64,  kernel_size=(3,3), stride=[1,1,1,1], padding='SAME')
        block4 = conv2d_bn_relu(block4, name='3', num_kernels=128, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME')
        block4 = maxpool(block4, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block5') as scope:
        block4 = flatten(block4)
        block5 = dense_bn_relu(block4, name='1', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='2', num_hiddens=256)
        block5 = dense_bn_relu(block5, name='3', num_hiddens=num_class)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 19
	all mac         = 16.7 (M)
	all param_size  = 1.9 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  0.319887    (0.890625)  |  0.213413    (0.939000)  |   1.8 min  
 0.2    4.0   01132   0.100000  |  0.236478    (0.937500)  |  0.098956    (0.968333)  |   2.1 min  
 0.3    6.0   01698   0.100000  |  0.025956    (0.992188)  |  0.066490    (0.981667)  |   2.4 min  
 0.3    8.0   02264   0.100000  |  0.077606    (0.976562)  |  0.043105    (0.986000)  |   2.8 min  
 0.4   10.0   02830   0.100000  |  0.021956    (0.992188)  |  0.037112    (0.990333)  |   3.1 min  
 0.5   12.0   03396   0.100000  |  0.040580    (0.984375)  |  0.035428    (0.991333)  |   3.4 min  
 0.6   14.0   03962   0.100000  |  0.030998    (1.000000)  |  0.032433    (0.990333)  |   3.7 min  
 0.7   16.0   04528   0.100000  |  0.016408    (0.992188)  |  0.025205    (0.994000)  |   4.0 min  
 0.8   18.0   05094   0.100000  |  0.009536    (1.000000)  |  0.022767    (0.994333)  |   4.4 min  
 0.8   20.0   05660   0.100000  |  0.052547    (0.984375)  |  0.019696    (0.994000)  |   4.7 min  
 0.9   22.0   06226   0.100000  |  0.051347    (0.984375)  |  0.024891    (0.992000)  |   5.0 min  
 1.0   24.0   06792   0.100000  |  0.126667    (0.960938)  |  0.024163    (0.994333)  |   6.9 min  
 1.1   26.0   07358   0.100000  |  0.024687    (0.992188)  |  0.018622    (0.995000)  |   7.2 min  
 1.2   28.0   07924   0.100000  |  0.106427    (0.976562)  |  0.021052    (0.994333)  |   7.5 min  
 1.3   30.0   08490   0.100000  |  0.053063    (0.992188)  |  0.015501    (0.997000)  |   7.8 min  
 1.3   32.0   09056   0.100000  |  0.043272    (0.992188)  |  0.025091    (0.992667)  |   8.2 min  
 1.4   34.0   09622   0.100000  |  0.002971    (1.000000)  |  0.021772    (0.993000)  |   8.5 min  
 1.5   36.0   10188   0.100000  |  0.009910    (0.992188)  |  0.017672    (0.995333)  |   8.8 min  
 1.6   38.0   10754   0.100000  |  0.013073    (1.000000)  |  0.019541    (0.994667)  |   9.1 min  
 1.7   40.0   11320   0.100000  |  0.095139    (0.968750)  |  0.017504    (0.994333)  |   9.4 min  
 1.8   42.0   11886   0.100000  |  0.080945    (0.976562)  |  0.014854    (0.996667)  |   9.8 min  
 1.9   44.0   12452   0.100000  |  0.030160    (0.992188)  |  0.017383    (0.995333)  |  10.1 min  
 1.9   46.0   13018   0.100000  |  0.068723    (0.992188)  |  0.017118    (0.993333)  |  10.4 min  
 2.0   48.0   13584   0.100000  |  0.059180    (0.984375)  |  0.015621    (0.995667)  |  12.3 min  
 2.1   50.0   14150   0.100000  |  0.050767    (0.984375)  |  0.018421    (0.995000)  |  12.6 min  
 2.2   52.0   14716   0.100000  |  0.026623    (0.992188)  |  0.013164    (0.996000)  |  12.9 min  
 2.3   54.0   15282   0.100000  |  0.021869    (0.992188)  |  0.015420    (0.994333)  |  13.3 min  
 2.4   56.0   15848   0.100000  |  0.003216    (1.000000)  |  0.017832    (0.994000)  |  13.6 min  
 2.4   58.0   16414   0.100000  |  0.071759    (0.968750)  |  0.019940    (0.995000)  |  13.9 min  
 2.5   60.0   16980   0.100000  |  0.031617    (0.992188)  |  0.019283    (0.994000)  |  14.2 min  
 2.6   62.0   17546   0.100000  |  0.021650    (0.992188)  |  0.011835    (0.996667)  |  14.6 min  
 2.7   64.0   18112   0.100000  |  0.004968    (1.000000)  |  0.017122    (0.995667)  |  14.9 min  
 2.8   66.0   18678   0.100000  |  0.002668    (1.000000)  |  0.014309    (0.995333)  |  15.2 min  
 2.9   68.0   19244   0.100000  |  0.032083    (0.992188)  |  0.016208    (0.994667)  |  15.5 min  
 2.9   70.0   19810   0.100000  |  0.093488    (0.976562)  |  0.012802    (0.997000)  |  15.8 min  
 3.0   72.0   20376   0.010000  |  0.024242    (0.992188)  |  0.014749    (0.994667)  |  17.7 min  
 3.1   74.0   20942   0.010000  |  0.017318    (0.992188)  |  0.014993    (0.995333)  |  18.0 min  
 3.2   76.0   21508   0.010000  |  0.000815    (1.000000)  |  0.014377    (0.995000)  |  18.3 min  
 3.3   78.0   22074   0.010000  |  0.047838    (0.992188)  |  0.012739    (0.996000)  |  18.7 min  
 3.4   80.0   22640   0.010000  |  0.067282    (0.976562)  |  0.012953    (0.995333)  |  19.0 min  
 3.5   82.0   23206   0.010000  |  0.005339    (1.000000)  |  0.012750    (0.996000)  |  19.3 min  
 3.5   84.0   23772   0.010000  |  0.022751    (0.992188)  |  0.012769    (0.995333)  |  19.6 min  
 3.6   86.0   24338   0.010000  |  0.025528    (0.992188)  |  0.012498    (0.996000)  |  20.0 min  
 3.7   88.0   24904   0.010000  |  0.013994    (1.000000)  |  0.012083    (0.996333)  |  20.3 min  
 3.8   90.0   25470   0.010000  |  0.023218    (0.992188)  |  0.012321    (0.996667)  |  20.6 min  
 3.9   92.0   26036   0.010000  |  0.053882    (0.984375)  |  0.012024    (0.996000)  |  20.9 min  
 4.0   94.0   26602   0.010000  |  0.010707    (0.992188)  |  0.011202    (0.996333)  |  21.3 min  
 4.0   96.0   27168   0.010000  |  0.000352    (1.000000)  |  0.011837    (0.996000)  |  23.1 min  
 4.1   98.0   27734   0.010000  |  0.001329    (1.000000)  |  0.012509    (0.996000)  |  23.5 min  
 4.2  100.0   28300   0.010000  |  0.029631    (0.992188)  |  0.011511    (0.996000)  |  23.8 min  
 4.3  102.0   28866   0.010000  |  0.043526    (0.984375)  |  0.010944    (0.996333)  |  24.1 min  
 4.4  104.0   29432   0.010000  |  0.009153    (0.992188)  |  0.011537    (0.997000)  |  24.4 min  
 4.5  106.0   29998   0.010000  |  0.074857    (0.984375)  |  0.011501    (0.996333)  |  24.7 min  
 4.6  108.0   30564   0.010000  |  0.069183    (0.984375)  |  0.011699    (0.996000)  |  25.1 min  
 4.6  110.0   31130   0.010000  |  0.021213    (0.992188)  |  0.012181    (0.996000)  |  25.4 min  
 4.7  112.0   31696   0.010000  |  0.002893    (1.000000)  |  0.012543    (0.996000)  |  25.7 min  
 4.8  114.0   32262   0.010000  |  0.038414    (0.992188)  |  0.012659    (0.996000)  |  26.0 min  
 4.9  116.0   32828   0.010000  |  0.000423    (1.000000)  |  0.012804    (0.995667)  |  26.4 min  
 5.0  118.0   33394   0.010000  |  0.010990    (0.992188)  |  0.012224    (0.996000)  |  26.7 min  
 5.1  120.0   33960   0.010000  |  0.005155    (1.000000)  |  0.011862    (0.996667)  |  28.6 min  
 5.1  122.1   34526   0.010000  |  0.048652    (0.992188)  |  0.012523    (0.996000)  |  28.9 min  
 5.2  124.1   35092   0.010000  |  0.053525    (0.992188)  |  0.011270    (0.996333)  |  29.2 min  
 5.3  126.1   35658   0.010000  |  0.045161    (0.992188)  |  0.012755    (0.996000)  |  29.5 min  
 5.4  128.1   36224   0.010000  |  0.041553    (0.984375)  |  0.011414    (0.996667)  |  29.9 min  
 5.5  130.1   36790   0.010000  |  0.000752    (1.000000)  |  0.011945    (0.996000)  |  30.2 min  
 5.6  132.1   37356   0.010000  |  0.001175    (1.000000)  |  0.012146    (0.996333)  |  30.5 min  
 5.6  134.1   37922   0.010000  |  0.069484    (0.984375)  |  0.011587    (0.997000)  |  30.8 min  
 5.7  136.1   38488   0.010000  |  0.027073    (0.992188)  |  0.010796    (0.997000)  |  31.2 min  
 5.8  138.1   39054   0.010000  |  0.003617    (1.000000)  |  0.011144    (0.997000)  |  31.5 min  
 5.9  140.1   39620   0.010000  |  0.015169    (1.000000)  |  0.010997    (0.997333)  |  31.8 min  
 6.0  142.1   40186   0.010000  |  0.018505    (0.992188)  |  0.011063    (0.996667)  |  32.1 min  
 6.1  144.1   40752   0.001000  |  0.045476    (0.984375)  |  0.011412    (0.997000)  |  34.0 min  
 6.2  146.1   41318   0.001000  |  0.053615    (0.992188)  |  0.010647    (0.997000)  |  34.3 min  
 6.2  148.1   41884   0.001000  |  0.001772    (1.000000)  |  0.010970    (0.996667)  |  34.6 min  
 6.3  150.1   42450   0.001000  |  0.051126    (0.984375)  |  0.011832    (0.997000)  |  35.0 min  
 6.4  152.1   43016   0.001000  |  0.025208    (0.992188)  |  0.010619    (0.996667)  |  35.3 min  
 6.5  154.1   43582   0.001000  |  0.001422    (1.000000)  |  0.011834    (0.995667)  |  35.6 min  
 6.6  156.1   44148   0.001000  |  0.000898    (1.000000)  |  0.011796    (0.996333)  |  35.9 min  
 6.7  158.1   44714   0.001000  |  0.000776    (1.000000)  |  0.011957    (0.996667)  |  36.3 min  
 6.7  160.1   45280   0.001000  |  0.022562    (0.992188)  |  0.011411    (0.997000)  |  36.6 min  
 6.8  162.1   45846   0.001000  |  0.000665    (1.000000)  |  0.010822    (0.996667)  |  36.9 min  
 6.9  164.1   46412   0.001000  |  0.052245    (0.992188)  |  0.011168    (0.996667)  |  37.2 min  
 7.0  166.1   46978   0.001000  |  0.029671    (0.992188)  |  0.010328    (0.997333)  |  37.5 min  
 7.1  168.1   47544   0.001000  |  0.022897    (0.992188)  |  0.011083    (0.996333)  |  39.4 min  
 7.2  170.1   48110   0.001000  |  0.012670    (1.000000)  |  0.010476    (0.997000)  |  39.7 min  
 7.2  172.1   48676   0.001000  |  0.020703    (0.992188)  |  0.010916    (0.996667)  |  40.1 min  
 7.3  174.1   49242   0.001000  |  0.033586    (0.992188)  |  0.010392    (0.996333)  |  40.4 min  
 7.4  176.1   49808   0.001000  |  0.064590    (0.976562)  |  0.010984    (0.996333)  |  40.7 min  
 7.5  178.1   50374   0.001000  |  0.000574    (1.000000)  |  0.010701    (0.996333)  |  41.0 min  
 7.6  180.1   50940   0.001000  |  0.043239    (0.984375)  |  0.010227    (0.997000)  |  41.3 min  
 7.7  182.1   51506   0.001000  |  0.022599    (0.992188)  |  0.010988    (0.997000)  |  41.7 min  
 7.8  184.1   52072   0.001000  |  0.007521    (0.992188)  |  0.010361    (0.997333)  |  42.0 min  
 7.8  186.1   52638   0.001000  |  0.003525    (1.000000)  |  0.010765    (0.996333)  |  42.3 min  
 7.9  188.1   53204   0.001000  |  0.000722    (1.000000)  |  0.011328    (0.996667)  |  42.6 min  
 8.0  190.1   53770   0.000100  |  0.029900    (0.984375)  |  0.010695    (0.997000)  |  44.5 min  
 8.1  192.1   54336   0.000100  |  0.036615    (0.984375)  |  0.011182    (0.997000)  |  44.8 min  
 8.2  194.1   54902   0.000100  |  0.068445    (0.976562)  |  0.010086    (0.997000)  |  45.2 min  
 8.3  196.1   55468   0.000100  |  0.036431    (0.992188)  |  0.010506    (0.997000)  |  45.5 min  
 8.3  198.1   56034   0.000100  |  0.010648    (0.992188)  |  0.011257    (0.996333)  |  45.8 min  
 8.4  200.1   56600   0.000100  |  0.000442    (1.000000)  |  0.011298    (0.996333)  |  46.1 min  
 8.5  202.1   57166   0.000100  |  0.000451    (1.000000)  |  0.010738    (0.997000)  |  46.5 min  
 8.6  204.1   57732   0.000100  |  0.047743    (0.992188)  |  0.010489    (0.997000)  |  46.8 min  
 8.7  206.1   58298   0.000100  |  0.040364    (0.984375)  |  0.010599    (0.997000)  |  47.1 min  
 8.8  208.1   58864   0.000100  |  0.000150    (1.000000)  |  0.011076    (0.996000)  |  47.4 min  
 8.8  210.1   59430   0.000100  |  0.014561    (1.000000)  |  0.011512    (0.997333)  |  47.8 min  
 8.9  212.1   59996   0.000100  |  0.020308    (0.992188)  |  0.010382    (0.996667)  |  48.1 min  
 9.0  213.7   60453   0.000100  |  0.033166    (0.992188)  |  0.011222    (0.997000)  |  48.4 min  

** evaluation on test set **
test_loss=0.181845    (test_acc=0.959937)

sucess
--- [START 2017-02-25 04:44:01] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_2(input_shape=(1, 1, 1), output_shape=(1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block1 = bn(block1)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block2 = bn(block2)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=False)
        block3 = bn(block3)
        block3 = relu(block3)
        block3 = dropout(block3, keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=False)
        block4 = bn(block4)
        block4 = relu(block4)
        block4 = dropout(block4, keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  2.419962    (0.257812)  |  1.544730    (0.493667)  |   2.7 min  
 0.2    4.0   01132   0.100000  |  2.151333    (0.335938)  |  0.957586    (0.714667)  |   3.0 min  
 0.3    6.0   01698   0.100000  |  1.621366    (0.531250)  |  0.624163    (0.800667)  |   3.2 min  
 0.3    8.0   02264   0.100000  |  1.277227    (0.617188)  |  0.424778    (0.897000)  |   3.5 min  
 0.4   10.0   02830   0.100000  |  1.161059    (0.656250)  |  0.322448    (0.909667)  |   3.7 min  
 0.5   12.0   03396   0.100000  |  1.128621    (0.664062)  |  0.264080    (0.925667)  |   4.0 min  
 0.6   14.0   03962   0.100000  |  0.905867    (0.773438)  |  0.236829    (0.920667)  |   4.3 min  
 0.7   16.0   04528   0.100000  |  0.718405    (0.781250)  |  0.176057    (0.958333)  |   4.5 min  
 0.8   18.0   05094   0.100000  |  0.749810    (0.773438)  |  0.153945    (0.947000)  |   4.8 min  
 0.8   20.0   05660   0.100000  |  0.742555    (0.820312)  |  0.110675    (0.964333)  |   5.0 min  
 0.9   22.0   06226   0.100000  |  0.723688    (0.773438)  |  0.090722    (0.979000)  |   5.3 min  
 1.0   24.0   06792   0.100000  |  0.794578    (0.789062)  |  0.076502    (0.978333)  |   8.1 min  
 1.1   26.0   07358   0.100000  |  0.909905    (0.710938)  |  0.066036    (0.986333)  |   8.3 min  
 1.2   28.0   07924   0.100000  |  0.619858    (0.789062)  |  0.057538    (0.983333)  |   8.6 min  
 1.3   30.0   08490   0.100000  |  0.777490    (0.742188)  |  0.078596    (0.972333)  |   8.8 min  
 1.3   32.0   09056   0.100000  |  0.512789    (0.875000)  |  0.059833    (0.983667)  |   9.1 min  
 1.4   34.0   09622   0.100000  |  0.528293    (0.859375)  |  0.031891    (0.993000)  |   9.3 min  
 1.5   36.0   10188   0.100000  |  0.532824    (0.867188)  |  0.050048    (0.986667)  |   9.6 min  
 1.6   38.0   10754   0.100000  |  0.621374    (0.828125)  |  0.037298    (0.993000)  |   9.9 min  
 1.7   40.0   11320   0.100000  |  0.638942    (0.789062)  |  0.037166    (0.989667)  |  10.1 min  
 1.8   42.0   11886   0.100000  |  0.419714    (0.851562)  |  0.032620    (0.992667)  |  10.4 min  
 1.9   44.0   12452   0.100000  |  0.580141    (0.835938)  |  0.044256    (0.990000)  |  10.6 min  
 1.9   46.0   13018   0.100000  |  0.502819    (0.859375)  |  0.038814    (0.990667)  |  10.9 min  
 2.0   48.0   13584   0.100000  |  0.532977    (0.859375)  |  0.026195    (0.993667)  |  13.7 min  
 2.1   50.0   14150   0.100000  |  0.405184    (0.890625)  |  0.020362    (0.995000)  |  13.9 min  
 2.2   52.0   14716   0.100000  |  0.486437    (0.835938)  |  0.025692    (0.993000)  |  14.2 min  
 2.3   54.0   15282   0.100000  |  0.444454    (0.867188)  |  0.023940    (0.992667)  |  14.4 min  
 2.4   56.0   15848   0.100000  |  0.544866    (0.859375)  |  0.020337    (0.993333)  |  14.7 min  
 2.4   58.0   16414   0.100000  |  0.396728    (0.875000)  |  0.016712    (0.997000)  |  14.9 min  
 2.5   60.0   16980   0.100000  |  0.496867    (0.882812)  |  0.020873    (0.994333)  |  15.2 min  
 2.6   62.0   17546   0.100000  |  0.465471    (0.867188)  |  0.022505    (0.995667)  |  15.4 min  
 2.7   64.0   18112   0.100000  |  0.525606    (0.867188)  |  0.020416    (0.995000)  |  15.7 min  
 2.8   66.0   18678   0.100000  |  0.599038    (0.859375)  |  0.017227    (0.996667)  |  16.0 min  
 2.9   68.0   19244   0.100000  |  0.335590    (0.906250)  |  0.020401    (0.994000)  |  16.2 min  
 2.9   70.0   19810   0.100000  |  0.515390    (0.843750)  |  0.013977    (0.997000)  |  16.5 min  
 3.0   72.0   20376   0.010000  |  0.497842    (0.859375)  |  0.007479    (0.998333)  |  19.2 min  
 3.1   74.0   20942   0.010000  |  0.267684    (0.890625)  |  0.006945    (0.998333)  |  19.5 min  
 3.2   76.0   21508   0.010000  |  0.339762    (0.898438)  |  0.006443    (0.999000)  |  19.7 min  
 3.3   78.0   22074   0.010000  |  0.451396    (0.859375)  |  0.006770    (0.998667)  |  20.0 min  
 3.4   80.0   22640   0.010000  |  0.344351    (0.906250)  |  0.006046    (0.998333)  |  20.2 min  
 3.5   82.0   23206   0.010000  |  0.304916    (0.906250)  |  0.006254    (0.998667)  |  20.5 min  
 3.5   84.0   23772   0.010000  |  0.215315    (0.937500)  |  0.006766    (0.998667)  |  20.8 min  
 3.6   86.0   24338   0.010000  |  0.265501    (0.945312)  |  0.005207    (0.999000)  |  21.0 min  
 3.7   88.0   24904   0.010000  |  0.274649    (0.914062)  |  0.006365    (0.998667)  |  21.3 min  
 3.8   90.0   25470   0.010000  |  0.421134    (0.859375)  |  0.006717    (0.998333)  |  21.5 min  
 3.9   92.0   26036   0.010000  |  0.212572    (0.945312)  |  0.005751    (0.999000)  |  21.8 min  
 4.0   94.0   26602   0.010000  |  0.156447    (0.953125)  |  0.005325    (0.999667)  |  22.1 min  
 4.0   96.0   27168   0.010000  |  0.183934    (0.953125)  |  0.005077    (0.999000)  |  24.8 min  
 4.1   98.0   27734   0.010000  |  0.243892    (0.945312)  |  0.005522    (0.999000)  |  25.1 min  
 4.2  100.0   28300   0.010000  |  0.399373    (0.875000)  |  0.004685    (0.999000)  |  25.4 min  
 4.3  102.0   28866   0.010000  |  0.176348    (0.937500)  |  0.005321    (0.999000)  |  25.6 min  
 4.4  104.0   29432   0.010000  |  0.265283    (0.921875)  |  0.004022    (0.999333)  |  25.9 min  
 4.5  106.0   29998   0.010000  |  0.273098    (0.953125)  |  0.004802    (0.998667)  |  26.1 min  
 4.6  108.0   30564   0.010000  |  0.325859    (0.898438)  |  0.006282    (0.998333)  |  26.4 min  
 4.6  110.0   31130   0.010000  |  0.321927    (0.898438)  |  0.004979    (0.998667)  |  26.7 min  
 4.7  112.0   31696   0.010000  |  0.292833    (0.906250)  |  0.005361    (0.998667)  |  26.9 min  
 4.8  114.0   32262   0.010000  |  0.391897    (0.890625)  |  0.004896    (0.998667)  |  27.2 min  
 4.9  116.0   32828   0.010000  |  0.192574    (0.945312)  |  0.006101    (0.998333)  |  27.5 min  
 5.0  118.0   33394   0.010000  |  0.299682    (0.890625)  |  0.004839    (0.999333)  |  27.7 min  
 5.1  120.0   33960   0.010000  |  0.375523    (0.898438)  |  0.004327    (0.999333)  |  30.5 min  
 5.1  122.1   34526   0.010000  |  0.493321    (0.859375)  |  0.004316    (0.999000)  |  30.7 min  
 5.2  124.1   35092   0.010000  |  0.381997    (0.906250)  |  0.006336    (0.998000)  |  31.0 min  
 5.3  126.1   35658   0.010000  |  0.168850    (0.953125)  |  0.004731    (0.999000)  |  31.2 min  
 5.4  128.1   36224   0.010000  |  0.261009    (0.929688)  |  0.004862    (0.998667)  |  31.5 min  
 5.5  130.1   36790   0.010000  |  0.350541    (0.906250)  |  0.003922    (0.999333)  |  31.7 min  
 5.6  132.1   37356   0.010000  |  0.384171    (0.898438)  |  0.004958    (0.999000)  |  32.0 min  
 5.6  134.1   37922   0.010000  |  0.329220    (0.898438)  |  0.005127    (0.999000)  |  32.3 min  
 5.7  136.1   38488   0.010000  |  0.157940    (0.953125)  |  0.004661    (0.998333)  |  32.5 min  
 5.8  138.1   39054   0.010000  |  0.261849    (0.937500)  |  0.004966    (0.998000)  |  32.8 min  
 5.9  140.1   39620   0.010000  |  0.213850    (0.937500)  |  0.005000    (0.998667)  |  33.0 min  
 6.0  142.1   40186   0.010000  |  0.239397    (0.937500)  |  0.004594    (0.999000)  |  33.3 min  
 6.1  144.1   40752   0.001000  |  0.309012    (0.906250)  |  0.003982    (0.999333)  |  36.0 min  
 6.2  146.1   41318   0.001000  |  0.245951    (0.937500)  |  0.003794    (0.999333)  |  36.3 min  
 6.2  148.1   41884   0.001000  |  0.245715    (0.937500)  |  0.004151    (0.999000)  |  36.6 min  
 6.3  150.1   42450   0.001000  |  0.332944    (0.906250)  |  0.004155    (0.999333)  |  36.8 min  
 6.4  152.1   43016   0.001000  |  0.373246    (0.906250)  |  0.004167    (0.999667)  |  37.1 min  
 6.5  154.1   43582   0.001000  |  0.525177    (0.851562)  |  0.004141    (0.999333)  |  37.3 min  
 6.6  156.1   44148   0.001000  |  0.185332    (0.953125)  |  0.004182    (0.998667)  |  37.6 min  
 6.7  158.1   44714   0.001000  |  0.374502    (0.898438)  |  0.004334    (0.999333)  |  37.8 min  
 6.7  160.1   45280   0.001000  |  0.243386    (0.921875)  |  0.004238    (0.999333)  |  38.1 min  
 6.8  162.1   45846   0.001000  |  0.227522    (0.945312)  |  0.004301    (0.999000)  |  38.4 min  
 6.9  164.1   46412   0.001000  |  0.283709    (0.914062)  |  0.004384    (0.998667)  |  38.6 min  
 7.0  166.1   46978   0.001000  |  0.171034    (0.945312)  |  0.004369    (0.999000)  |  38.9 min  
 7.1  168.1   47544   0.001000  |  0.226537    (0.945312)  |  0.003778    (0.999333)  |  41.6 min  
 7.2  170.1   48110   0.001000  |  0.179177    (0.960938)  |  0.004425    (0.999333)  |  41.9 min  
 7.2  172.1   48676   0.001000  |  0.324028    (0.906250)  |  0.004424    (0.999333)  |  42.1 min  
 7.3  174.1   49242   0.001000  |  0.182519    (0.929688)  |  0.004303    (0.999333)  |  42.4 min  
 7.4  176.1   49808   0.001000  |  0.204367    (0.945312)  |  0.004251    (0.998333)  |  42.6 min  
 7.5  178.1   50374   0.001000  |  0.178504    (0.937500)  |  0.003772    (0.999333)  |  42.9 min  
 7.6  180.1   50940   0.001000  |  0.287561    (0.906250)  |  0.004492    (0.999000)  |  43.2 min  
 7.7  182.1   51506   0.001000  |  0.163067    (0.968750)  |  0.003455    (0.999333)  |  43.4 min  
 7.8  184.1   52072   0.001000  |  0.346523    (0.921875)  |  0.003928    (0.999000)  |  43.7 min  
 7.8  186.1   52638   0.001000  |  0.362468    (0.921875)  |  0.004188    (0.999000)  |  43.9 min  
 7.9  188.1   53204   0.001000  |  0.445642    (0.875000)  |  0.004488    (0.998667)  |  44.2 min  
 8.0  190.1   53770   0.000100  |  0.295078    (0.890625)  |  0.004094    (0.998667)  |  47.0 min  
 8.1  192.1   54336   0.000100  |  0.203418    (0.937500)  |  0.004078    (0.998333)  |  47.2 min  
 8.2  194.1   54902   0.000100  |  0.319761    (0.921875)  |  0.004009    (0.998667)  |  47.5 min  
 8.3  196.1   55468   0.000100  |  0.298837    (0.929688)  |  0.003978    (0.998667)  |  47.7 min  
 8.3  198.1   56034   0.000100  |  0.184077    (0.968750)  |  0.003653    (0.999000)  |  48.0 min  
 8.4  200.1   56600   0.000100  |  0.177928    (0.945312)  |  0.003626    (0.998667)  |  48.2 min  
 8.5  202.1   57166   0.000100  |  0.402347    (0.898438)  |  0.003917    (0.999000)  |  48.5 min  
 8.6  204.1   57732   0.000100  |  0.188080    (0.945312)  |  0.003828    (0.998667)  |  48.8 min  
 8.7  206.1   58298   0.000100  |  0.291267    (0.906250)  |  0.003786    (0.999000)  |  49.0 min  
 8.8  208.1   58864   0.000100  |  0.305578    (0.882812)  |  0.003351    (0.999333)  |  49.3 min  
 8.8  210.1   59430   0.000100  |  0.319434    (0.890625)  |  0.003912    (0.998667)  |  49.6 min  
 8.9  212.1   59996   0.000100  |  0.187962    (0.929688)  |  0.003900    (0.999000)  |  49.8 min  
 9.0  213.7   60453   0.000100  |  0.266624    (0.929688)  |  0.003684    (0.998667)  |  50.0 min  

** evaluation on test set **
test_loss=0.072976    (test_acc=0.979889)

sucess
--- [START 2017-02-25 06:11:37] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.1, 0.01, 0.001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_2(input_shape=(1, 1, 1), output_shape=(1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block1 = bn(block1)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME',
                        has_bias=False)
        block2 = bn(block2)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=False)
        block3 = bn(block3)
        block3 = relu(block3)
        block3 = dropout(block3, keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=False)
        block4 = bn(block4)
        block4 = relu(block4)
        block4 = dropout(block4, keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.100000  |  1.150059    (0.671875)  |  0.525531    (0.847000)  |   1.8 min  
 0.2    4.0   01132   0.100000  |  0.745144    (0.804688)  |  0.199944    (0.938667)  |   2.0 min  
 0.3    6.0   01698   0.100000  |  0.325554    (0.914062)  |  0.177798    (0.959333)  |   2.3 min  
 0.3    8.0   02264   0.100000  |  0.459363    (0.859375)  |  0.094228    (0.972667)  |   2.5 min  
 0.4   10.0   02830   0.100000  |  0.339623    (0.906250)  |  0.063214    (0.981000)  |   2.8 min  
 0.5   12.0   03396   0.100000  |  0.338716    (0.914062)  |  0.051773    (0.984667)  |   3.1 min  
 0.6   14.0   03962   0.100000  |  0.341947    (0.906250)  |  0.049483    (0.986333)  |   3.3 min  
 0.7   16.0   04528   0.100000  |  0.181437    (0.937500)  |  0.030504    (0.991667)  |   3.6 min  
 0.8   18.0   05094   0.100000  |  0.164876    (0.937500)  |  0.034475    (0.988333)  |   3.8 min  
 0.8   20.0   05660   0.100000  |  0.395350    (0.890625)  |  0.027988    (0.992667)  |   4.1 min  
 0.9   22.0   06226   0.100000  |  0.258136    (0.937500)  |  0.028036    (0.993000)  |   4.4 min  
 1.0   24.0   06792   0.100000  |  0.370629    (0.914062)  |  0.026918    (0.991000)  |   6.2 min  
 1.1   26.0   07358   0.100000  |  0.155142    (0.945312)  |  0.032116    (0.992000)  |   6.4 min  
 1.2   28.0   07924   0.100000  |  0.242428    (0.937500)  |  0.023990    (0.994000)  |   6.7 min  
 1.3   30.0   08490   0.100000  |  0.265930    (0.914062)  |  0.025404    (0.992667)  |   6.9 min  
 1.3   32.0   09056   0.100000  |  0.282867    (0.945312)  |  0.024691    (0.993333)  |   7.2 min  
 1.4   34.0   09622   0.100000  |  0.036754    (0.992188)  |  0.040218    (0.993667)  |   7.4 min  
 1.5   36.0   10188   0.100000  |  0.201673    (0.945312)  |  0.015448    (0.995667)  |   7.7 min  
 1.6   38.0   10754   0.100000  |  0.170183    (0.968750)  |  0.015218    (0.996333)  |   8.0 min  
 1.7   40.0   11320   0.100000  |  0.292235    (0.921875)  |  0.015450    (0.995667)  |   8.2 min  
 1.8   42.0   11886   0.100000  |  0.155947    (0.953125)  |  0.017344    (0.995333)  |   8.5 min  
 1.9   44.0   12452   0.100000  |  0.148475    (0.937500)  |  0.015489    (0.996333)  |   8.7 min  
 1.9   46.0   13018   0.100000  |  0.059886    (0.968750)  |  0.014223    (0.996000)  |   9.0 min  
 2.0   48.0   13584   0.100000  |  0.136684    (0.968750)  |  0.012572    (0.996667)  |  10.8 min  
 2.1   50.0   14150   0.100000  |  0.143743    (0.968750)  |  0.011248    (0.997333)  |  11.1 min  
 2.2   52.0   14716   0.100000  |  0.116578    (0.976562)  |  0.015549    (0.996000)  |  11.3 min  
 2.3   54.0   15282   0.100000  |  0.090526    (0.968750)  |  0.011519    (0.997333)  |  11.6 min  
 2.4   56.0   15848   0.100000  |  0.061613    (0.984375)  |  0.013810    (0.997333)  |  11.8 min  
 2.4   58.0   16414   0.100000  |  0.141967    (0.960938)  |  0.012354    (0.997667)  |  12.1 min  
 2.5   60.0   16980   0.100000  |  0.223374    (0.937500)  |  0.010121    (0.998000)  |  12.4 min  
 2.6   62.0   17546   0.100000  |  0.054308    (0.992188)  |  0.010441    (0.997333)  |  12.6 min  
 2.7   64.0   18112   0.100000  |  0.120653    (0.960938)  |  0.011221    (0.997000)  |  12.9 min  
 2.8   66.0   18678   0.100000  |  0.082573    (0.968750)  |  0.010354    (0.997000)  |  13.1 min  
 2.9   68.0   19244   0.100000  |  0.086202    (0.976562)  |  0.014016    (0.996333)  |  13.4 min  
 2.9   70.0   19810   0.100000  |  0.236597    (0.937500)  |  0.016428    (0.996000)  |  13.7 min  
 3.0   72.0   20376   0.010000  |  0.093667    (0.976562)  |  0.011429    (0.997667)  |  15.4 min  
 3.1   74.0   20942   0.010000  |  0.140727    (0.968750)  |  0.010343    (0.998000)  |  15.7 min  
 3.2   76.0   21508   0.010000  |  0.040894    (0.984375)  |  0.010598    (0.998000)  |  16.0 min  
 3.3   78.0   22074   0.010000  |  0.065335    (0.984375)  |  0.009773    (0.998333)  |  16.2 min  
 3.4   80.0   22640   0.010000  |  0.180309    (0.945312)  |  0.010886    (0.998000)  |  16.5 min  
 3.5   82.0   23206   0.010000  |  0.059285    (0.976562)  |  0.008445    (0.998000)  |  16.7 min  
 3.5   84.0   23772   0.010000  |  0.113108    (0.968750)  |  0.009133    (0.997667)  |  17.0 min  
 3.6   86.0   24338   0.010000  |  0.051174    (0.992188)  |  0.008853    (0.998333)  |  17.3 min  
 3.7   88.0   24904   0.010000  |  0.186132    (0.960938)  |  0.008944    (0.997667)  |  17.5 min  
 3.8   90.0   25470   0.010000  |  0.105381    (0.960938)  |  0.008757    (0.997667)  |  17.8 min  
 3.9   92.0   26036   0.010000  |  0.083177    (0.976562)  |  0.007999    (0.998000)  |  18.1 min  
 4.0   94.0   26602   0.010000  |  0.062935    (0.984375)  |  0.008759    (0.998333)  |  18.3 min  
 4.0   96.0   27168   0.010000  |  0.102205    (0.968750)  |  0.008252    (0.998000)  |  20.1 min  
 4.1   98.0   27734   0.010000  |  0.028206    (0.984375)  |  0.008270    (0.998000)  |  20.4 min  
 4.2  100.0   28300   0.010000  |  0.119273    (0.968750)  |  0.007452    (0.998333)  |  20.7 min  
 4.3  102.0   28866   0.010000  |  0.154555    (0.976562)  |  0.008032    (0.998000)  |  20.9 min  
 4.4  104.0   29432   0.010000  |  0.074463    (0.968750)  |  0.007828    (0.998667)  |  21.2 min  
 4.5  106.0   29998   0.010000  |  0.095531    (0.976562)  |  0.007695    (0.998333)  |  21.5 min  
 4.6  108.0   30564   0.010000  |  0.107144    (0.968750)  |  0.006233    (0.998667)  |  21.7 min  
 4.6  110.0   31130   0.010000  |  0.046506    (0.984375)  |  0.007592    (0.998333)  |  22.0 min  
 4.7  112.0   31696   0.010000  |  0.101683    (0.976562)  |  0.007064    (0.998333)  |  22.3 min  
 4.8  114.0   32262   0.010000  |  0.116835    (0.968750)  |  0.007784    (0.998333)  |  22.5 min  
 4.9  116.0   32828   0.010000  |  0.066782    (0.968750)  |  0.006952    (0.998667)  |  22.8 min  
 5.0  118.0   33394   0.010000  |  0.112183    (0.960938)  |  0.006100    (0.998667)  |  23.1 min  
 5.1  120.0   33960   0.010000  |  0.116001    (0.968750)  |  0.006885    (0.998333)  |  24.8 min  
 5.1  122.1   34526   0.010000  |  0.081550    (0.968750)  |  0.007776    (0.998333)  |  25.1 min  
 5.2  124.1   35092   0.010000  |  0.141204    (0.960938)  |  0.007174    (0.998333)  |  25.4 min  
 5.3  126.1   35658   0.010000  |  0.097914    (0.976562)  |  0.006529    (0.998667)  |  25.6 min  
 5.4  128.1   36224   0.010000  |  0.148224    (0.960938)  |  0.007029    (0.998000)  |  25.9 min  
 5.5  130.1   36790   0.010000  |  0.101332    (0.976562)  |  0.006510    (0.998333)  |  26.2 min  
 5.6  132.1   37356   0.010000  |  0.005293    (1.000000)  |  0.007065    (0.998667)  |  26.4 min  
 5.6  134.1   37922   0.010000  |  0.164178    (0.953125)  |  0.006330    (0.998667)  |  26.7 min  
 5.7  136.1   38488   0.010000  |  0.096785    (0.976562)  |  0.006029    (0.998000)  |  27.0 min  
 5.8  138.1   39054   0.010000  |  0.032427    (0.992188)  |  0.006009    (0.998333)  |  27.2 min  
 5.9  140.1   39620   0.010000  |  0.066213    (0.984375)  |  0.005962    (0.998667)  |  27.5 min  
 6.0  142.1   40186   0.010000  |  0.070996    (0.984375)  |  0.008425    (0.997333)  |  27.8 min  
 6.1  144.1   40752   0.001000  |  0.077442    (0.976562)  |  0.006912    (0.998000)  |  29.5 min  
 6.2  146.1   41318   0.001000  |  0.173651    (0.937500)  |  0.006521    (0.998333)  |  29.8 min  
 6.2  148.1   41884   0.001000  |  0.045857    (0.984375)  |  0.006465    (0.998333)  |  30.0 min  
 6.3  150.1   42450   0.001000  |  0.130825    (0.953125)  |  0.007008    (0.998333)  |  30.3 min  
 6.4  152.1   43016   0.001000  |  0.084659    (0.976562)  |  0.006081    (0.998667)  |  30.6 min  
 6.5  154.1   43582   0.001000  |  0.033483    (0.992188)  |  0.006123    (0.998667)  |  30.8 min  
 6.6  156.1   44148   0.001000  |  0.046299    (0.976562)  |  0.006301    (0.998667)  |  31.1 min  
 6.7  158.1   44714   0.001000  |  0.024733    (0.984375)  |  0.006576    (0.998667)  |  31.4 min  
 6.7  160.1   45280   0.001000  |  0.059618    (0.984375)  |  0.005884    (0.998000)  |  31.6 min  
 6.8  162.1   45846   0.001000  |  0.064087    (0.976562)  |  0.007000    (0.998333)  |  31.9 min  
 6.9  164.1   46412   0.001000  |  0.074528    (0.976562)  |  0.006657    (0.998333)  |  32.2 min  
 7.0  166.1   46978   0.001000  |  0.066794    (0.968750)  |  0.006410    (0.998333)  |  32.4 min  
 7.1  168.1   47544   0.001000  |  0.109028    (0.968750)  |  0.006216    (0.998000)  |  34.2 min  
 7.2  170.1   48110   0.001000  |  0.058013    (0.992188)  |  0.006268    (0.998000)  |  34.5 min  
 7.2  172.1   48676   0.001000  |  0.083522    (0.976562)  |  0.006335    (0.998000)  |  34.7 min  
 7.3  174.1   49242   0.001000  |  0.091339    (0.968750)  |  0.006546    (0.997667)  |  35.0 min  
 7.4  176.1   49808   0.001000  |  0.146869    (0.968750)  |  0.006614    (0.998667)  |  35.3 min  
 7.5  178.1   50374   0.001000  |  0.188774    (0.976562)  |  0.006187    (0.998000)  |  35.5 min  
 7.6  180.1   50940   0.001000  |  0.064369    (0.984375)  |  0.005816    (0.998000)  |  35.8 min  
 7.7  182.1   51506   0.001000  |  0.082128    (0.968750)  |  0.006683    (0.998000)  |  36.1 min  
 7.8  184.1   52072   0.001000  |  0.083381    (0.976562)  |  0.006138    (0.998333)  |  36.3 min  
 7.8  186.1   52638   0.001000  |  0.040831    (0.992188)  |  0.006105    (0.998333)  |  36.6 min  
 7.9  188.1   53204   0.001000  |  0.037142    (0.984375)  |  0.006200    (0.998667)  |  36.9 min  
 8.0  190.1   53770   0.000100  |  0.094411    (0.976562)  |  0.006153    (0.998333)  |  38.6 min  
 8.1  192.1   54336   0.000100  |  0.059613    (0.984375)  |  0.006038    (0.998667)  |  38.9 min  
 8.2  194.1   54902   0.000100  |  0.122457    (0.968750)  |  0.006491    (0.998333)  |  39.2 min  
 8.3  196.1   55468   0.000100  |  0.096878    (0.968750)  |  0.005956    (0.998333)  |  39.4 min  
 8.3  198.1   56034   0.000100  |  0.059445    (0.984375)  |  0.005843    (0.998000)  |  39.7 min  
 8.4  200.1   56600   0.000100  |  0.193438    (0.992188)  |  0.005784    (0.998333)  |  39.9 min  
 8.5  202.1   57166   0.000100  |  0.006492    (1.000000)  |  0.005982    (0.998333)  |  40.2 min  
 8.6  204.1   57732   0.000100  |  0.066974    (0.976562)  |  0.005690    (0.998333)  |  40.5 min  
 8.7  206.1   58298   0.000100  |  0.122859    (0.953125)  |  0.005964    (0.998333)  |  40.7 min  
 8.8  208.1   58864   0.000100  |  0.025461    (0.992188)  |  0.005201    (0.998333)  |  41.0 min  
 8.8  210.1   59430   0.000100  |  0.039114    (0.992188)  |  0.006770    (0.998000)  |  41.3 min  
 8.9  212.1   59996   0.000100  |  0.055905    (0.984375)  |  0.005880    (0.998000)  |  41.5 min  
 9.0  213.7   60453   0.000100  |  0.050434    (0.984375)  |  0.005976    (0.998333)  |  41.8 min  

** evaluation on test set **
test_loss=0.083175    (test_acc=0.981710)

sucess
--- [START 2017-02-25 09:37:05] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/project/udacity/project02-basic/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  0.949984    (0.789062)  |  0.455895    (0.855000)  |   1.6 min  
 0.2    4.0   01132   0.010000  |  0.478957    (0.890625)  |  0.318601    (0.914667)  |   1.8 min  
 0.3    6.0   01698   0.010000  |  0.111331    (0.976562)  |  0.182661    (0.954333)  |   1.9 min  
 0.3    8.0   02264   0.010000  |  0.205940    (0.929688)  |  0.182553    (0.956333)  |   2.1 min  
 0.4   10.0   02830   0.010000  |  0.108643    (0.968750)  |  0.139382    (0.960667)  |   2.2 min  
 0.5   12.0   03396   0.010000  |  0.117086    (0.960938)  |  0.128371    (0.971667)  |   2.4 min  
 0.6   14.0   03962   0.010000  |  0.090946    (0.976562)  |  0.108462    (0.976667)  |   2.6 min  
 0.7   16.0   04528   0.010000  |  0.045240    (0.984375)  |  0.101627    (0.978667)  |   2.7 min  
 0.8   18.0   05094   0.010000  |  0.081889    (0.968750)  |  0.147654    (0.970000)  |   2.9 min  
 0.8   20.0   05660   0.010000  |  0.141360    (0.984375)  |  0.156008    (0.973000)  |   3.0 min  
 0.9   22.0   06226   0.010000  |  0.087273    (0.976562)  |  0.114551    (0.978000)  |   3.2 min  
 1.0   24.0   06792   0.010000  |  0.352245    (0.937500)  |  0.088876    (0.983667)  |   4.8 min  
 1.1   26.0   07358   0.010000  |  0.147658    (0.960938)  |  0.095331    (0.985000)  |   5.0 min  
 1.2   28.0   07924   0.010000  |  0.179558    (0.953125)  |  0.083326    (0.980000)  |   5.1 min  
 1.3   30.0   08490   0.010000  |  0.063153    (0.976562)  |  0.091139    (0.986000)  |   5.3 min  
 1.3   32.0   09056   0.010000  |  0.079985    (0.976562)  |  0.091122    (0.987000)  |   5.5 min  
 1.4   34.0   09622   0.010000  |  0.016912    (0.992188)  |  0.080610    (0.986333)  |   5.6 min  
 1.5   36.0   10188   0.010000  |  0.094255    (0.976562)  |  0.076893    (0.986000)  |   5.8 min  
 1.6   38.0   10754   0.010000  |  0.039443    (0.992188)  |  0.092082    (0.985333)  |   5.9 min  
 1.7   40.0   11320   0.010000  |  0.184068    (0.953125)  |  0.090138    (0.984667)  |   6.1 min  
 1.8   42.0   11886   0.010000  |  0.072927    (0.976562)  |  0.082815    (0.987333)  |   6.3 min  
 1.9   44.0   12452   0.010000  |  0.038813    (0.992188)  |  0.082811    (0.987333)  |   6.4 min  
 1.9   46.0   13018   0.010000  |  0.005848    (1.000000)  |  0.082192    (0.987667)  |   6.6 min  
 2.0   48.0   13584   0.010000  |  0.099825    (0.976562)  |  0.078856    (0.987000)  |   8.2 min  
 2.1   50.0   14150   0.010000  |  0.087383    (0.968750)  |  0.092783    (0.986000)  |   8.4 min  
 2.2   52.0   14716   0.010000  |  0.046725    (0.984375)  |  0.087985    (0.986333)  |   8.5 min  
 2.3   54.0   15282   0.010000  |  0.030118    (0.992188)  |  0.099861    (0.987667)  |   8.7 min  
 2.4   56.0   15848   0.010000  |  0.016340    (0.992188)  |  0.109299    (0.985333)  |   8.9 min  
 2.4   58.0   16414   0.010000  |  0.060479    (0.976562)  |  0.107089    (0.986000)  |   9.0 min  
 2.5   60.0   16980   0.010000  |  0.125835    (0.968750)  |  0.084335    (0.988333)  |   9.2 min  
 2.6   62.0   17546   0.010000  |  0.106751    (0.984375)  |  0.084515    (0.990000)  |   9.4 min  
 2.7   64.0   18112   0.010000  |  0.088045    (0.976562)  |  0.081691    (0.988667)  |   9.5 min  
 2.8   66.0   18678   0.010000  |  0.044344    (0.984375)  |  0.074327    (0.990333)  |   9.7 min  
 2.9   68.0   19244   0.010000  |  0.104356    (0.976562)  |  0.096548    (0.987333)  |   9.9 min  
 2.9   70.0   19810   0.010000  |  0.098452    (0.968750)  |  0.079904    (0.989667)  |  10.0 min  
 3.0   72.0   20376   0.001000  |  0.059147    (0.968750)  |  0.069462    (0.991333)  |  11.6 min  
 3.1   74.0   20942   0.001000  |  0.065138    (0.984375)  |  0.071219    (0.992000)  |  11.8 min  
 3.2   76.0   21508   0.001000  |  0.004832    (1.000000)  |  0.072305    (0.990667)  |  11.9 min  
 3.3   78.0   22074   0.001000  |  0.039661    (0.992188)  |  0.071262    (0.992000)  |  12.1 min  
 3.4   80.0   22640   0.001000  |  0.118403    (0.945312)  |  0.069851    (0.992000)  |  12.3 min  
 3.5   82.0   23206   0.001000  |  0.019543    (0.992188)  |  0.071243    (0.992000)  |  12.4 min  
 3.5   84.0   23772   0.001000  |  0.039757    (0.984375)  |  0.065986    (0.992000)  |  12.6 min  
 3.6   86.0   24338   0.001000  |  0.030160    (0.992188)  |  0.066119    (0.992000)  |  12.8 min  
 3.7   88.0   24904   0.001000  |  0.026379    (0.992188)  |  0.066227    (0.992000)  |  12.9 min  
 3.8   90.0   25470   0.001000  |  0.045581    (0.984375)  |  0.065254    (0.991333)  |  13.1 min  
 3.9   92.0   26036   0.001000  |  0.056608    (0.984375)  |  0.069389    (0.991667)  |  13.3 min  
 4.0   94.0   26602   0.001000  |  0.047276    (0.992188)  |  0.065319    (0.991333)  |  13.4 min  
 4.0   96.0   27168   0.001000  |  0.004270    (1.000000)  |  0.071116    (0.991333)  |  15.1 min  
 4.1   98.0   27734   0.001000  |  0.001005    (1.000000)  |  0.074306    (0.991000)  |  15.2 min  
 4.2  100.0   28300   0.001000  |  0.062340    (0.976562)  |  0.068833    (0.990667)  |  15.4 min  
 4.3  102.0   28866   0.001000  |  0.054175    (0.984375)  |  0.070343    (0.991333)  |  15.5 min  
 4.4  104.0   29432   0.001000  |  0.014321    (0.992188)  |  0.069315    (0.992333)  |  15.7 min  
 4.5  106.0   29998   0.001000  |  0.079474    (0.984375)  |  0.068146    (0.991667)  |  15.9 min  
 4.6  108.0   30564   0.001000  |  0.066543    (0.984375)  |  0.069807    (0.990667)  |  16.0 min  
 4.6  110.0   31130   0.001000  |  0.025086    (0.992188)  |  0.066270    (0.991667)  |  16.2 min  
 4.7  112.0   31696   0.001000  |  0.013695    (0.992188)  |  0.069121    (0.992000)  |  16.4 min  
 4.8  114.0   32262   0.001000  |  0.037699    (0.992188)  |  0.073621    (0.992000)  |  16.5 min  
 4.9  116.0   32828   0.001000  |  0.023171    (0.992188)  |  0.072320    (0.991667)  |  16.7 min  
 5.0  118.0   33394   0.001000  |  0.010755    (0.992188)  |  0.069010    (0.991667)  |  16.9 min  
 5.1  120.0   33960   0.001000  |  0.014411    (0.992188)  |  0.074987    (0.991000)  |  18.5 min  
 5.1  122.1   34526   0.001000  |  0.043733    (0.992188)  |  0.074908    (0.990667)  |  18.6 min  
 5.2  124.1   35092   0.001000  |  0.084110    (0.976562)  |  0.074352    (0.991333)  |  18.8 min  
 5.3  126.1   35658   0.001000  |  0.057605    (0.984375)  |  0.072292    (0.991000)  |  18.9 min  
 5.4  128.1   36224   0.001000  |  0.064528    (0.984375)  |  0.075085    (0.990667)  |  19.1 min  
 5.5  130.1   36790   0.001000  |  0.000511    (1.000000)  |  0.071454    (0.991333)  |  19.3 min  
 5.6  132.1   37356   0.001000  |  0.004762    (1.000000)  |  0.072095    (0.991333)  |  19.4 min  
 5.6  134.1   37922   0.001000  |  0.070660    (0.976562)  |  0.071157    (0.991000)  |  19.6 min  
 5.7  136.1   38488   0.001000  |  0.029779    (0.992188)  |  0.072412    (0.991333)  |  19.8 min  
 5.8  138.1   39054   0.001000  |  0.012945    (0.992188)  |  0.073266    (0.991000)  |  19.9 min  
 5.9  140.1   39620   0.001000  |  0.025742    (0.992188)  |  0.070387    (0.992000)  |  20.1 min  
 6.0  142.1   40186   0.001000  |  0.047083    (0.992188)  |  0.070765    (0.991000)  |  20.3 min  
 6.1  144.1   40752   0.000100  |  0.054656    (0.984375)  |  0.069659    (0.991667)  |  21.9 min  
 6.2  146.1   41318   0.000100  |  0.076301    (0.976562)  |  0.069658    (0.991667)  |  22.0 min  
 6.2  148.1   41884   0.000100  |  0.020767    (0.992188)  |  0.069286    (0.991667)  |  22.2 min  
 6.3  150.1   42450   0.000100  |  0.056249    (0.984375)  |  0.069108    (0.991667)  |  22.3 min  
 6.4  152.1   43016   0.000100  |  0.056484    (0.984375)  |  0.070163    (0.991667)  |  22.5 min  
 6.5  154.1   43582   0.000100  |  0.003062    (1.000000)  |  0.070247    (0.991667)  |  22.7 min  
 6.6  156.1   44148   0.000100  |  0.001031    (1.000000)  |  0.070042    (0.991333)  |  22.8 min  
 6.7  158.1   44714   0.000100  |  0.000179    (1.000000)  |  0.070437    (0.991000)  |  23.0 min  
 6.7  160.1   45280   0.000100  |  0.023934    (0.992188)  |  0.070023    (0.991000)  |  23.2 min  
 6.8  162.1   45846   0.000100  |  0.016652    (1.000000)  |  0.070964    (0.991000)  |  23.4 min  
 6.9  164.1   46412   0.000100  |  0.031974    (0.992188)  |  0.071640    (0.991000)  |  23.5 min  
 7.0  166.1   46978   0.000100  |  0.022166    (0.992188)  |  0.071198    (0.991000)  |  23.7 min  
 7.1  168.1   47544   0.000100  |  0.052341    (0.984375)  |  0.071395    (0.991000)  |  25.3 min  
 7.2  170.1   48110   0.000100  |  0.026019    (0.992188)  |  0.070967    (0.991333)  |  25.5 min  
 7.2  172.1   48676   0.000100  |  0.018550    (0.992188)  |  0.070481    (0.991333)  |  25.6 min  
 7.3  174.1   49242   0.000100  |  0.063372    (0.976562)  |  0.070579    (0.991333)  |  25.8 min  
 7.4  176.1   49808   0.000100  |  0.070885    (0.976562)  |  0.071315    (0.991333)  |  26.0 min  
 7.5  178.1   50374   0.000100  |  0.000155    (1.000000)  |  0.071303    (0.991333)  |  26.1 min  
 7.6  180.1   50940   0.000100  |  0.028660    (0.992188)  |  0.070770    (0.991333)  |  26.3 min  
 7.7  182.1   51506   0.000100  |  0.034728    (0.992188)  |  0.071349    (0.992000)  |  26.5 min  
 7.8  184.1   52072   0.000100  |  0.033434    (0.992188)  |  0.070652    (0.991667)  |  26.6 min  
 7.8  186.1   52638   0.000100  |  0.002068    (1.000000)  |  0.070950    (0.991667)  |  26.8 min  
 7.9  188.1   53204   0.000100  |  0.000407    (1.000000)  |  0.071215    (0.991667)  |  27.0 min  
 8.0  190.1   53770   0.000100  |  0.036580    (0.992188)  |  0.071737    (0.991667)  |  28.6 min  
 8.1  192.1   54336   0.000100  |  0.042664    (0.984375)  |  0.071593    (0.992000)  |  28.7 min  
 8.2  194.1   54902   0.000100  |  0.068864    (0.976562)  |  0.071643    (0.991667)  |  28.9 min  
 8.3  196.1   55468   0.000100  |  0.034668    (0.992188)  |  0.071122    (0.992000)  |  29.1 min  
 8.3  198.1   56034   0.000100  |  0.020738    (0.992188)  |  0.071369    (0.992000)  |  29.2 min  
 8.4  200.1   56600   0.000100  |  0.000239    (1.000000)  |  0.071629    (0.992000)  |  29.4 min  
 8.5  202.1   57166   0.000100  |  0.000057    (1.000000)  |  0.071559    (0.991667)  |  29.6 min  
 8.6  204.1   57732   0.000100  |  0.055592    (0.984375)  |  0.071818    (0.991667)  |  29.7 min  
 8.7  206.1   58298   0.000100  |  0.035408    (0.992188)  |  0.071564    (0.991667)  |  29.9 min  
 8.8  208.1   58864   0.000100  |  0.000202    (1.000000)  |  0.072241    (0.991667)  |  30.1 min  
 8.8  210.1   59430   0.000100  |  0.017670    (1.000000)  |  0.072307    (0.991667)  |  30.2 min  
 8.9  212.1   59996   0.000100  |  0.024520    (0.992188)  |  0.072753    (0.991333)  |  30.4 min  
 9.0  213.7   60453   0.000100  |  0.037233    (0.992188)  |  0.072919    (0.991333)  |  30.5 min  

** evaluation on test set **
test_loss=0.596872    (test_acc=0.951227)

sucess
--- [START 2017-02-26 20:03:56] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
--- [START 2017-02-26 20:09:30] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  2.066639    (0.367188)  |  1.413443    (0.537000)  |   2.7 min  
 0.2    4.0   01132   0.010000  |  1.622117    (0.507812)  |  0.805137    (0.723667)  |   2.9 min  
 0.3    6.0   01698   0.010000  |  1.177753    (0.679688)  |  0.524537    (0.823333)  |   3.1 min  
 0.3    8.0   02264   0.010000  |  1.132903    (0.632812)  |  0.428578    (0.849000)  |   3.2 min  
 0.4   10.0   02830   0.010000  |  0.958296    (0.750000)  |  0.298851    (0.890333)  |   3.4 min  
 0.5   12.0   03396   0.010000  |  0.911369    (0.742188)  |  0.256435    (0.908333)  |   3.6 min  
 0.6   14.0   03962   0.010000  |  0.507086    (0.851562)  |  0.160415    (0.943667)  |   3.7 min  
 0.7   16.0   04528   0.010000  |  0.708167    (0.781250)  |  0.143182    (0.952333)  |   3.9 min  
 0.8   18.0   05094   0.010000  |  0.818521    (0.796875)  |  0.154346    (0.941000)  |   4.1 min  
 0.8   20.0   05660   0.010000  |  0.401529    (0.875000)  |  0.130630    (0.960333)  |   4.2 min  
 0.9   22.0   06226   0.010000  |  0.587793    (0.796875)  |  0.081274    (0.971333)  |   4.4 min  
 1.0   24.0   06792   0.010000  |  0.421519    (0.859375)  |  0.056743    (0.981333)  |   7.1 min  
 1.1   26.0   07358   0.010000  |  0.516116    (0.820312)  |  0.092578    (0.971333)  |   7.3 min  
 1.2   28.0   07924   0.010000  |  0.285831    (0.914062)  |  0.055302    (0.983333)  |   7.4 min  
 1.3   30.0   08490   0.010000  |  0.534130    (0.859375)  |  0.087955    (0.970333)  |   7.6 min  
 1.3   32.0   09056   0.010000  |  0.395367    (0.882812)  |  0.063291    (0.981333)  |   7.8 min  
 1.4   34.0   09622   0.010000  |  0.218672    (0.937500)  |  0.054409    (0.981333)  |   7.9 min  
 1.5   36.0   10188   0.010000  |  0.328650    (0.914062)  |  0.044570    (0.986333)  |   8.1 min  
 1.6   38.0   10754   0.010000  |  0.336243    (0.898438)  |  0.044640    (0.985667)  |   8.3 min  
 1.7   40.0   11320   0.010000  |  0.321558    (0.914062)  |  0.031914    (0.989333)  |   8.5 min  
 1.8   42.0   11886   0.010000  |  0.178864    (0.937500)  |  0.023108    (0.992000)  |   8.6 min  
 1.9   44.0   12452   0.010000  |  0.376245    (0.890625)  |  0.045232    (0.985667)  |   8.8 min  
 1.9   46.0   13018   0.010000  |  0.373113    (0.914062)  |  0.023949    (0.991000)  |   9.0 min  
 2.0   48.0   13584   0.010000  |  0.303160    (0.914062)  |  0.023386    (0.990000)  |  11.7 min  
 2.1   50.0   14150   0.010000  |  0.352061    (0.898438)  |  0.019959    (0.994000)  |  11.8 min  
 2.2   52.0   14716   0.010000  |  0.310145    (0.906250)  |  0.025016    (0.990667)  |  12.0 min  
 2.3   54.0   15282   0.010000  |  0.343924    (0.914062)  |  0.024401    (0.991000)  |  12.2 min  
 2.4   56.0   15848   0.010000  |  0.304921    (0.914062)  |  0.029082    (0.991000)  |  12.3 min  
 2.4   58.0   16414   0.010000  |  0.319410    (0.906250)  |  0.017972    (0.994333)  |  12.5 min  
 2.5   60.0   16980   0.010000  |  0.272993    (0.921875)  |  0.021530    (0.994667)  |  12.7 min  
 2.6   62.0   17546   0.010000  |  0.140890    (0.968750)  |  0.016095    (0.995333)  |  12.8 min  
 2.7   64.0   18112   0.010000  |  0.384623    (0.882812)  |  0.019181    (0.993000)  |  13.0 min  
 2.8   66.0   18678   0.010000  |  0.338422    (0.914062)  |  0.016995    (0.995333)  |  13.2 min  
 2.9   68.0   19244   0.010000  |  0.176805    (0.953125)  |  0.014290    (0.995000)  |  13.3 min  
 2.9   70.0   19810   0.010000  |  0.246046    (0.953125)  |  0.021219    (0.991000)  |  13.5 min  
 3.0   72.0   20376   0.001000  |  0.264562    (0.937500)  |  0.007061    (0.998333)  |  16.2 min  
 3.1   74.0   20942   0.001000  |  0.074119    (0.984375)  |  0.005770    (0.997667)  |  16.4 min  
 3.2   76.0   21508   0.001000  |  0.155924    (0.937500)  |  0.006054    (0.997667)  |  16.6 min  
 3.3   78.0   22074   0.001000  |  0.266751    (0.921875)  |  0.005191    (0.998000)  |  16.7 min  
 3.4   80.0   22640   0.001000  |  0.094549    (0.984375)  |  0.005563    (0.997667)  |  16.9 min  
 3.5   82.0   23206   0.001000  |  0.111084    (0.968750)  |  0.005814    (0.998000)  |  17.1 min  
 3.5   84.0   23772   0.001000  |  0.097824    (0.984375)  |  0.006097    (0.997667)  |  17.2 min  
 3.6   86.0   24338   0.001000  |  0.104254    (0.960938)  |  0.005376    (0.998000)  |  17.4 min  
 3.7   88.0   24904   0.001000  |  0.135071    (0.968750)  |  0.006107    (0.998000)  |  17.6 min  
 3.8   90.0   25470   0.001000  |  0.207889    (0.937500)  |  0.006102    (0.997333)  |  17.7 min  
 3.9   92.0   26036   0.001000  |  0.128446    (0.960938)  |  0.005709    (0.997667)  |  17.9 min  
 4.0   94.0   26602   0.001000  |  0.050556    (0.984375)  |  0.005225    (0.999000)  |  18.1 min  
 4.0   96.0   27168   0.001000  |  0.216283    (0.953125)  |  0.005244    (0.998667)  |  20.8 min  
 4.1   98.0   27734   0.001000  |  0.176243    (0.960938)  |  0.005531    (0.998667)  |  21.0 min  
 4.2  100.0   28300   0.001000  |  0.166851    (0.945312)  |  0.005176    (0.998000)  |  21.1 min  
 4.3  102.0   28866   0.001000  |  0.056612    (0.992188)  |  0.004908    (0.999000)  |  21.3 min  
 4.4  104.0   29432   0.001000  |  0.079417    (0.976562)  |  0.005660    (0.997667)  |  21.5 min  
 4.5  106.0   29998   0.001000  |  0.148423    (0.953125)  |  0.004947    (0.999000)  |  21.6 min  
 4.6  108.0   30564   0.001000  |  0.266288    (0.937500)  |  0.005225    (0.998667)  |  21.8 min  
 4.6  110.0   31130   0.001000  |  0.139903    (0.953125)  |  0.005249    (0.997667)  |  22.0 min  
 4.7  112.0   31696   0.001000  |  0.129487    (0.960938)  |  0.005079    (0.998000)  |  22.1 min  
 4.8  114.0   32262   0.001000  |  0.129439    (0.968750)  |  0.005168    (0.998333)  |  22.3 min  
 4.9  116.0   32828   0.001000  |  0.153728    (0.937500)  |  0.005483    (0.998000)  |  22.5 min  
 5.0  118.0   33394   0.001000  |  0.141483    (0.945312)  |  0.005111    (0.999000)  |  22.7 min  
 5.1  120.0   33960   0.001000  |  0.202189    (0.953125)  |  0.004628    (0.997667)  |  25.4 min  
 5.1  122.1   34526   0.001000  |  0.127567    (0.953125)  |  0.004253    (0.998667)  |  25.5 min  
 5.2  124.1   35092   0.001000  |  0.170309    (0.945312)  |  0.005142    (0.997667)  |  25.7 min  
 5.3  126.1   35658   0.001000  |  0.029802    (1.000000)  |  0.005011    (0.998000)  |  25.9 min  
 5.4  128.1   36224   0.001000  |  0.076283    (0.992188)  |  0.004826    (0.997667)  |  26.0 min  
 5.5  130.1   36790   0.001000  |  0.309368    (0.937500)  |  0.005455    (0.997667)  |  26.2 min  
 5.6  132.1   37356   0.001000  |  0.106807    (0.976562)  |  0.005482    (0.998000)  |  26.4 min  
 5.6  134.1   37922   0.001000  |  0.079129    (0.976562)  |  0.006052    (0.998333)  |  26.6 min  
 5.7  136.1   38488   0.001000  |  0.173626    (0.960938)  |  0.005263    (0.998333)  |  26.7 min  
 5.8  138.1   39054   0.001000  |  0.109293    (0.976562)  |  0.004665    (0.998000)  |  26.9 min  
 5.9  140.1   39620   0.001000  |  0.107964    (0.960938)  |  0.004535    (0.998667)  |  27.1 min  
 6.0  142.1   40186   0.001000  |  0.092714    (0.968750)  |  0.004424    (0.998333)  |  27.2 min  
 6.1  144.1   40752   0.000100  |  0.206274    (0.953125)  |  0.003715    (0.999333)  |  30.0 min  
 6.2  146.1   41318   0.000100  |  0.118098    (0.960938)  |  0.003731    (0.999333)  |  30.1 min  
 6.2  148.1   41884   0.000100  |  0.100490    (0.968750)  |  0.004013    (0.999000)  |  30.3 min  
 6.3  150.1   42450   0.000100  |  0.097022    (0.984375)  |  0.004084    (0.999000)  |  30.5 min  
 6.4  152.1   43016   0.000100  |  0.152466    (0.945312)  |  0.003917    (0.999000)  |  30.6 min  
 6.5  154.1   43582   0.000100  |  0.212765    (0.929688)  |  0.003894    (0.999000)  |  30.8 min  
 6.6  156.1   44148   0.000100  |  0.047942    (0.992188)  |  0.004176    (0.998667)  |  31.0 min  
 6.7  158.1   44714   0.000100  |  0.128010    (0.960938)  |  0.004256    (0.998667)  |  31.1 min  
 6.7  160.1   45280   0.000100  |  0.152295    (0.945312)  |  0.004283    (0.998667)  |  31.3 min  
 6.8  162.1   45846   0.000100  |  0.118665    (0.968750)  |  0.004259    (0.998667)  |  31.5 min  
 6.9  164.1   46412   0.000100  |  0.081274    (0.976562)  |  0.004339    (0.999000)  |  31.6 min  
 7.0  166.1   46978   0.000100  |  0.065679    (0.976562)  |  0.004191    (0.998333)  |  31.8 min  
 7.1  168.1   47544   0.000100  |  0.086537    (0.968750)  |  0.004335    (0.998667)  |  34.5 min  
 7.2  170.1   48110   0.000100  |  0.052799    (0.992188)  |  0.004116    (0.998333)  |  34.7 min  
 7.2  172.1   48676   0.000100  |  0.119635    (0.968750)  |  0.004092    (0.998667)  |  34.9 min  
 7.3  174.1   49242   0.000100  |  0.069912    (0.968750)  |  0.004269    (0.999000)  |  35.0 min  
 7.4  176.1   49808   0.000100  |  0.109598    (0.968750)  |  0.004217    (0.999000)  |  35.2 min  
 7.5  178.1   50374   0.000100  |  0.085566    (0.968750)  |  0.004229    (0.998667)  |  35.4 min  
 7.6  180.1   50940   0.000100  |  0.172443    (0.945312)  |  0.004279    (0.998333)  |  35.5 min  
 7.7  182.1   51506   0.000100  |  0.047067    (0.984375)  |  0.004450    (0.998000)  |  35.7 min  
 7.8  184.1   52072   0.000100  |  0.234810    (0.937500)  |  0.004311    (0.998333)  |  35.9 min  
 7.8  186.1   52638   0.000100  |  0.085083    (0.976562)  |  0.004315    (0.998333)  |  36.0 min  
 7.9  188.1   53204   0.000100  |  0.148994    (0.960938)  |  0.004382    (0.998333)  |  36.2 min  
 8.0  190.1   53770   0.000100  |  0.215789    (0.914062)  |  0.004525    (0.997667)  |  39.0 min  
 8.1  192.1   54336   0.000100  |  0.090063    (0.976562)  |  0.004326    (0.998667)  |  39.1 min  
 8.2  194.1   54902   0.000100  |  0.139097    (0.968750)  |  0.004507    (0.998000)  |  39.3 min  
 8.3  196.1   55468   0.000100  |  0.097882    (0.976562)  |  0.004346    (0.998333)  |  39.5 min  
 8.3  198.1   56034   0.000100  |  0.041849    (0.992188)  |  0.004492    (0.997667)  |  39.6 min  
 8.4  200.1   56600   0.000100  |  0.152385    (0.953125)  |  0.004410    (0.998000)  |  39.8 min  
 8.5  202.1   57166   0.000100  |  0.136352    (0.968750)  |  0.004424    (0.997667)  |  39.9 min  
 8.6  204.1   57732   0.000100  |  0.101633    (0.968750)  |  0.004546    (0.997333)  |  40.1 min  
 8.7  206.1   58298   0.000100  |  0.108001    (0.968750)  |  0.004653    (0.998000)  |  40.3 min  
 8.8  208.1   58864   0.000100  |  0.120384    (0.976562)  |  0.004360    (0.997667)  |  40.5 min  
 8.8  210.1   59430   0.000100  |  0.146303    (0.960938)  |  0.004484    (0.998000)  |  40.6 min  
 8.9  212.1   59996   0.000100  |  0.157703    (0.968750)  |  0.004530    (0.998000)  |  40.8 min  
 9.0  213.7   60453   0.000100  |  0.161457    (0.953125)  |  0.004309    (0.998000)  |  41.0 min  

** evaluation on test set **
test_loss=0.133753    (test_acc=0.976089)

sucess
--- [START 2017-02-26 20:57:47] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  0.423371    (0.882812)  |  0.434117    (0.860000)  |   0.2 min  
 0.2    4.0   01132   0.010000  |  0.143645    (0.968750)  |  0.206798    (0.937667)  |   0.4 min  
 0.3    6.0   01698   0.010000  |  0.086771    (0.976562)  |  0.144690    (0.959333)  |   0.5 min  
 0.3    8.0   02264   0.010000  |  0.053675    (0.984375)  |  0.128049    (0.968667)  |   0.7 min  
 0.4   10.0   02830   0.010000  |  0.009565    (1.000000)  |  0.115212    (0.974333)  |   0.8 min  
 0.5   12.0   03396   0.010000  |  0.020500    (0.992188)  |  0.097455    (0.976000)  |   1.0 min  
 0.6   14.0   03962   0.010000  |  0.051340    (0.984375)  |  0.082319    (0.980667)  |   1.2 min  
 0.7   16.0   04528   0.010000  |  0.007191    (1.000000)  |  0.091703    (0.984000)  |   1.3 min  
 0.8   18.0   05094   0.010000  |  0.003698    (1.000000)  |  0.099224    (0.977333)  |   1.5 min  
 0.8   20.0   05660   0.010000  |  0.000353    (1.000000)  |  0.082736    (0.982333)  |   1.6 min  
 0.9   22.0   06226   0.010000  |  0.017798    (0.992188)  |  0.066400    (0.988667)  |   1.8 min  
 1.0   24.0   06792   0.010000  |  0.037363    (0.984375)  |  0.095987    (0.981667)  |   2.0 min  
 1.1   26.0   07358   0.010000  |  0.002096    (1.000000)  |  0.063520    (0.989000)  |   2.2 min  
 1.2   28.0   07924   0.010000  |  0.018505    (0.992188)  |  0.081104    (0.986333)  |   2.4 min  
 1.3   30.0   08490   0.010000  |  0.000543    (1.000000)  |  0.066140    (0.990000)  |   2.5 min  
 1.3   32.0   09056   0.010000  |  0.000226    (1.000000)  |  0.064204    (0.991000)  |   2.7 min  
 1.4   34.0   09622   0.010000  |  0.000500    (1.000000)  |  0.072214    (0.990667)  |   2.9 min  
 1.5   36.0   10188   0.010000  |  0.016368    (1.000000)  |  0.111647    (0.978333)  |   3.0 min  
 1.6   38.0   10754   0.010000  |  0.001563    (1.000000)  |  0.083517    (0.985333)  |   3.2 min  
 1.7   40.0   11320   0.010000  |  0.000080    (1.000000)  |  0.060815    (0.990000)  |   3.4 min  
 1.8   42.0   11886   0.010000  |  0.015845    (0.992188)  |  0.069054    (0.987333)  |   3.5 min  
 1.9   44.0   12452   0.010000  |  0.000045    (1.000000)  |  0.068473    (0.988333)  |   3.7 min  
 1.9   46.0   13018   0.010000  |  0.003629    (1.000000)  |  0.066165    (0.988000)  |   3.9 min  
 2.0   48.0   13584   0.010000  |  0.000014    (1.000000)  |  0.066988    (0.989333)  |   4.1 min  
 2.1   50.0   14150   0.010000  |  0.000095    (1.000000)  |  0.064943    (0.990333)  |   4.3 min  
 2.2   52.0   14716   0.010000  |  0.000344    (1.000000)  |  0.069231    (0.990000)  |   4.4 min  
 2.3   54.0   15282   0.010000  |  0.000039    (1.000000)  |  0.065197    (0.990333)  |   4.6 min  
 2.4   56.0   15848   0.010000  |  0.000100    (1.000000)  |  0.064976    (0.990000)  |   4.8 min  
 2.4   58.0   16414   0.010000  |  0.000064    (1.000000)  |  0.065665    (0.990000)  |   4.9 min  
 2.5   60.0   16980   0.010000  |  0.000108    (1.000000)  |  0.064737    (0.990333)  |   5.1 min  
 2.6   62.0   17546   0.010000  |  0.000044    (1.000000)  |  0.064695    (0.990333)  |   5.3 min  
 2.7   64.0   18112   0.010000  |  0.000047    (1.000000)  |  0.065487    (0.990333)  |   5.4 min  
 2.8   66.0   18678   0.010000  |  0.000020    (1.000000)  |  0.065362    (0.990333)  |   5.6 min  
 2.9   68.0   19244   0.010000  |  0.000158    (1.000000)  |  0.065637    (0.990333)  |   5.8 min  
 2.9   70.0   19810   0.010000  |  0.000008    (1.000000)  |  0.066360    (0.990333)  |   5.9 min  
 3.0   72.0   20376   0.001000  |  0.000070    (1.000000)  |  0.066017    (0.990000)  |   6.2 min  
 3.1   74.0   20942   0.001000  |  0.000075    (1.000000)  |  0.066054    (0.990000)  |   6.3 min  
 3.2   76.0   21508   0.001000  |  0.000015    (1.000000)  |  0.066112    (0.990000)  |   6.5 min  
 3.3   78.0   22074   0.001000  |  0.000067    (1.000000)  |  0.066201    (0.990000)  |   6.7 min  
 3.4   80.0   22640   0.001000  |  0.000023    (1.000000)  |  0.066147    (0.990000)  |   6.8 min  
 3.5   82.0   23206   0.001000  |  0.000031    (1.000000)  |  0.066076    (0.990333)  |   7.0 min  
 3.5   84.0   23772   0.001000  |  0.000110    (1.000000)  |  0.066150    (0.990333)  |   7.2 min  
 3.6   86.0   24338   0.001000  |  0.000066    (1.000000)  |  0.066184    (0.990333)  |   7.4 min  
 3.7   88.0   24904   0.001000  |  0.000015    (1.000000)  |  0.066155    (0.990333)  |   7.5 min  
 3.8   90.0   25470   0.001000  |  0.000016    (1.000000)  |  0.066249    (0.990667)  |   7.7 min  
 3.9   92.0   26036   0.001000  |  0.000007    (1.000000)  |  0.066335    (0.990333)  |   7.9 min  
 4.0   94.0   26602   0.001000  |  0.000161    (1.000000)  |  0.066350    (0.990333)  |   8.0 min  
 4.0   96.0   27168   0.001000  |  0.000020    (1.000000)  |  0.066350    (0.990333)  |   8.3 min  
 4.1   98.0   27734   0.001000  |  0.000025    (1.000000)  |  0.066393    (0.990667)  |   8.4 min  
 4.2  100.0   28300   0.001000  |  0.000024    (1.000000)  |  0.066428    (0.990333)  |   8.6 min  
 4.3  102.0   28866   0.001000  |  0.000010    (1.000000)  |  0.066439    (0.990333)  |   8.8 min  
 4.4  104.0   29432   0.001000  |  0.000010    (1.000000)  |  0.066436    (0.990333)  |   9.0 min  
 4.5  106.0   29998   0.001000  |  0.000023    (1.000000)  |  0.066510    (0.990333)  |   9.1 min  
 4.6  108.0   30564   0.001000  |  0.000008    (1.000000)  |  0.066619    (0.990333)  |   9.3 min  
 4.6  110.0   31130   0.001000  |  0.000008    (1.000000)  |  0.066572    (0.990667)  |   9.5 min  
 4.7  112.0   31696   0.001000  |  0.000080    (1.000000)  |  0.066669    (0.990667)  |   9.6 min  
 4.8  114.0   32262   0.001000  |  0.000005    (1.000000)  |  0.066674    (0.990667)  |   9.8 min  
 4.9  116.0   32828   0.001000  |  0.000079    (1.000000)  |  0.066670    (0.990667)  |  10.0 min  
 5.0  118.0   33394   0.001000  |  0.000028    (1.000000)  |  0.066628    (0.990667)  |  10.2 min  
 5.1  120.0   33960   0.001000  |  0.000010    (1.000000)  |  0.066667    (0.990667)  |  10.4 min  
 5.1  122.1   34526   0.001000  |  0.000018    (1.000000)  |  0.066704    (0.990667)  |  10.6 min  
 5.2  124.1   35092   0.001000  |  0.000031    (1.000000)  |  0.066695    (0.990667)  |  10.7 min  
 5.3  126.1   35658   0.001000  |  0.000012    (1.000000)  |  0.066794    (0.990667)  |  10.9 min  
 5.4  128.1   36224   0.001000  |  0.000004    (1.000000)  |  0.066726    (0.990667)  |  11.1 min  
 5.5  130.1   36790   0.001000  |  0.000008    (1.000000)  |  0.066727    (0.990667)  |  11.2 min  
 5.6  132.1   37356   0.001000  |  0.000123    (1.000000)  |  0.066736    (0.990667)  |  11.4 min  
 5.6  134.1   37922   0.001000  |  0.000047    (1.000000)  |  0.066718    (0.990667)  |  11.6 min  
 5.7  136.1   38488   0.001000  |  0.000250    (1.000000)  |  0.066827    (0.990667)  |  11.8 min  
 5.8  138.1   39054   0.001000  |  0.000005    (1.000000)  |  0.066791    (0.990667)  |  11.9 min  
 5.9  140.1   39620   0.001000  |  0.000024    (1.000000)  |  0.066760    (0.990667)  |  12.1 min  
 6.0  142.1   40186   0.001000  |  0.000014    (1.000000)  |  0.066773    (0.990667)  |  12.3 min  
 6.1  144.1   40752   0.000100  |  0.000015    (1.000000)  |  0.066821    (0.990333)  |  12.5 min  
 6.2  146.1   41318   0.000100  |  0.000025    (1.000000)  |  0.066828    (0.990333)  |  12.7 min  
 6.2  148.1   41884   0.000100  |  0.000013    (1.000000)  |  0.066838    (0.990333)  |  12.9 min  
 6.3  150.1   42450   0.000100  |  0.000023    (1.000000)  |  0.066834    (0.990333)  |  13.0 min  
 6.4  152.1   43016   0.000100  |  0.000005    (1.000000)  |  0.066835    (0.990333)  |  13.2 min  
 6.5  154.1   43582   0.000100  |  0.000003    (1.000000)  |  0.066836    (0.990333)  |  13.4 min  
 6.6  156.1   44148   0.000100  |  0.000013    (1.000000)  |  0.066846    (0.990333)  |  13.5 min  
 6.7  158.1   44714   0.000100  |  0.000044    (1.000000)  |  0.066850    (0.990333)  |  13.7 min  
 6.7  160.1   45280   0.000100  |  0.000023    (1.000000)  |  0.066856    (0.990333)  |  13.9 min  
 6.8  162.1   45846   0.000100  |  0.000009    (1.000000)  |  0.066862    (0.990333)  |  14.1 min  
 6.9  164.1   46412   0.000100  |  0.000014    (1.000000)  |  0.066870    (0.990333)  |  14.2 min  
 7.0  166.1   46978   0.000100  |  0.000005    (1.000000)  |  0.066871    (0.990333)  |  14.4 min  
 7.1  168.1   47544   0.000100  |  0.000016    (1.000000)  |  0.066864    (0.990333)  |  14.6 min  
 7.2  170.1   48110   0.000100  |  0.000004    (1.000000)  |  0.066866    (0.990333)  |  14.8 min  
 7.2  172.1   48676   0.000100  |  0.000011    (1.000000)  |  0.066871    (0.990333)  |  15.0 min  
 7.3  174.1   49242   0.000100  |  0.000010    (1.000000)  |  0.066873    (0.990333)  |  15.1 min  
 7.4  176.1   49808   0.000100  |  0.000014    (1.000000)  |  0.066876    (0.990333)  |  15.3 min  
 7.5  178.1   50374   0.000100  |  0.000050    (1.000000)  |  0.066878    (0.990333)  |  15.5 min  
 7.6  180.1   50940   0.000100  |  0.000008    (1.000000)  |  0.066875    (0.990333)  |  15.7 min  
 7.7  182.1   51506   0.000100  |  0.000160    (1.000000)  |  0.066878    (0.990333)  |  15.8 min  
 7.8  184.1   52072   0.000100  |  0.000030    (1.000000)  |  0.066885    (0.990333)  |  16.0 min  
 7.8  186.1   52638   0.000100  |  0.000098    (1.000000)  |  0.066885    (0.990333)  |  16.2 min  
 7.9  188.1   53204   0.000100  |  0.000022    (1.000000)  |  0.066891    (0.990333)  |  16.4 min  
 8.0  190.1   53770   0.000100  |  0.000020    (1.000000)  |  0.066897    (0.990333)  |  16.6 min  
 8.1  192.1   54336   0.000100  |  0.000009    (1.000000)  |  0.066901    (0.990333)  |  16.7 min  
 8.2  194.1   54902   0.000100  |  0.000011    (1.000000)  |  0.066896    (0.990333)  |  16.9 min  
 8.3  196.1   55468   0.000100  |  0.000012    (1.000000)  |  0.066895    (0.990333)  |  17.1 min  
 8.3  198.1   56034   0.000100  |  0.000027    (1.000000)  |  0.066898    (0.990333)  |  17.3 min  
 8.4  200.1   56600   0.000100  |  0.000043    (1.000000)  |  0.066906    (0.990333)  |  17.4 min  
 8.5  202.1   57166   0.000100  |  0.000351    (1.000000)  |  0.066905    (0.990333)  |  17.6 min  
 8.6  204.1   57732   0.000100  |  0.000035    (1.000000)  |  0.066907    (0.990333)  |  17.8 min  
 8.7  206.1   58298   0.000100  |  0.000028    (1.000000)  |  0.066905    (0.990333)  |  17.9 min  
 8.8  208.1   58864   0.000100  |  0.000006    (1.000000)  |  0.066909    (0.990333)  |  18.1 min  
 8.8  210.1   59430   0.000100  |  0.000036    (1.000000)  |  0.066912    (0.990333)  |  18.3 min  
 8.9  212.1   59996   0.000100  |  0.000031    (1.000000)  |  0.066915    (0.990333)  |  18.4 min  
 9.0  213.7   60453   0.000100  |  0.000012    (1.000000)  |  0.066911    (0.990333)  |  18.6 min  

** evaluation on test set **
test_loss=0.631585    (test_acc=0.931750)

sucess
--- [START 2017-02-26 21:32:13] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 36209
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
--- [START 2017-02-26 21:32:36] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 36209
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_0( input_shape=(1,1,1), output_shape = (1)):

    H, W, C   = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5,5), stride=[1,1,1,1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3  = flatten(block2)
        block3  = dense(block3, num_hiddens=100, has_bias=True)
        block3  = relu(block3)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 2.0    2.0   00566   0.010000  |  0.153569    (0.937500)  |  0.258869    (0.918000)  |   0.2 min  
 4.0    4.0   01132   0.001000  |  0.047067    (0.992188)  |  0.108161    (0.976000)  |   0.3 min  
 6.0    6.0   01698   0.000100  |  0.030238    (0.992188)  |  0.098638    (0.974667)  |   0.5 min  
 8.1    8.0   02264   0.000100  |  0.038952    (0.992188)  |  0.096263    (0.977333)  |   0.7 min  
 9.0    8.9   02529   0.000100  |  0.022698    (1.000000)  |  0.095657    (0.977000)  |   0.8 min  

** evaluation on test set **
test_loss=0.531760    (test_acc=0.898021)

sucess
--- [START 2017-02-26 21:43:03] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
--- [START 2017-02-26 21:43:19] ----------------------------------------------------------------

** some experiment setting **
	SEED    = 202
	file    = /root/share/docs/git/hengck23-udacity/udacity-driverless-car-nd-p2/basic/code/traffic_sign_trainer.py
	out_dir = /root/share/out/udacity/11

** some data setting **
	height, width, channel = 32, 32, 3
	num_test  = 12630
	num_valid = 3000
	num_train = 36209
	num_train (after flip)= 62187
	num_argument = 860000

** some solver setting **
	batch_size = 128
	max_run  = 9
	steps    = (0, 3, 6, 8)
	rates    = (0.01, 0.001, 0.0001, 0.0001)

	keep     = 0.200000
	num_per_class = 20000

** net constuction source: **
def LeNet_1( input_shape=(1,1,1), output_shape = (1)):
    H, W, C = input_shape
    num_class = output_shape
    input = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')

    with tf.variable_scope('block1') as scope:
        block1 = conv2d(input, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block1 = relu(block1)
        block1 = maxpool(block1, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block2') as scope:
        block2 = conv2d(block1, num_kernels=108, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME', has_bias=True)
        block2 = relu(block2)
        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')

    with tf.variable_scope('block3') as scope:
        block3 = flatten(block2)
        block3 = dense(block3, num_hiddens=100, has_bias=True)
        block3 = relu(block3)
        block3 = dropout(block3,keep=0.5)

    with tf.variable_scope('block4') as scope:
        block4 = dense(block3, num_hiddens=100, has_bias=True)
        block4 = relu(block4)
        block4 = dropout(block4,keep=0.5)

    with tf.variable_scope('block5') as scope:
        block5 = dense(block4, num_hiddens=num_class, has_bias=True)

    logit = block5
    return logit
-------------------------------


net summary : 
	num of conv     = 11
	all mac         = 85.1 (M)
	all param_size  = 2.4 (M)


 run  epoch   iter    rate      |  train_loss    (acc)     |  valid_loss    (acc)     |     
--------------------------------------------------------------------------------------------
 0.1    2.0   00566   0.010000  |  3.575819    (0.015625)  |  3.528084    (0.059667)  |   2.6 min  
 0.2    4.0   01132   0.010000  |  3.515047    (0.031250)  |  3.448129    (0.017000)  |   2.8 min  
 0.3    6.0   01698   0.010000  |  3.339529    (0.070312)  |  3.316690    (0.076000)  |   3.0 min  
 0.3    8.0   02264   0.010000  |  3.256264    (0.078125)  |  3.052181    (0.133667)  |   3.1 min  
 0.4   10.0   02830   0.010000  |  2.945503    (0.171875)  |  2.555401    (0.229000)  |   3.3 min  
 0.5   12.0   03396   0.010000  |  2.677042    (0.179688)  |  2.126527    (0.305000)  |   3.5 min  
 0.6   14.0   03962   0.010000  |  2.558269    (0.148438)  |  1.809718    (0.388667)  |   3.6 min  
 0.7   16.0   04528   0.010000  |  2.408717    (0.218750)  |  1.578594    (0.463000)  |   3.8 min  
 0.8   18.0   05094   0.010000  |  2.270343    (0.273438)  |  1.543065    (0.440000)  |   4.0 min  
 0.8   20.0   05660   0.010000  |  2.261281    (0.328125)  |  1.340856    (0.514333)  |   4.1 min  
 0.9   22.0   06226   0.010000  |  2.151750    (0.296875)  |  1.184775    (0.586333)  |   4.3 min  
 1.0   24.0   06792   0.010000  |  2.074672    (0.343750)  |  1.099691    (0.601000)  |   7.0 min  
 1.1   26.0   07358   0.010000  |  1.837069    (0.359375)  |  1.016495    (0.626000)  |   7.1 min  
 1.2   28.0   07924   0.010000  |  1.683609    (0.351562)  |  0.917702    (0.672667)  |   7.3 min  
 1.3   30.0   08490   0.010000  |  1.672509    (0.429688)  |  0.850626    (0.688333)  |   7.5 min  
 1.3   32.0   09056   0.010000  |  1.344800    (0.539062)  |  0.781418    (0.706000)  |   7.6 min  
 1.4   34.0   09622   0.010000  |  1.280858    (0.593750)  |  0.759825    (0.711333)  |   7.8 min  
 1.5   36.0   10188   0.010000  |  1.325667    (0.593750)  |  0.669787    (0.763333)  |   8.0 min  
 1.6   38.0   10754   0.010000  |  1.409543    (0.523438)  |  0.638937    (0.769333)  |   8.1 min  
 1.7   40.0   11320   0.010000  |  1.297218    (0.585938)  |  0.614187    (0.800000)  |   8.3 min  
 1.8   42.0   11886   0.010000  |  1.231383    (0.625000)  |  0.521155    (0.827000)  |   8.5 min  
 1.9   44.0   12452   0.010000  |  1.183428    (0.632812)  |  0.526637    (0.820000)  |   8.7 min  
 1.9   46.0   13018   0.010000  |  1.171381    (0.546875)  |  0.447510    (0.864000)  |   8.8 min  
 2.0   48.0   13584   0.010000  |  1.201054    (0.578125)  |  0.387124    (0.875000)  |  11.5 min  
 2.1   50.0   14150   0.010000  |  1.094736    (0.687500)  |  0.383190    (0.888333)  |  11.6 min  
 2.2   52.0   14716   0.010000  |  1.079929    (0.656250)  |  0.299509    (0.923333)  |  11.8 min  
 2.3   54.0   15282   0.010000  |  1.009204    (0.671875)  |  0.336903    (0.879667)  |  12.0 min  
 2.4   56.0   15848   0.010000  |  0.960202    (0.695312)  |  0.281141    (0.878000)  |  12.1 min  
 2.4   58.0   16414   0.010000  |  1.011647    (0.734375)  |  0.216098    (0.943667)  |  12.3 min  
 2.5   60.0   16980   0.010000  |  0.842682    (0.703125)  |  0.241232    (0.921000)  |  12.5 min  
 2.6   62.0   17546   0.010000  |  0.704519    (0.789062)  |  0.193122    (0.951667)  |  12.6 min  
 2.7   64.0   18112   0.010000  |  0.737794    (0.765625)  |  0.213748    (0.941000)  |  12.8 min  
 2.8   66.0   18678   0.010000  |  0.822661    (0.695312)  |  0.163722    (0.954333)  |  13.0 min  
 2.9   68.0   19244   0.010000  |  0.660007    (0.804688)  |  0.154071    (0.960667)  |  13.2 min  
 2.9   70.0   19810   0.010000  |  0.776056    (0.742188)  |  0.139356    (0.959667)  |  13.3 min  
 3.0   72.0   20376   0.001000  |  0.961724    (0.726562)  |  0.106839    (0.975000)  |  16.0 min  
 3.1   74.0   20942   0.001000  |  0.645929    (0.781250)  |  0.099060    (0.977333)  |  16.1 min  
 3.2   76.0   21508   0.001000  |  0.635049    (0.765625)  |  0.097953    (0.975667)  |  16.3 min  
 3.3   78.0   22074   0.001000  |  0.702920    (0.820312)  |  0.088558    (0.975667)  |  16.5 min  
 3.4   80.0   22640   0.001000  |  0.437865    (0.867188)  |  0.085899    (0.978333)  |  16.6 min  
 3.5   82.0   23206   0.001000  |  0.536624    (0.820312)  |  0.082699    (0.981000)  |  16.8 min  
 3.5   84.0   23772   0.001000  |  0.461739    (0.828125)  |  0.079264    (0.980667)  |  16.9 min  
 3.6   86.0   24338   0.001000  |  0.514092    (0.859375)  |  0.082162    (0.978000)  |  17.1 min  
 3.7   88.0   24904   0.001000  |  0.483772    (0.890625)  |  0.077937    (0.980667)  |  17.3 min  
 3.8   90.0   25470   0.001000  |  0.694132    (0.789062)  |  0.080365    (0.978667)  |  17.5 min  
 3.9   92.0   26036   0.001000  |  0.485270    (0.828125)  |  0.071701    (0.983333)  |  17.6 min  
 4.0   94.0   26602   0.001000  |  0.290385    (0.898438)  |  0.076150    (0.981333)  |  17.8 min  
 4.0   96.0   27168   0.001000  |  0.520755    (0.851562)  |  0.070957    (0.982333)  |  20.4 min  
 4.1   98.0   27734   0.001000  |  0.481501    (0.812500)  |  0.076775    (0.980667)  |  20.6 min  
 4.2  100.0   28300   0.001000  |  0.628637    (0.781250)  |  0.073927    (0.981000)  |  20.8 min  
 4.3  102.0   28866   0.001000  |  0.330850    (0.921875)  |  0.072026    (0.983000)  |  20.9 min  
 4.4  104.0   29432   0.001000  |  0.445529    (0.859375)  |  0.069122    (0.982333)  |  21.1 min  
 4.5  106.0   29998   0.001000  |  0.585291    (0.820312)  |  0.075128    (0.980000)  |  21.3 min  
 4.6  108.0   30564   0.001000  |  0.588706    (0.859375)  |  0.071446    (0.980000)  |  21.4 min  
 4.6  110.0   31130   0.001000  |  0.577596    (0.820312)  |  0.071002    (0.982667)  |  21.6 min  
 4.7  112.0   31696   0.001000  |  0.501907    (0.820312)  |  0.069807    (0.982333)  |  21.8 min  
 4.8  114.0   32262   0.001000  |  0.511112    (0.820312)  |  0.064852    (0.981667)  |  21.9 min  
 4.9  116.0   32828   0.001000  |  0.585511    (0.835938)  |  0.066006    (0.983333)  |  22.1 min  
 5.0  118.0   33394   0.001000  |  0.503330    (0.843750)  |  0.065148    (0.985000)  |  22.3 min  
 5.1  120.0   33960   0.001000  |  0.593234    (0.835938)  |  0.064048    (0.984333)  |  24.9 min  
 5.1  122.1   34526   0.001000  |  0.658384    (0.789062)  |  0.062348    (0.982333)  |  25.1 min  
 5.2  124.1   35092   0.001000  |  0.504434    (0.875000)  |  0.059125    (0.985000)  |  25.2 min  
 5.3  126.1   35658   0.001000  |  0.443765    (0.851562)  |  0.063686    (0.983333)  |  25.4 min  
 5.4  128.1   36224   0.001000  |  0.620062    (0.804688)  |  0.063774    (0.983000)  |  25.6 min  
 5.5  130.1   36790   0.001000  |  0.580850    (0.773438)  |  0.063457    (0.985333)  |  25.7 min  
 5.6  132.1   37356   0.001000  |  0.408354    (0.859375)  |  0.058635    (0.985333)  |  25.9 min  
 5.6  134.1   37922   0.001000  |  0.626701    (0.835938)  |  0.058438    (0.984333)  |  26.1 min  
 5.7  136.1   38488   0.001000  |  0.551654    (0.851562)  |  0.061606    (0.984000)  |  26.2 min  
 5.8  138.1   39054   0.001000  |  0.421013    (0.867188)  |  0.059111    (0.984667)  |  26.4 min  
 5.9  140.1   39620   0.001000  |  0.537047    (0.843750)  |  0.055977    (0.987333)  |  26.6 min  
 6.0  142.1   40186   0.001000  |  0.439915    (0.875000)  |  0.057274    (0.984333)  |  26.8 min  
 6.1  144.1   40752   0.000100  |  0.547314    (0.820312)  |  0.055964    (0.985667)  |  29.4 min  
 6.2  146.1   41318   0.000100  |  0.393815    (0.882812)  |  0.055942    (0.986667)  |  29.5 min  
 6.2  148.1   41884   0.000100  |  0.470523    (0.835938)  |  0.055117    (0.985667)  |  29.7 min  
 6.3  150.1   42450   0.000100  |  0.410463    (0.867188)  |  0.055182    (0.985000)  |  29.9 min  
 6.4  152.1   43016   0.000100  |  0.498896    (0.828125)  |  0.054969    (0.985667)  |  30.0 min  
 6.5  154.1   43582   0.000100  |  0.442948    (0.890625)  |  0.055310    (0.986000)  |  30.2 min  
 6.6  156.1   44148   0.000100  |  0.288859    (0.867188)  |  0.055524    (0.986333)  |  30.4 min  
 6.7  158.1   44714   0.000100  |  0.411419    (0.898438)  |  0.054491    (0.986333)  |  30.5 min  
 6.7  160.1   45280   0.000100  |  0.451292    (0.882812)  |  0.054560    (0.986667)  |  30.7 min  
 6.8  162.1   45846   0.000100  |  0.504186    (0.843750)  |  0.053171    (0.986667)  |  30.9 min  
 6.9  164.1   46412   0.000100  |  0.526852    (0.851562)  |  0.054203    (0.985333)  |  31.1 min  
 7.0  166.1   46978   0.000100  |  0.356247    (0.906250)  |  0.054451    (0.986667)  |  31.2 min  
 7.1  168.1   47544   0.000100  |  0.353086    (0.898438)  |  0.054224    (0.986000)  |  33.9 min  
 7.2  170.1   48110   0.000100  |  0.437755    (0.867188)  |  0.053457    (0.986667)  |  34.0 min  
 7.2  172.1   48676   0.000100  |  0.396634    (0.835938)  |  0.054223    (0.987333)  |  34.2 min  
 7.3  174.1   49242   0.000100  |  0.402767    (0.890625)  |  0.054188    (0.987000)  |  34.4 min  
 7.4  176.1   49808   0.000100  |  0.502994    (0.828125)  |  0.054615    (0.986333)  |  34.5 min  
 7.5  178.1   50374   0.000100  |  0.450327    (0.851562)  |  0.053989    (0.986333)  |  34.7 min  
 7.6  180.1   50940   0.000100  |  0.541447    (0.835938)  |  0.054053    (0.986000)  |  34.9 min  
 7.7  182.1   51506   0.000100  |  0.343505    (0.898438)  |  0.054198    (0.987000)  |  35.0 min  
 7.8  184.1   52072   0.000100  |  0.530594    (0.835938)  |  0.052965    (0.985667)  |  35.2 min  
 7.8  186.1   52638   0.000100  |  0.431260    (0.890625)  |  0.053993    (0.986333)  |  35.4 min  
 7.9  188.1   53204   0.000100  |  0.551564    (0.859375)  |  0.053053    (0.986667)  |  35.6 min  
 8.0  190.1   53770   0.000100  |  0.544884    (0.851562)  |  0.053522    (0.987333)  |  38.2 min  
 8.1  192.1   54336   0.000100  |  0.751446    (0.812500)  |  0.053476    (0.986000)  |  38.4 min  
 8.2  194.1   54902   0.000100  |  0.591494    (0.835938)  |  0.052711    (0.987000)  |  38.6 min  
 8.3  196.1   55468   0.000100  |  0.450042    (0.875000)  |  0.052713    (0.987000)  |  38.7 min  
 8.3  198.1   56034   0.000100  |  0.384556    (0.875000)  |  0.052617    (0.985667)  |  38.9 min  
 8.4  200.1   56600   0.000100  |  0.523975    (0.835938)  |  0.051638    (0.986667)  |  39.1 min  
 8.5  202.1   57166   0.000100  |  0.602890    (0.828125)  |  0.051969    (0.986667)  |  39.2 min  
 8.6  204.1   57732   0.000100  |  0.506462    (0.898438)  |  0.052942    (0.987000)  |  39.4 min  
 8.7  206.1   58298   0.000100  |  0.507935    (0.851562)  |  0.052919    (0.986333)  |  39.6 min  
 8.8  208.1   58864   0.000100  |  0.490460    (0.835938)  |  0.053651    (0.986333)  |  39.7 min  
 8.8  210.1   59430   0.000100  |  0.450119    (0.875000)  |  0.053433    (0.986667)  |  39.9 min  
 8.9  212.1   59996   0.000100  |  0.591414    (0.812500)  |  0.051709    (0.986333)  |  40.1 min  
 9.0  213.7   60453   0.000100  |  0.562911    (0.859375)  |  0.051229    (0.986667)  |  40.2 min  

** evaluation on test set **
test_loss=0.162102    (test_acc=0.955582)

sucess
